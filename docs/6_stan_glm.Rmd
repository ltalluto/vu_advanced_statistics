---
title: "Intro to Stan & Generalised Linear Modelling"
author: "Matthew Talluto"
date: "28.11.2022"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
  beamer_presentation: default
---


```{r setup, include=FALSE, results = "hide", cache = FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png")

library(ggplot2)
library(igraph)
library(ggplot2)
library(ggnetwork)
library(data.table)
library(rstan)

Howell1 = fread("https://github.com/rmcelreath/rethinking/raw/master/data/Howell1.csv")


cols = c("#F8766D", "#7CAE00", "#00BFC4", "#C77CFF")

```



## What is Stan? 
<div class="left lt">

* Stan is a modelling language for scientific computing
* Stan is a *probabilistic programming language*
   - deterministic variables: `mu = a + b * x`
   - Stochastic variables: `y ~ normal(mu, sigma)`
* Samples from the posterior using Hamiltonian Monte Carlo

</div>

<div class="right rt">

</div>


## What is Stan? 

<div class="left lt">

* Stan is a modelling language for scientific computing
* Stan is a *probabilistic programming language*
   - deterministic variables: `mu = a + b * x`
   - Stochastic variables: `y ~ normal(mu, sigma)`
* Samples from the posterior using Hamiltonian Monte Carlo

</div>

<div class="right rt">

1. Write a Stan model in a `.stan` file
2. Prepare all data in R
3. Use the `rstan` package to invoke the Stan interpreter
   - Translates your model into a C++ program w/ an HMC sampler, then compiles for your computer
4. Run the program from R using `rstan`.
5. Perform posterior inference using various R packages.

</div>



## Representing models in Stan 

<div class="left lt">

```{stan output.var = "noeval", eval=FALSE}
mu = a + b*x;
y ~ normal(mu, s);
a ~ normal(0, 20);
b ~ normal(0, 5);
s ~ exponential(0.2);
```

</div>

<div class="right rt">


```{r, echo = FALSE}
gr = graph_from_literal(a-+"mu", b-+"mu", s-+y, x-+"mu", "mu"-+y)
V(gr)$type = c("random", "deterministic", "random", "random", "random", "deterministic")
V(gr)$source = c("unknown", "unknown", "unknown", "unknown", "known", "known")
layout = matrix(c(0.5,2,  0.5,1,  1,2,  1.5,2,  1.25,0, 0,2), byrow=TRUE, ncol=2)


n = ggnetwork(gr, layout=layout)
pl = ggplot(n, aes(x = x, y = y, xend = xend, yend = yend)) + 
   geom_edges(colour="gray50", arrow=arrow(length = unit(6, "pt"), type = "closed")) + 
   theme_blank() + geom_nodes(aes(color=type, shape = source), size=6) + 
   geom_nodelabel(aes(label = name), fontface = "bold", nudge_x=-0.1)
pl
```
</div>



## Why Stan?

> - For many models, faster than other MCMC samplers
> - Fast convergence
> - Very concise and natural model specification


## Writing models - data block
* Stan models come in blocks
* The `data` block tells Stan what to expect from all the data you will pass it
	- These are usually **knowns** in your model graph
	- Stan doesn't how big a vector is when you pass it, so you have to tell it

<div class="left lt">
```{r eval=FALSE}
## R version
log_posterior = function(params, data) {
   y = data[['y']]
   x = data[['x']]
}
```

</div>

<div class="right rt">
```{stan output.var="stancode", eval=FALSE}
// Stan version
data {
   int n; // a single integer named n, number of data points
   vector [n] y; // a vector of n real numbers, named y
   vector [n] x;
}
```
</div>

## Writing models - parameters block

* The `parameters` block identifies **random unknowns**
	- specifying **constraints** allows us to avoid impossible values

<div class="left lt">
```{r eval=FALSE}
## R version
log_posterior = function(params, data) {
   y = data[['y']]
   x = data[['x']]
   a = params['a']
   b = params['b']
   s = params['s']

   if(s <= 0)
      return(-Inf)
}
```



</div>

<div class="right rt">
```{stan output.var="stancode", eval=FALSE}
// Stan version
data {
   int n; // a single integer named n, number of data points
   vector [n] y; // a vector of n real numbers, named y
   vector [n] x;
}
parameters {
   real a;
   real b;
   real <lower=0> s; // a single real number that must be positive
}
```

</div>

## Writing models - transformed parameters block

* Optionally, `transformed parameters` does what the name implies
	- Any variables declared in `data` or `parameters` can be used for transformations
	- For example, you can specify **deterministic unkowns** here

<div class="left lt">
```{r eval=FALSE}
## R version
log_posterior = function(params, data) {
   y = data[['y']]
   x = data[['x']]
   a = params['a']
   b = params['b']
   s = params['s']

   if(s <= 0)
      return(-Inf)
   
   mu = a + b * x
}
```



</div>

<div class="right rt">
```{stan output.var="stancode", eval=FALSE}
// Stan version
data {
   int n; // a single integer named n, number of data points
   vector [n] y; // a vector of n real numbers, named y
   vector [n] x;
}
parameters {
   real a;
   real b;
   real <lower=0> s; // a single real number that must be positive
}
transformed parameters {
	vector [n] mu = a * b * x;
	// or another example
	// vector [n] log_x = log(x);
}
```

</div>



## Writing models - model block

* The model block tells Stan how to evaluate the log posterior
	- Likelihood/prior distributions of **random variables** are specified with the `~` symbol

<div class="left lt">
```{r eval=FALSE}
## R version
log_posterior = function(params, data) {
   y = data[['y']]
   x = data[['x']]
   a = params['a']
   b = params['b']
   s = params['s']

   if(s <= 0)
      return(-Inf)
   
   mu = a + b * x
   log_lhood = sum(dnorm(y, mu, s, log = TRUE))
}
```



</div>

<div class="right rt">
```{stan output.var="stancode", eval=FALSE}
// Stan version
data {
   int n; // a single integer named n, number of data points
   vector [n] y; // a vector of n real numbers, named y
   vector [n] x;
}
parameters {
   real a;
   real b;
   real <lower=0> s; // a single real number that must be positive
}
transformed parameters {
	vector [n] mu = a * b * x;
}
model {
	y ~ normal(mu, s);
}
```

</div>




## Writing models - model block

* The model block tells Stan how to evaluate the log posterior
	- Likelihood/prior distributions of **random variables** are specified with the `~` symbol

<div class="left lt">
```{r eval = FALSE}
## R version
log_posterior = function(params, data) {
   y = data[['y']]
   x = data[['x']]
   a = params['a']
   b = params['b']
   s = params['s']

   if(s <= 0)
      return(-Inf)
   
   mu = a + b * x
   log_lhood = sum(dnorm(y, mu, s, log = TRUE))
   log_prior = dnorm(a, 0, 10, log=TRUE) + 
		dnorm(B, 0, 5, log=TRUE) + 
		dexp(s, 0.1, log=TRUE)
   return(log_lhood + log_prior)
}
```

</div>

<div class="right rt">


```{stan output.var="stancode", eval=FALSE}
// Stan version
data {
   int n; // a single integer named n, number of data points
   vector [n] y; // a vector of n real numbers, named y
   vector [n] x;
}
parameters {
   real a;
   real b;
   real <lower=0> s; // a single real number that must be positive
}
transformed parameters {
	vector [n] mu = a * b * x;
}
model {
	y ~ normal(mu, s);
	a ~ normal(0, 10);
	b ~ normal(0, 5);
	s ~ exponential(0.1);
}
```

<p class="bullet2">
Save this code to a file. For example: `stan/sample_stan_lm.stan`
</p>

</div>




## Fitting models
* Save your Stan code in a .stan file
* Compile your model using the `stan_model` function
* Take samples using the `sampling` function

<div class="left lt">
```{r eval=FALSE}
## R version
# get the data and format it
library(data.table)
Howell1 = fread("https://github.com/rmcelreath/rethinking/raw/master/data/Howell1.csv")
mh_data = list(
	y = Howell1$height
	x = Howell1$weight
)
# load the metropolis sampler from this course
source("r/mh.r")

# take 10000 samples, with adaptation
samples = metropolis(log_posterior, initial = c(a = 0, b = 0, s = 1), 
					 data = mh_data)
```

</div>

<div class="right rt">


```{r eval=FALSE}
## rstan version
library(rstan)
library(data.table)

# get the data and format it for rstan
Howell1 = fread("https://github.com/rmcelreath/rethinking/raw/master/data/Howell1.csv")
stan_data = list(
	y = Howell1$height,
	x = Howell1$weight,
	n = length(Howell1$weight)
)
# compile the model
model = stan_model(file = "stan/sample_stan_lm.stan")

# take 5000 samples
samples = sampling(model, iter=5000, data = standata)
```

</div>






## Variables in Stan

<div class="left lt">
* Stan variables have a strong, fixed type that must be declared
* Variables can have **constraints**

**Scalar types**

* `int`
* `real`

</div>


<div class="right rt">

```{stan output.var="garbage", eval=FALSE}
int my_int;
real my_real;
real <lower=0> my_positive_real;
real <lower=0, upper=1> my_proportion;
```

</div>


## Variables in Stan - containers

<div class="left lt">

### Vectors
* 1-dimension
* real numbers only

`vector <constraint> [length] name`

</div>

<div class="right rt">

```{stan output.var="garbage", eval=FALSE}
vector [10] my_ten_reals;
vector <lower=0> [n] n_positive_numbers;
```

</div>


## Variables in Stan - containers

<div class="left lt">

### Vectors
* 1-dimension
* real numbers only

`vector <constraint> [length] name`

### Matrices
* 2-dimensions
* real numbers only

`matrix <constraint> [rows, columns] name`
</div>

<div class="right rt">

```{stan output.var="garbage", eval=FALSE}
vector [10] my_ten_reals;
vector <lower=0> [n] n_positive_numbers;
matrix [n,k] my_matrix;
```

</div>



## Variables in Stan - containers

<div class="left lt">

### Vectors
* 1-dimension
* real numbers only

`vector <constraint> [length] name`

### Matrices
* 2-dimensions
* real numbers only

`matrix <constraint> [rows, columns] name`

### Arrays
* any dimensions
* any type (even other containers)

`type <constraint> name [dimensions]`
</div>

<div class="right rt">

```{stan output.var="garbage", eval=FALSE}
vector [10] my_ten_reals;
vector <lower=0> [n] n_positive_numbers;
matrix [n,k] my_matrix;
int ten_integers [10]; // array of ints
real ten_reals [10]; // array of reals
vector <lower=0> [10] array_of_vectors [5]; // 5 vectors, each of length 10
```

</div>

## German Tank problem in Stan

2. Recall the German tank problem presented in lecture. Use the following captured serial numbers:

`s = c(147, 126, 183, 88, 9, 203, 16, 10, 112, 205)`

* We decided that tank serial numbers are **uniformly distributed** with a minimum of 1 and a maximum possible value equal to $N_{max}$
* We want to estimate $N_{max}$ from the data
* Here we use a very vague prior for $N_{max}$

$$
\begin{aligned}
s & \sim \mathrm{U}(1, N_{max}) \\
N_{max} & \sim \mathrm{Gamma}(\alpha=0.001, \beta=0.001)
\end{aligned}
$$



## German Tank problem: Stan model

<div class="left lt">

```{r, echo=FALSE, fig.height=5, fig.width=7}
gr = graph_from_literal(s+-Nmax, Nmax+-"α=0.001", Nmax+-"β=0.001")
V(gr)$type = c("random", "random", "deterministic", "deterministic")
V(gr)$source = c("known", "unknown", "known", "known")
layout = matrix(c(0,2,  0,1, -0.5,0, 0.5,0), byrow=TRUE, ncol=2)
nt = ggnetwork(gr, layout=layout)
grpl = ggplot(nt, aes(x = x, y = y, xend = xend, yend = yend)) + 
   geom_edges(colour="gray50", arrow=arrow(length = unit(6, "pt"), type = "closed")) + 
   theme_blank() + geom_nodes(aes(color=type, shape = source), size=6) + 
   geom_nodelabel(aes(label = name), fontface = "bold", nudge_x=-0.25) + 
   annotate(geom="label", x = 1.3, y = -0.1, label = "Hyperparameter layer") + 
   annotate(geom="label", x = 1.3, y = 0.5, label = "Parameter layer") + 
   annotate(geom="label", x = 1.3, y = 1, label = "Observation layer") + 
   xlim(-0.5, 1.5)
grpl

```



</div>

<div class="right rt">

```{stan output.var="tanks_mod", cache = TRUE}
data {
   int <lower = 1> n; // number of data points
   vector <lower=1> [n] s; // captured serial numbers
}
parameters {
   real <lower = max(s)> Nmax;
}
model {
   s ~ uniform(1, Nmax); // likelihood
   // this is a super vague prior!
   Nmax ~ gamma(0.001, 0.001);  
}
```

```{r eval = FALSE}
library(rstan)
 # the code above is saved in stan/tanks.stan
tanks_mod = stan_model("stan/tanks.stan")
```

</div>


## German Tank problem: MAP estimate

* Sometimes we are only interested in the posterior mode (i.e., the MAP)
* Can use `rstan::optimizing` to get this

```{r message=FALSE, warning=FALSE}
s = c(147, 126, 183, 88, 9, 203, 16, 10, 112, 205)
standat = list(n = length(s), s = s)
fit_map = optimizing(tanks_mod, data = standat)
fit_map$par
```

As before, the MAP is identical to `max(s)`, but many other values are possible!



## German Tank problem: Sampling

* It's clear we can learn a lot more from the entire posterior!
* The `bayesplot` package is great for visualising output from `stan`, `metropolis`, and many other Bayesian packages.

<div class="left lt">

```{r message=FALSE, cache = TRUE}
# parameter explanations:
#	iter: how many iterations (you will get iter/2 samples)
#	chains: how many parallel MCMC chains?
#	refresh: if 0, stan won't show you a progress bar (nice for rmd files)
fit = sampling(tanks_mod, data = standat, iter = 5000, chains = 1, refresh = 0)
samples = as.matrix(fit)
head(samples)

source("../r/metrop.r") ## for HDPI
rbind(hdpi = hpdi(samples[,1]), 
      quantile = quantile(samples[,1], c(0.05, 0.95)))

```


</div>

<div class="right rt">

```{r message = FALSE, fig.height=4.5, fig.width=7}
library(bayesplot)
mcmc_combo(samples, c("hist", "trace"), pars = "Nmax")
```

</div>


## More linear models

<div class="left lt">

* Our linear model used a single x-variable: $\mathbb{E}(y) = a + bx$
* It is trivial to add additional predictors to a model: $\mathbb{E}(y) = a + b_1x_1 + b_2x_2 + \dots + b_nx_n$

</div>

## More linear models

<div class="left lt">
* Our linear model used a single x-variable: $\mathbb{E}(y) = a + bx$
* It is trivial to add additional predictors to a model: $\mathbb{E}(y) = a + b_1x_1 + b_2x_2 + \dots + b_nx_n$
* You can also easily add a "curve" in the relationship between x and y by transforming x:

$$
\mathbb{E}(y) = a + b_1x + b_2x^2
$$

* Use caution: curves can predict silly things
   - does it make sense that height decreases after a certain weight?
* This is still a linear model: $\mathbb{E}(y)$ is linear with respect to transformations of x

</div>

<div class="right rt">
```{r, echo = FALSE}
fit = lm(height ~ weight + I(weight^2), data = Howell1)
pldat = data.frame(weight = seq(min(Howell1$weight), max(Howell1$weight), length.out=200))
pldat$height = predict(fit, newdata = pldat)
ggplot(Howell1, aes(x = weight, y = height)) + geom_point() + 
  geom_line(data = pldat, aes(x = weight, y = height), linewidth=1.5, col='blue') + theme_minimal()
```
</div>





## Categorical Variables

<div class="left lt">
* Perhaps we want to model a categorical variable based on age.


```{r}
Howell1$age_group = cut(Howell1$age, breaks = c(-1, 12, 22, 200), 
						labels = c("child", "young", "adult"))
Howell1
```

</div>

<div class="right rt">

```{r echo = FALSE}
pl = ggplot(Howell1, aes(x=weight, y = height, colour=age_group)) + 
          geom_point() + theme_minimal()
pl
```

</div>


## Categorical Variables

<div class="left lt">
* Perhaps we want to model a categorical variable based on age.
* `lm` and other tools often handle this automatically. 
* We can reproduce this in stan (and in custom models in R) with one of 2 methods:
	- dummy variables
	- indexing (will be shown later)
* We can represent a categorical variable with $k$ levels using $k-1$ binary (0 or 1) variables



</div>

<div class="right rt">

```{r}
Howell1$is_child = ifelse(Howell1$age_group == "child", 1, 0)
Howell1$is_young = ifelse(Howell1$age_group == "young", 1, 0)
Howell1
```

</div>

## Categorical Variables

<div class="left lt">
* Our categorical variable might represent a different **mean** for each group
	- Model this by adding something to the model intercept

```{r echo = FALSE}
mod = lm(height ~ weight + age_group, data = Howell1)

pl + geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1], col = cols[1]) + 
	geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1] + coef(mod)[3], col = cols[2]) + 
	geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1] + coef(mod)[4], col = cols[3])
```


</div>

<div class="right rt">

```{stan output.var="howell_cat", eval = FALSE}
data {
   int n; 
   vector [n] height; 
   vector [n] weight;
   vector  <lower = 0, upper = 1> [n] is_child;
   vector  <lower = 0, upper = 1> [n] is_young;
}
parameters {
   real a; // default slope (for adults)
   real a_child; // additional effect of being a child
   real a_young; // additional effect of being young
   real b;
   real <lower=0> s;
}
transformed parameters {
	vector [n] intercept = a + a_child * is_child + a_young * is_young;
	vector [n] mu = intercept + b * x;
}
model {
	y ~ normal(mu, s);
	a ~ normal(0, 10);
	a_child ~ normal(0, 10);
	a_young ~ normal(0, 10);
	b ~ normal(0, 5);
	s ~ exponential(0.1);
}
```

</div>


## Categorical Variables

<div class="left lt">
* Our categorical variable might represent a different **mean** for each group
	- Model this by adding something to the model intercept
* Or we might represent a different **rate of change** for each group (e.g., children grow more quickly)
	- Modelled by adding something to the slope term
	- Can also think of this as an **interaction** between a categorical and continuous variable


```{r echo = FALSE}
mod = lm(height ~ weight * age_group, data = Howell1)

pl + geom_abline(slope = coef(mod)[2], intercept = coef(mod)[1], col = cols[1]) + 
	geom_abline(slope = coef(mod)[2] + coef(mod)[5], intercept = coef(mod)[1] + coef(mod)[3], col = cols[2]) + 
	geom_abline(slope = coef(mod)[2] + coef(mod)[6], intercept = coef(mod)[1] + coef(mod)[4], col = cols[3])
```

</div>

<div class="right rt">

```{stan output.var="howell_cat", eval = FALSE}
data {
   int n; 
   vector [n] height; 
   vector [n] weight;
   vector  <lower = 0, upper = 1> [n]is_child;
   vector  <lower = 0, upper = 1> [n] is_young;
}
parameters {
   real a; // default slope (for adults)
   real a_child; // additional effect of being a child
   real a_young; // additional effect of being young
   real b;
   real b_child; // additional slope effect of being a child
   real b_young; // additional slope effect of being young
   real <lower=0> s;
}
transformed parameters {
	vector [n] intercept = a + a_child * is_child + a_young * is_young;
	vector [n] slope = b + b_child * is_child + b_young * is_young;
	vector [n] mu = intercept + slope * x;
}
model {
	y ~ normal(mu, s);
	a ~ normal(0, 10);
	a_child ~ normal(0, 10);
	a_young ~ normal(0, 10);
	b ~ normal(0, 5);
	b_child ~ normal(0, 5);
	b_young ~ normal(0, 5);
	s ~ exponential(0.1);
}
```

</div>

## Bayesian Analysis Workflow

