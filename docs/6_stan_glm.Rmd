

```{r setup, include=FALSE, results = "hide", cache = FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png")

library(ggplot2)
library(igraph)
library(ggplot2)
library(ggnetwork)
library(data.table)
library(rstan)



cols = c("#F8766D", "#7CAE00", "#00BFC4", "#C77CFF")

```







## Bayesian analysis workflow
1. Specify joint posterior graphically, mathematically, and in code

## Bayesian analysis workflow
1. Specify joint posterior graphically, mathematically, and in code
2. Draw samples from the joint posterior distribution

## Bayesian analysis workflow
1. Specify joint posterior graphically, mathematically, and in code
2. Draw samples from the joint posterior distribution
3. Evaluate/diagnose the model's performance

## Bayesian analysis workflow
1. Specify joint posterior graphically, mathematically, and in code
2. Draw samples from the joint posterior distribution
3. Evaluate/diagnose the model's performance
4. Perform posterior inference


## 1. Joint posterior: likelihood

* Liklihood is a **generative model** (the distribution from which the observations are generated)
  - whenever possible use knowledge of the appropriate processes
  - transform parameters and sample in transformed space to improve behaviour
  - e.g. use log($\sigma$) instead of $\sigma$ to avoid impossible negative variance in sampler
  - in Stan, this is done by adding variable **constraints** (`real <lower=0> sigma`)
  
## 1. Joint posterior: likelihood

* Liklihood is a **generative model** (the distribution from which the observations are generated)
  - whenever possible use knowledge of the appropriate processes
  - transform parameters and sample in transformed space to improve behaviour
  - e.g. use log($/sigma$) instead of $/sigma$ to avoid impossible negative variance in sampler
  - in Stan, this is done by adding variable **constraints** (`real <lower=0> sigma`)
* Models should be specified to be scale independent
  - Easy: scaling your predictors to have mean=0, sd=1
  - Hard: scaling outcomes (y); this can change the generative model!
  - Prefer to transform outcomes with the **link function**
  
## 1. Joint posterior: priors
* Any unknowns must have a **prior**
   - (or possibly a hierarchical generative model and hyperpriors)
* Prefer regularising priors to vague priors
   - Normal(0, 5) instead of Normal(0,500)
* Avoid improper priors: Uniform(-Inf, Inf)


## 1. Joint posterior: priors
* Avoid hard boundaries
   - **No**: Uniform(0, 1000)
   - **Yes**: Exponential(0.1)
* With informative priors, specify reasonable initial values
* Begin sampling with weaker priors, gradually strengthen once you know the model is working
* Specify priors for everything—avoid defaults
* Draw out your model as a digraph to make sure you don’t miss anything

## GLM in Stan

For this exercise, you will use the [birddiv](https://raw.githubusercontent.com/mtalluto/vu_advanced_statistics/main/docs/exercises/data/birddiv.csv) (in `docs/exercises/data/birddiv.csv`) dataset; you can load it directly from github using `data.table::fread()`. Bird diversity was measured in 1-km^2 plots in multiple countries of Europe, investigating the effects of habitat fragmentation and productivity on diversity. We will consider a subset of the data. Specificially, we will ask how various covariates are associated with the diversity of birds specializing on different habitat types. The data have the following potential predictors:

* **Grow.degd**: growing degree days, a proxy for how warm the climate is.
* **For.cover**: Forest cover near the sampling location
* **NDVI**: normalized difference vegetation index, a proxy for productivity
* **For.diver**: Forest diversity in the forested area nearby
* **Agr.diver**: Diversity of agricultural landscapes
* **For.fragm**: Index of forest fragmentation

All of the above variables are standardized to a 0-100 scale. Consider this when choosing priors.

Your response variable will be **richness**, the bird species richness in the plot. Additionally, you have an indicator variable **hab_type**. This is not telling you what habitat type was sampled (plots included multiple habitats). Rather, this is telling you what type of bird species were counted for the richness measurement: so `hab_type == "forest" & richness == 7` indicates that 7 forest specialists were observed in that plot.

Build one or more generalised linear models for bird richness. Your task should be to describe two things: (1) how does richness vary with climate, productivity, fragmentation, or habitat diversity, and (2) do these relationships vary depending on what habitat bird species specialize on. 


## 1. Specify joint posterior

<div class="left lt">
* We should specify a **generative model**
* Best to graph the model, ensure graph and stan code match
* Strive for independence of the scale of the x-variables
* Carefully choose priors
    - With no prior information, prefer **regularising priors**
    - Avoid priors that give probability mass to impossible values (e.g., normal(0,1) for a standard deviation)
    - Avoid flat priors or very long tails

```{r, echo=FALSE, fig.height=5, fig.width=7}
gr = graph_from_literal(richness+-"λ", "λ"+-a, "λ"+-B, "λ"+-X, a+-"μa=0", a+-"σa=10", B+-"μB=0", B+-"σB=5")
V(gr)$type = c("random", "deterministic", "random", "random", rep("deterministic", 5))
V(gr)$source = c("known", "unknown", "unknown", "unknown", rep("known", 5))
layout = matrix(c(-0.5,3,   0,2,   -0.5,1,   0.5,1,   0.5,3,   -0.7,0,   -0.3,0,   0.3,0,   0.7,0), byrow=TRUE, ncol=2)
nt = ggnetwork(gr, layout=layout)
grpl = ggplot(nt, aes(x = x, y = y, xend = xend, yend = yend)) + 
   geom_edges(colour="gray50", arrow=arrow(length = unit(6, "pt"), type = "closed")) + 
   theme_blank() + geom_nodes(aes(color=type, shape = source), size=6) + 
   geom_nodelabel(aes(label = name), fontface = "bold", nudge_x=-0.1)
grpl

```

</div>

<div class="right rt">

```{stan output.var="garbage", eval = FALSE}
transformed parameters {
	lambda = exp(a + X * B);
}
model {
	richness ~ poisson(lambda);
	a ~ normal(0, 10);
	B ~ normal(0, 5);
}
```


</div>


## 1. Specify joint posterior

<div class="left lt">
* We should specify a **generative model**
* Best to graph the model, ensure graph and stan code match
* Strive for independence of the scale of the x-variables
* Carefully choose priors
    - With no prior information, prefer **regularising priors**
    - Avoid priors that give probability mass to impossible values (e.g., normal(0,1) for a standard deviation)
    - Avoid flat priors or very long tails

```{r, echo=FALSE, fig.height=5, fig.width=7}
grpl

```

</div>

<div class="right rt">

```{stan output.var="bird_glm", cache=TRUE}
data {
	int <lower=0> n; // number of data points
	int <lower=0> k; // number of x-variables
	int <lower=0> richness [n];
	matrix [n,k] X;
}
parameters {
	real a;
	vector [k] B;
}
transformed parameters {
	vector <lower=0> [n] lambda;
	lambda = exp(a + X * B);
}
model {
	richness ~ poisson(lambda);
	a ~ normal(0, 10);
	B ~ normal(0, 5);
}
generated quantities {
	int r_predict [n];
	for(i in 1:n)
		r_predict[i] = poisson_rng(lambda[i]);
	r_predict = poisson_rng(lambda);
}
```

```{r eval = FALSE}
bird_glm = stan_model("stan/bird_glm.stan")
```


</div>


## 2. Sample from joint posterior

* Start with a small run (10s-100s of iterations) to catch errors, make sure nothing surprising happens
* Increase to a few thousand to view convergence
* Select starting values and run until convergence (1.000s for Stan, 10.000s—100.000s or more for Metropolis)


## 2. Sample from the joint posterior

<div class="left lt">

```{r, message=FALSE, cache = TRUE}
library(data.table)
birds = fread("exercises/data/birddiv.csv")

# stan can't handle NAs
birds = birds[complete.cases(birds)]

# original variables scaled from 0-100; rescale to go from 0 to 1
X_scaled = as.matrix(birds[,c(2:7)])/100

# I will try two models
# First, simple model, only forest birds in relation to forest cover
for_i = which(birds$hab_type == "forest")
standat1 = list(
	n = length(for_i), 
	k = 1,
	richness = birds$richness[for_i],
	X = X_scaled[for_i, "NDVI", drop=FALSE])

fit1 = sampling(bird_glm, data = standat1, iter=2000, 
   chains = 4, refresh = 0)

```

</div>

<div class="right rt">

```{r, message=FALSE, cache = TRUE}
# Second, looking at how two variables influence birds of different types

# grab two variables
X = X_scaled[, c("For.cover", "NDVI")]

# add a categorical variable for bird type
X = cbind(X, open=ifelse(birds$hab_type == "open",1, 0))
X = cbind(X, generalist=ifelse(birds$hab_type == "generalist",1, 0))

# add interaction terms with the categories
X = cbind(X, op_forCov=X[,1]*X[,3], op_NDVI=X[,1]*X[,4], 
   ge_forCov=X[,2]*X[,3], ge_NDVI=X[,2]*X[,4])

head(X)

standat2 = with(birds, list(
	n = length(richness), 
	k = ncol(X),
	richness = richness,
	X = X))
fit2 = sampling(bird_glm, data = standat2, iter=2000, 
   chains = 4, refresh = 0)
```


</div>



## 3. Evaluate the model fit

<div class="left lt">
* Traceplots can tell you about model convergence and efficiency
* Histograms can alert you to problems with multi-modality
* Run multiple chains to help with diagnostics

```{r message = FALSE, fig.width=7}
## Use as.array if you want to keep different mcmc chains separate
## This is ideal for diagnostics
## For inference, you usually want to lump all chains
## In this case, you use as.matrix
samp1_pars = as.array(fit1, pars=c('a', 'B'))
mcmc_combo(samp1_pars, c("hist", "trace"))

```

</div>

<div class="right rt">

* Printing the model also gives useful metrics
* Can filter by parameters of interest
* n_eff: Effective sample size (after removing autocorrelation)
    - This gives you an indication of how much precision in the tails of the posterior you have
* Rhat: convergence diagnostic, available with multiple chains
    - ideally, Rhat = 1
    - Worry about for "real" parameters (not hierarchical, not deterministic)
    - Rhat > 1.1 for a real parameter is a problem
    - Rhat < 1.05 is probably ok

```{r, message=FALSE}
print(fit1, pars = c('a', 'B'))
```

</div>



## 4. Inference: parameter estimates

<div class="left lt">
```{r message = FALSE, fig.width=5}
mcmc_intervals(samp1_pars)
```
</div>

<div class="right rt">
```{r message = FALSE, fig.width=7}
samp2_pars = as.array(fit2, pars=c('a', 'B'))
mcmc_intervals(samp2_pars)
```
</div>



## 4. Inference: Retrodiction

<div class="left lt">
* How close is the model to the original data?
* How well does our generative model describe the data

```{r message = FALSE, fig.width=9, echo=FALSE}
library(gridExtra)
# this gives us posterior samples for:
#    -lambda (the expected value of richness)
#    -r_predict (posterior predictive simulations)
# for each original data point
samp1_pr = as.matrix(fit1, pars=c('lambda', 'r_predict'))

# now we compute the median and 90% quantiles for lambda (the expected value)
# and for r (the posterior predictive sim for richness) for each data point
samp1_intervals = apply(samp1_pr, 2, quantile, c(0.5, 0.05, 0.9))


## now we reshape these a bit to get them into a data frame for visualisation
## want everything alongside the original data
pldat1 = data.table(standat1$X)
pldat1 = cbind(pldat1, data.table(
	richness = standat1$richness,
	lambda = samp1_intervals[1, grep("lambda", colnames(samp1_intervals))],
	lambda_l = samp1_intervals[2, grep("lambda", colnames(samp1_intervals))],
	lambda_u = samp1_intervals[3, grep("lambda", colnames(samp1_intervals))],
	rpr = samp1_intervals[1, grep("r_predict", colnames(samp1_intervals))],
	rpr_l = samp1_intervals[2, grep("r_predict", colnames(samp1_intervals))],
	rpr_u = samp1_intervals[3, grep("r_predict", colnames(samp1_intervals))]))
pldat1 = pldat1[order(NDVI)]

pl_left = ggplot(pldat1) + geom_ribbon(aes(x=NDVI, ymin=rpr_l, ymax=rpr_u), fill=cols[2], alpha=0.3) + 
	geom_ribbon(aes(x=NDVI, ymin=lambda_l, ymax=lambda_u), fill=cols[3], alpha=0.5) + 
	geom_line(aes(x=NDVI, y=lambda), col=cols[3]) + 
	geom_point(aes(x = NDVI, y = richness)) + 
	theme_minimal() + ylab("Forest bird richness")

pl_right = ggplot(pldat1) + xlab("Observed Richness") + 
	geom_errorbar(aes(x=richness, ymin=rpr_l, ymax=rpr_u), linewidth=1.5, col=cols[2], width=0, alpha=0.3) + 
	geom_errorbar(aes(x=richness, ymin=lambda_l, ymax=lambda_u), col=cols[3], width=0, alpha=0.6) + 
	geom_point(aes(x=richness, y=lambda)) +
	geom_abline(intercept=0, slope=1, lty=2) + theme_minimal() + xlim(0, 23) + ylim(0, 23)

grid.arrange(pl_left, pl_right, ncol=2, nrow=1)
```
</div>

<div class="right rt">

```{r message = FALSE, fig.width=7}
# compute posterior distribution of the residual sum of squares
samp1_lam = as.matrix(fit1, pars='lambda')
sq_resid1 = apply(samp1_lam, 1, function(x) (standat1$richness - x)^2)

# compute posterior distribution of dispersion parameter, which is just 
# sum(squared residuals)/(n - k)
# here k is 2, we have an intercept and one slope
# if phi > 1, we have overdispersion and need a better model
phi = apply(sq_resid1, 2, function(x) sum(x)/(length(x) - 2))
quantile(phi, c(0.5, 0.05, 0.95))

```
</div>





## 4. Inference: Improve the model

<div class="left lt">

```{r message = FALSE, fig.width=9, echo=FALSE}
library(gridExtra)
# this gives us posterior samples for:
#    -lambda (the expected value of richness)
#    -r_predict (posterior predictive simulations)
# for each original data point
samp2_pr = as.matrix(fit2, pars=c('lambda', 'r_predict'))

# now we compute the median and 90% quantiles for lambda (the expected value)
# and for r (the posterior predictive sim for richness) for each data point
samp2_intervals = apply(samp2_pr, 2, quantile, c(0.5, 0.05, 0.9))


## now we reshape these a bit to get them into a data frame for visualisation
## want everything alongside the original data
pldat2 = data.table(standat2$X)
pldat2 = cbind(pldat2, data.table(
	richness = standat2$richness,
	lambda = samp2_intervals[1, grep("lambda", colnames(samp2_intervals))],
	lambda_l = samp2_intervals[2, grep("lambda", colnames(samp2_intervals))],
	lambda_u = samp2_intervals[3, grep("lambda", colnames(samp2_intervals))],
	rpr = samp2_intervals[1, grep("r_predict", colnames(samp2_intervals))],
	rpr_l = samp2_intervals[2, grep("r_predict", colnames(samp2_intervals))],
	rpr_u = samp2_intervals[3, grep("r_predict", colnames(samp2_intervals))]))

## add an indicator as to what kind of bird we are talking about
pldat2[, bird := ifelse(open == 1, "open", ifelse(generalist == 1, "generalist", "forest"))]


ggplot(pldat2) + xlab("Observed Richness") + 
	geom_errorbar(aes(x=richness, ymin=rpr_l, ymax=rpr_u), size=1.5, col=cols[4], width=0, alpha=0.3) + 
	geom_errorbar(aes(x=richness, ymin=lambda_l, ymax=lambda_u), width=0, alpha=0.6) + 
	geom_point(aes(x=richness, y=lambda, colour = bird)) +
	facet_grid(.~bird) + 
	geom_abline(intercept=0, slope=1, lty=2) + theme_minimal() + xlim(0, 23) + ylim(0, 23) + 
	labs(colour="Type of bird") 

```
</div>

<div class="right rt">

```{r message = FALSE, fig.width=7}
# compute posterior distribution of the residual sum of squares
samp2_lam = as.matrix(fit2, pars='lambda')
sq_resid2 = apply(samp2_lam, 1, function(x) (standat2$richness - x)^2)

# compute posterior distribution of dispersion parameter, which is just 
# sum(squared residuals)/(n - k)
# here k is 2, we have an intercept and one slope
# if phi > 1, we have overdispersion and need a better model
phi = apply(sq_resid2, 2, function(x) sum(x)/(length(x) - 2))
quantile(phi, c(0.5, 0.05, 0.95))

```

* This model is still quite overdispersed
   - Consider more variables
   - Consider other likelihoods (e.g., Negative Binomial)

</div>





## 4. Inference: Partial Response Curves

* How does richness respond to the individual variables, holding other variables constant?

```{r, message=FALSE, echo = FALSE, fig.width=10}
## generate a dataset to predict a line for all combinations
xx = seq(-1.8,1.8, length.out = 200)
predict_dat = rbind(data.table(forcover = xx, ndvi = 0, open=0, gen = 0, op_fc = 0, op_nd = 0, ge_fc = 0, ge_ndvi = 0), 
			data.table(forcover = xx, ndvi = 0, open=1, gen = 0, op_fc = xx, op_nd = 0, ge_fc = 0, ge_ndvi = 0),
			data.table(forcover = xx, ndvi = 0, open=0, gen = 1, op_fc = 0, op_nd = 0, ge_fc = xx, ge_ndvi = 0),
			data.table(forcover = 0, ndvi = xx, open=0, gen = 0, op_fc = 0, op_nd = 0, ge_fc = 0, ge_ndvi = 0),
			data.table(forcover = 0, ndvi = xx, open=1, gen = 0, op_fc = 0, op_nd = 1, ge_fc = 0, ge_ndvi = 0),
			data.table(forcover = 0, ndvi = xx, open=0, gen = 1, op_fc = 0, op_nd = 0, ge_fc = 0, ge_ndvi = 1))
predict_dat = cbind(intercept=1, predict_dat)

## this computes a posterior distribution for E(y) | predict_dat
## in other words, for the x-values we have chosen for visualisation, what
## is the distribution of the average of y at those x-values
y = exp(as.matrix(fit2, pars = c('a', 'B')) %*% t(as.matrix(predict_dat)))

## Compute median and 90% quantile interals for E(y) and add to the data frame for ggplot
predict_dat$y_med = apply(y, 2, median)
predict_dat$y_upper = apply(y, 2, quantile, 0.95)
predict_dat$y_lower = apply(y, 2, quantile, 0.05)

## Create some nice labels for ggplot
predict_dat$bird = with(predict_dat, ifelse(open == 1, "open", ifelse(gen == 1, "generalist", "forest")))
predict_dat$panel = "Forest Cover | NDVI=0"
predict_dat$panel[601:1200] = "NDVI | Forest Cover=0"

## Create a combined x-variable for ggplot; this works because forcover
## and ndvi are transformed to mean = 0, and the predictions are conditional; 
## if one variable is nonzero, the other must be zero
predict_dat$x = predict_dat$forcover + predict_dat$ndvi

ggplot(predict_dat, aes(x=x, y=y_med, col=bird)) + 
	geom_ribbon(aes(x=x, ymin=y_lower, ymax=y_upper, fill=bird), alpha=0.5) + 
	geom_line() + facet_grid(.~panel) + 
	theme_minimal() + ylab("Species Richness") + xlab(expression(sigma)) + 
	labs(fill="Type of bird", colour="Type of bird") + 
	xlim(-1.8, 1.8)

```


## 4. Inference: Response Surfaces
```{r, message=FALSE, echo = FALSE, fig.width=15}
xy = data.table(expand.grid(forcover=seq(0,1, length.out = 50), ndvi=seq(0,1, length.out = 50)))
pr_for = cbind(intercept=1, xy, open=0, gen = 0, op_fc = 0, op_nd = 0, ge_fc = 0, ge_ndvi = 0)
pr_for$y = apply(exp(as.matrix(fit2, pars = c('a', 'B')) %*% t(as.matrix(pr_for))), 2, median)

pr_open = cbind(intercept=1, xy, open=1, gen = 0, op_fc = xy$forcover, op_nd = xy$ndvi, ge_fc = 0, ge_ndvi = 0)
pr_open$y = apply(exp(as.matrix(fit2, pars = c('a', 'B')) %*% t(as.matrix(pr_open))), 2, median)

pr_gen = cbind(intercept=1, xy, open=0, gen = 1, op_fc = 0, op_nd = 0, ge_fc = xy$forcover, ge_ndvi = xy$ndvi)
pr_gen$y = apply(exp(as.matrix(fit2, pars = c('a', 'B')) %*% t(as.matrix(pr_gen))), 2, median)

pts = data.table(standat2$X)
pts$richness = standat2$richness
pts[, type := ifelse(open==1, "open", ifelse(generalist==0, "generalist", "forest"))]


forpl = ggplot(pr_for, aes(x=forcover, y=ndvi, fill=y)) + geom_tile() + guides(fill="none") +
	scico::scale_fill_scico(palette = "bilbao", limits=c(0,12)) + theme_minimal() + 
	xlab("Forest Cover") + ylab("NDVI") + labs(fill="Predicted richness") + ggtitle("Forest Birds") + 
	geom_point(data=pts[type == "forest"], aes(x=For.cover, y=NDVI, size=richness), fill="white", alpha=0.5) +
	guides(size="none") + scale_size(limits=c(0, 20), range=c(0.1, 7))
openpl = ggplot(pr_open, aes(x=forcover, y=ndvi, fill=y)) + geom_tile() + guides(fill="none") +
	scico::scale_fill_scico(palette = "bilbao", limits=c(0,12)) + theme_minimal() + 
	xlab("Forest Cover") + ylab("") + labs(fill="Predicted richness") + ggtitle("Open Birds") + 
	geom_point(data=pts[type == "open"], aes(x=For.cover, y=NDVI, size=richness), fill="white", alpha=0.5) +
	guides(size="none") + scale_size(limits=c(0, 20), range=c(0.1, 4))
genpl = ggplot(pr_gen, aes(x=forcover, y=ndvi, fill=y)) + geom_tile() + 
	scico::scale_fill_scico(palette = "bilbao", limits=c(0,12)) + theme_minimal() + 
	xlab("Forest Cover") + ylab("") + labs(fill="Predicted richness") + ggtitle("Generalist Birds") + 
	geom_point(data=pts[type == "generalist"], aes(x=For.cover, y=NDVI, size=richness), fill="white", alpha=0.5) +
	guides(size="none") + scale_size(limits=c(0, 20), range=c(0.1, 4))

grid.arrange(forpl, openpl, genpl, nrow=1, ncol=3, widths=c(1,1,1.3))
```



