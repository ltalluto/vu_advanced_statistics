---
title: "Hierarchical Models"
author: "Matthew Talluto"
date: "26.11.2020"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
  beamer_presentation: default
---

```{r setup, include=FALSE, results = "hide"}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png")

library(rethinking)
library(ggplot2)
library(igraph)
library(ggnetwork)
library(data.table)
library(gridExtra)

cols = c("#F8766D", "#7CAE00", "#00BFC4", "#C77CFF")

```

## No pooling, full pooling

<div class="left lt">
* 48 experimental ponds
* 3 densities of tadpoles
* experimental addition of predators
* large or small tadpoles

<br/>
**First question**:

What is the average survival (ignoring treatments)?

* We can estimate survival with pooling (one average across all ponds)
* Alternatively, we can use no pooling (separate average across all ponds)
</div>

<div class="right rt">
```{r}
frogs = fread("exercises/data/frogs.csv")
head(frogs)
```


</div>


## Three possible models for density


```{r, echo=FALSE, fig.height=5, fig.width=18, fig.height=8}
gr = graph_from_literal(surv+-p, surv+-N, p+-a, a+-"μ=0", a+-"σ=10")
V(gr)$type = c("random", "deterministic", "deterministic", "random", "deterministic", "deterministic")
V(gr)$source = c("known", "unknown", "known", "unknown", "known", "known")
layout = matrix(c(0,2,  0,1,  1,2,  1,1,  0.5,0, 1.5,0), byrow=TRUE, ncol=2)
nt = ggnetwork(gr, layout=layout)
grpl1 = ggplot(nt, aes(x = x, y = y, xend = xend, yend = yend)) + 
   geom_edges(colour="gray50", arrow=arrow(length = unit(6, "pt"), type = "closed")) + 
   theme_blank() + geom_nodes(aes(color=type, shape = source), size=6) + 
   geom_nodelabel(aes(label = name), fontface = "bold", nudge_x=-0.15, nudge_y=0.1) + 
   annotate(geom="label", x = 1.3, y = -0.1, label = "Hyperparameter layer") + 
   annotate(geom="label", x = 1.3, y = 0.5, label = "Parameter layer") + 
   annotate(geom="label", x = 1.3, y = 1, label = "Observation layer") + 
   xlim(-0.5, 1.5) + ggtitle("Complete Pooling") + guides(color=FALSE, shape=FALSE)


gr2 = graph_from_literal(surv+-"p1,p2...p48", surv+-N, "p1,p2...p48"+-"a1,a2...a48", "a1,a2...a48"+-"μ=0", "a1,a2...a48"+-"σ=10")
V(gr2)$type = c("random", "deterministic", "deterministic", "random", "deterministic", "deterministic")
V(gr2)$source = c("known", "unknown", "known", "unknown", "known", "known")
layout = matrix(c(0,2,  0,1,  1,2,  1,1,  0.5,0, 1.5,0), byrow=TRUE, ncol=2)
nt2 = ggnetwork(gr2, layout=layout)
grpl2 = ggplot(nt2, aes(x = x, y = y, xend = xend, yend = yend)) + 
   geom_edges(colour="gray50", arrow=arrow(length = unit(6, "pt"), type = "closed")) + 
   theme_blank() + geom_nodes(aes(color=type, shape = source), size=6) + 
   geom_nodelabel(aes(label = name), fontface = "bold", nudge_x=-0.15, nudge_y=0.1) + 
   annotate(geom="label", x = 1.3, y = -0.1, label = "Hyperparameter layer") + 
   annotate(geom="label", x = 1.3, y = 0.5, label = "Parameter layer") + 
   annotate(geom="label", x = 1.3, y = 1, label = "Observation layer") + 
   xlim(-0.5, 1.5) + ggtitle("No Pooling")

grid.arrange(grpl1, grpl2, nrow=1, ncol=2)
```





## Full vs no pooling

<div class="left lt">

```{stan output.var="frog1_pool", cache = TRUE}
data {
	int <lower=0> n; // number of data points
	int <lower=0> surv [n]; // number surviving
	int <lower=0> tadpole [n]; // number of tadpoles in the pond
}
parameters {
   real a;
}
transformed parameters {
   real <lower=0, upper=1> p;
   p = inv_logit(a);
}
model {
	surv ~ binomial(tadpole, p);
	a ~ normal(0, 10);
}
generated quantities {
   vector <lower=0, upper=1> [n] pr_surv_sim;
   for (i in 1:n) {
      pr_surv_sim[i] = binomial_rng(tadpole[i], p) / (1.0 * tadpole[i]);
   }
}
```
</div>

<div class="right rt">
```{stan output.var="frog1_nopool", cache = TRUE}
data {
	int <lower=0> n; // number of data points
	int <lower=0> surv [n]; // number surviving
	int <lower=0> tadpole [n]; // number of tadpoles in the pond
}
parameters {
   vector [n] a;
}
transformed parameters {
   vector <lower=0, upper=1> [n] p;
   p = inv_logit(a);
}
model {
	surv ~ binomial(tadpole, p);
	a ~ normal(0, 10);
}
generated quantities {
   vector <lower=0, upper=1> [n] pr_surv_sim;
   for (i in 1:n) {
      pr_surv_sim[i] = binomial_rng(tadpole[i], p[i]) / (1.0 * tadpole[i]); // 1.0 converts to floating point
   }
}
```
</div>


## Pooling tradeoffs

<div class="left lt">

```{r message = FALSE}
library(bayesplot)
standat = list(n = nrow(frogs), surv = frogs$surv, tadpole = frogs$tadpole)
fit_pool = sampling(frog1_pool, data = standat, iter=2000, refresh=0, chains=1)
samples_pool = as.matrix(fit_pool)
mcmc_intervals(samples_pool, pars="p") + xlim(c(0,1))
```
</div>

<div class="right rt">
```{r message = FALSE}
fit_nopool = sampling(frog1_nopool, data = standat, iter=2000, refresh=0, chains=1)
samples_nopool = as.matrix(fit_nopool, pars="p")
mcmc_intervals(samples_nopool) + xlim(0,1)

```
</div>






## Partial Pooling

<div class="left lt">
* Imagine instead that there is a population of possible ponds
* We have sampled some of them
* This population has a true mean and a true variance
* The sample ponds we've drawn will come from that distribution
* This can tell us something about all possible ponds, not just these ponds

```{r, echo=FALSE, fig.height=5, fig.width=6, fig.height=6}
gr3 = graph_from_literal(surv+-"p1,p2...p48", surv+-N, "p1,p2...p48"+-"a1,a2...a48", 
                         "a1,a2...a48"+-"μa", "a1,a2...a48"+-"σa", "μa"+-"μ=0", "μa"+-"σ=10",
                         "σa"+-"λ=0.2")
V(gr3)$type = c("random", "deterministic", "deterministic", 
                "random", "random", "random", "deterministic", "deterministic", "deterministic")
V(gr3)$source = c("known", "unknown", "known", "unknown", "unknown", "unknown", "known", "known", "known")
layout = matrix(c(0,2,  0,1,  1,2,  1,1,  0.5,0, 1.5,0,  0.25,-1,  0.75,-1,  1.5,-1), byrow=TRUE, ncol=2)
nt3 = ggnetwork(gr3, layout=layout)
grpl3 = ggplot(nt3, aes(x = x, y = y, xend = xend, yend = yend)) + 
   geom_edges(colour="gray50", arrow=arrow(length = unit(6, "pt"), type = "closed")) + 
   theme_blank() + geom_nodes(aes(color=type, shape = source), size=6) + 
   geom_nodelabel(aes(label = name), fontface = "bold", nudge_x=-0.15, nudge_y=0.1) + 
   annotate(geom="label", x = 1.3, y = 0, label = "Hyperprior layer") + 
   annotate(geom="label", x = 1.3, y = 0.33, label = "Hyperparameter layer") + 
   annotate(geom="label", x = 1.3, y = 0.67, label = "Parameter layer") + 
   annotate(geom="label", x = 1.3, y = 1, label = "Observation layer") + 
   xlim(-0.5, 1.5) + ggtitle("Partial Pooling")
grpl3

```

</div>

<div class="right rt">
```{stan output.var="frog1_ppool", cache = TRUE}
data {
	int <lower=0> n; // number of data points
	int <lower=0> surv [n]; // number surviving
	int <lower=0> tadpole [n]; // number of tadpoles in the pond
}
parameters {
   vector [n] a;
   
   // hyperparameters
   real mua;
   real siga;
}
transformed parameters {
   vector <lower=0, upper=1> [n] p;
   p = inv_logit(a);
}
model {
	surv ~ binomial(tadpole, p);
	a ~ normal(mua, siga);
}
generated quantities {
   vector <lower=0, upper=1> [n] pr_surv_sim;
   for (i in 1:n) {
      pr_surv_sim[i] = binomial_rng(tadpole[i], p[i]) / (1.0 * tadpole[i]);
   }
}
```
</div>











##

```{stan output.var="stan_hm", eval = FALSE}
data {
	int <lower=0> n; // number of data points
	int <lower=0> k; // number of x-variables
	int <lower=0> richness [n];
	matrix [n,k] X;

   // hierarchical data objects
	int <lower=1> np; // number of plots
	int <lower=1, upper=np> plotid [n];
}
parameters {
	vector [np] a; // one intercept per plot
	vector [k] B;
	
	// hyperparameters
	real amu;
	real <lower=0> asig;
}
transformed parameters {
	vector <lower=0> [n] lambda;
	vector [n] lpart = X * B;
	for(i in 1:n) {
	   lambda[i] = exp(a[plotid[i]] + lpart[i]); // .* is elementwise multiplication
	}
}
model {
	richness ~ poisson(lambda);
	a ~ normal(amu, asig);  // hierarchical prior on a
	B ~ normal(0, 5);
	
	// hyperpriors
	amu ~ normal(0, 10);
	asig ~ cauchy(0, 0.1);
}
generated quantities {
   vector[n] log_lik;
   for (i in 1:n) {
      log_lik[i] = poisson_lpmf(richness[i] | lambda[i]);
   }
}
```





## When do we need HM?
* repeated sampling within units
* Inference at multiple levels of organisation
* Uneven sampling among units
* Accounting for nonindependence of samples
* Avoiding pre-averaging (e.g., sample 5 times, do inference on the average)
* A special category of HM is often called mixed modeling; a HBM can replace a mixed model in all cases


## Pooling
* complete pooling: estimate a single parameter for all data.
   - the most data for a single estimate
   - can be quite precise
   - high prediction error, because it predicts the same thing for all groups
   - lowest MEAN prediction error for new groups 
* no pooling: independent estimates for each group
   - the least data per estimate
   - very imprecise, especially for groups with few samples
   - lowest prediction error for the sampled groups
   - cannot predict to new groups at all
* partial pooling (HM): estimates for each group is regularized using info from all groups
   - for a given group, we combine the information for this group with information from all groups
   - there is an implicit change of scale; the signal gets weaker (so within-group data is stronger than among)
   - balances precision and accuracy
   - can be VERY good for new groups, espeically if there is a group-level model as well

## Concepts to touch on

* "Borrowing strength"
* 


## HOW to do this
* go over the indexing approach
* this means it maybe makes more sense to intro stan first


## Working with posterior samples from HBMs
* Note sec 12.4.1 about how model predictions won't predict as well to observations (shrinkage, balacing accuracy and precision)
* Note also that posterior prediction gets harder. Do we want to predict new observations from a known group? Or new observations from an unknown group?
