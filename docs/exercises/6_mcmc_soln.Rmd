---
title: "MCMC Exercises"
date: "11.11.2021"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


1. **Optional** (but recommended). Implement your own Metropolis algorithm for generating MCMC samples. See the bottom of this page for some scaffolding code to get you started. Fill in the appropriate places with the code for making the algorithm work. You can test the code using any of the log_posterior functions we've written so far (this gives you a chance to compare the results when you know the answer), or you can try it out using exercise number 2.

2. Recall the German tank problem presented in lecture. Use the following captured serial numbers:

```{r}
s = c(147, 126, 183, 88, 9, 203, 16, 10, 112, 205)
```

Your goal is to estimate a single parameter, $N_{max}$, the highest possible serial number (indicating the number of tanks actually produced).

a. Write a log likelihood function that computes $\log pr(s | N_{max})$. What distribution is appropriate? Assume that the observations in $s$ are independent and random; $s$ can be any number between 1 and $N_{max}$, and the probability of any number in that range being observed is the same as for any other number.
b. Write a log prior for $N_{max}$, and combine it with the log likelihood into a log posterior function.

```{r}
#' log posterior density
#' @param params The parameter vector for the model
#' @param data A list of data needed to evaluate the model
#' @return The log unnormalized posterior probability of the parameters conditional on the data
log_posterior = function(params, data) {
	# here we 'unpack' the objects to make the names clearer
	Nmax = params[1]
	s = data[['s']]

	# the sampler sometimes selects impossible values for Nmax, need to rule them out
	if(Nmax < 1) {
		return(-Inf)
	}

	log_liklihood = sum(dunif(s, 1, Nmax, log=TRUE))
	log_prior = dgamma(Nmax, 0.001, 0.001, log = TRUE)  ## this is a super vague prior!
	# log_prior = 0
	return(log_liklihood + log_prior)
}
```

c. This posterior distribution cannot be estimated using Laplace approximation. However you can still use optim to get a MAP estimate. Get the MAP estimate, then plot the log_posterior against many values of $N_{max}$. Does your MAP estiamte make sense?

```{r, warning = FALSE}
# the max of s is a reasonable starting value
# don't be concerned about NA/Inf warnings, but do consider why they happen
fit = optim(max(s), log_posterior, data = list(s = s), 
            method = "Brent", control=list(fnscale=-1), lower=1, upper=500)

# check convergence
fit$covergence == 0

fit$par

nplt = 2:300
y = exp(sapply(nplt, log_posterior, data = list(s = s)))
plot(nplt, y, type='l', xlab=expression(N_max), ylab="pr(N|s)", bty='n', xlim=c(180, 300))
points(fit$par, exp(fit$value), pch=21, bg='#00BFC4', cex=1.4)
```



3. Use a metropolis sampler to generate 20000 samples from the posterior distribution for the German tank problem. You can either use the sampler you wrote for question 1, or use one provided for you. It can be found in `r/metrop.r` in this repository.
   a. Play with the scale parameter in the algorithm for short runs (2000 iterations is plenty). How does changing the scale influence the acceptance rate? Either use automatic adaptation, or play with the scale manually until you have an acceptance rate between 0.35 and 0.5. Make traceplots for some attempts with small, good, and large scales
   
```{r message=FALSE}
source("../../r/metrop.r")
library(bayesplot, quietly=TRUE)

test1 = metropolis(log_posterior, initial = max(s), data = list(s = s), iter = 2000, scale = 0.2)
test2 = metropolis(log_posterior, initial = max(s), data = list(s = s), iter = 2000, scale = 20)
test3 = metropolis(log_posterior, initial = max(s), data = list(s = s), iter = 2000, scale = 200)
c(test1$accept, test2$accept, test3$accept)

mcmc_trace(matrix(test1$chain, ncol=1, dimnames=list(NULL, 'Nmax1')))
mcmc_trace(matrix(test2$chain, ncol=1, dimnames=list(NULL, 'Nmax2')))
mcmc_trace(matrix(test3$chain, ncol=1, dimnames=list(NULL, 'Nmax3')))


# For the real sampler, we will let the automatic adaptation work
samples = metropolis(log_posterior, initial = max(s), data = list(s = s), 
                   iter = 20000, scale = NA)
```
   
b. Evaluate your samples using `mcmc_trace()` and `mcmc_hist()` from the `bayesplot` package (or implement your own versions). Compare the histogram of samples to the posterior density plot you made in 2c.
   
```{r}
# reshape the samples to work with bayesplot
sample_mat = matrix(samples$chain, ncol=1)
colnames(sample_mat) = "Nmax"
mcmc_combo(sample_mat, c("hist", "trace"))

```
   
   
   c. Generate a 90% highest posterior density interval (hpdi). A function for this is in `r/metrop.r`. Compare these credible intervals with the usual quantile intervals we have computed thus far. If they are meaningfully different, why?

```{r}
rbind(hdpi = hpdi(samples$chain), 
      quantile = quantile(samples$chain, c(0.05, 0.95)))
```

