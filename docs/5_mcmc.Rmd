---
title: "Markov Chain Monte Carlo"
author: "Matthew Talluto"
date: "25.11.2022"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
  beamer_presentation: default
---


```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png")
```

## Laplace approximation isn't always effective

<div class="left lt">
* Complex models (e.g., hierarchical models)
   - Model height as a function of weight
   - People from rich countries are taller on average
</div>

<div class="right rt">
$$
\begin{aligned}
\bar{h}_{country} & \sim \mathcal{N}(A + B \times income_{country}, \Sigma) \\
h & \sim \mathcal{N}(\bar{h}_{country} + b \times weight, \sigma)
\end{aligned}
$$
</div>

## Laplace approximation isn't always effective
<div class="left lt">
* Complex models (e.g., hierarchical models)
   - Model height as a function of weight
   - People from rich countries are taller on average
* Poorly identified models
  - correlated predictors
  - very flat posterior distributions
  - small sample sizes
  - very weak priors
</div>

<div class="right rt">
$$
\begin{aligned}
\bar{h}_{country} & \sim \mathcal{N}(A + B \times income_{country}, \Sigma) \\
h & \sim \mathcal{N}(\bar{h}_{country} + b \times weight, \sigma)
\end{aligned}
$$
</div>



## Laplace approximation isn't always effective
<div class="left lt">
* Complex models (e.g., hierarchical models)
   - Model height as a function of weight
   - People from rich countries are taller on average
* Poorly identified models
  - correlated predictors
  - very flat posterior distributions
  - small sample sizes
  - very weak priors
* Highly skewed distributions
</div>

<div class="right rt">
$$
\begin{aligned}
\bar{h}_{country} & \sim \mathcal{N}(A + B \times income_{country}, \Sigma) \\
h & \sim \mathcal{N}(\bar{h}_{country} + b \times weight, \sigma)
\end{aligned}
$$
</div>




## The German Tank Problem

<div class="left lt">

* During WW2, the Allies wanted to know the rate of tank production in German factories
* Known: factories stamped serial numbers on the tanks in order, starting with 1
* The problem: given a set of captured serial numbers $s$, estimate $N$, the maximum serial number
</div>

<div class="right rt">

```{r echo = FALSE}
s = 200
Npr = 2:2400
par(mgp=c(0.5,0,0), mar=c(2,2,0.2,0.2))
plot(Npr, dunif(s, 1, Npr), type='l', bty='l', lwd=2, col='#00BFC4', xaxt='n', xlab="N", yaxt='n', ylab="pr(s | N)")
pts = c(s, 2*s, 4*s, 10*s)
points(pts, dunif(s, 1, pts), pch=21, cex=1.8, bg='#F8766D')
text(pts, dunif(s, 1, pts), c("s", "2s", "4s", "10s"), pos=4)
```
</div>



## Rejection sampling

<div class="left lt">
* There are general algorithms for sampling from unknown distributions
* We have the **target distribution**, $t(N) = pr(s|N)$
* Find an easy-to-sample **proposal distribution**, call it $f(N)$, 
    - Scale it so that it's always higher than our target
    - Uniform is an obvious choice here, with a very high maximum: e.g., `dunif(s, 1, 1e6)`
    - This is a kind of prior - we are saying that it's impossible they made more than a million tanks
</div>
<div class="right rt">
```{r echo = FALSE}
ymax = dunif(s, 1, s)
scale = ymax/dunif(s, 1, 1e6)
par(mgp=c(0.5,0,0), mar=c(2,2,2,0.2))
plot(Npr, dunif(s, 1, Npr), type='l', bty='l', lwd=2, col='#00BFC4', xaxt='n', xlab="N", yaxt='n', ylab="pr(s | N)")
prop = dunif(Npr, 1, 1e6) * scale
lines(Npr, prop, lwd=2, col='#F8766D')
legend("right", legend=c("proposal", "target"), lwd=2, col=c('#F8766D', '#00BFC4'), bty='n')
```
</div>





## Rejection sampling

<div class="left lt">
* There are general algorithms for sampling from unknown distributions
* We have the **target distribution**, $t(N) = pr(s|N)$
* Find an easy-to-sample **proposal distribution**, call it $f(N)$
* Draw a sample $x$ from the proposal distribution, and compute an **acceptance probability**, $r = \frac{t(x)}{f(x)}$
* Accept the sample with probability $r$, reject with probability $1-r$
</div>
<div class="right rt">

```{r echo = FALSE}
par(mgp=c(0.5,0,0), mar=c(2,2,2,0.2))
plot(Npr, dunif(s, 1, Npr), type='l', bty='l', lwd=2, col='#00BFC4', xaxt='n', xlab="N", yaxt='n', ylab="pr(s | N)")
lines(Npr, prop, lwd=2, col='#F8766D')
legend("right", legend=c("proposal", "target"), lwd=2, col=c('#F8766D', '#00BFC4'), bty='n')
x = 596; y = dunif(s, 1, x) * 0.2
segments(x, 0, x, scale*dunif(x, 1, 1e6), col="#F8766D", lty=2, lwd=1.5)
segments(x, 0, x, dunif(s, 1, x), col="#00BFC4", lty=3, lwd=1.5)
points(x, y, pch=21, bg="#C77CFF", cex=2)
text(x, y, "x", pos=4)
text(x+50, grconvertY(0.5, "npc", "user"), expression(r==frac(t(x),f(x))), pos=4)
```
</div>


## Rejection sampling

<div class="left lt">
* We have the **target distribution**, $t(N) = pr(s|N)$
* Find an easy-to-sample **proposal distribution**, call it $f(N)$
* Draw a sample $x$ from the proposal distribution, and compute an **acceptance probability**, $r = \frac{t(x)}{f(x)}$
* Accept the sample with probability $r$, reject with probability $1-r$
* This is a VERY inefficient way to explore a distribution

```{r}
s = 200
target = function(x) dunif(s, 1, x)
proposal = function(x) dunif(x, 1, 1e6)

## this scale will make sure f(x) >= t(x)
scale = target(s) / proposal(s) 
candidates = as.integer(runif(1e6, 1, 1e6))
r = target(candidates)/(proposal(candidates) * scale)

## uniform numbers between 0 and 1
test = runif(length(r))  

## TRUE if r > test; WHY?
accept = ifelse(r > test, TRUE, FALSE) 
samples = candidates[accept]

```

</div>
<div class="right rt">

```{r echo = FALSE}
samples2 = samples[samples <= max(Npr)]
par(mgp=c(0.5,0,0), mar=c(2,2,2,0.2))
yy = dunif(s, 1, Npr)
plot(Npr, dunif(s, 1, Npr), type='l', bty='l', lwd=2, col='#00BFC4', xaxt='n', xlab="N", yaxt='n', ylab="pr(s | N)")
lines(Npr, prop, lwd=2, col='#F8766D')
par(new=TRUE)
hist(samples2, breaks = 20, freq=FALSE, axes=FALSE, xlim=c(0, max(Npr)),
     col="#C77CFF66", border = "#C77CFF", bty='l', xlab="N", ylab="pr(s | N)", main="")
legend("right", legend=c("proposal", "target", 
       paste("acceptance rate =", round(length(samples)/length(candidates), 3))), lwd=c(2, 2, 0), 
       col=c('#F8766D', '#00BFC4'), bty='n')
```
</div>



## Markov Chains

* Markov chains are defined by a **state vector** $S$
   - In this case, the value of $S$ represents some parameter
   - For a model with $k$ parameters, $S$ is a matrix with $k$ columns
* The model is *stochastic* and has a memory
   - Moves via random walk: $S_{t+1} = f(S_t)$
   - Chain must be **recurrent**: it must be possible to (eventually) reach any possible value from any other possible value
* Markov models are commonly used to model stochastic processes happening in discrete time steps (e.g., population growth)


## Markov Chain Monte Carlo

* MCMC lets us sample from a posterior distribution
* Individual samples are *not* independent (because proposals are centred around the previous value)
* Run for long enough, we can approximate the shape of the posterior


## Metropolis-Hastings

* The most general MCMC algorithm

<div class="left lt">
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$
</div>
<div class="right rt">

```{r, echo = FALSE, fig.height=9}
sd = 1.5
peaks = seq(-30*sd, 30*sd, 3*sd)
scales = 1 - (abs(peaks) / max(peaks+1))
weird = function(x, log=FALSE) sapply(x, function(xx) {
  val = sum(mapply(function(sc, ct) sc * dnorm(xx, ct, sd), scales, peaks))
  if(log)
    val = log(val)
  val
  })
xpr = seq(-40, 40, length.out=1000)
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
```
</div>



## Metropolis-Hastings: starting value

<div class="left lt">
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value

</div>
<div class="right rt">

```{r, echo = FALSE, fig.height=7}
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
cex.pt = 1.3
start = -18.7
points(start, weird(start), pch=21, cex=cex.pt, bg="#00BFC4")
```
</div>




## Metropolis-Hastings: proposal step

<div class="left lt">
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value $S_0$
2. Propose a candidate $S_{cand}$ by sampling from a proposal distribution **centred around $S_0$**
    - Frequently, $S_{cand} \sim \mathcal{N}(S_t, \sigma_p)$
    - $\sigma_p$ is the **proposal scale** (more on this later)
3. Compute an acceptance probability $r = \frac{t(S_{cand})}{t(S_0)}$
    - in practice: $r = e^{\log t(S_{cand}) - \log t(S_0)}$
    - accept or reject as in rejection sampling
    - If the candidate is better, $r > 1$, always accept


</div>
<div class="right rt">

```{r, echo = FALSE, fig.height=7}
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
cand = -18
points(cand, weird(cand), pch=21, cex=cex.pt, bg="#00BFC4")
points(start, weird(start), pch=21, cex=cex.pt, bg="#b6e8ea")
arrows(start, weird(start), cand, weird(cand), length=0.07)
plot(c(start, cand), c(0, -1), ylab = "Time", xlab = "x", 
     xaxt='n', yaxt='n', type='b', pch=21, cex=cex.pt/2, 
     bg="#00BFC4", ylim=c(-50, 0), xlim=range(xpr))
```
</div>





## Metropolis-Hastings: running the chain

<div class="left lt">
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value $S_0$
2. Propose a candidate $S_{cand}$ by sampling from a proposal distribution **centred around $S_0$**
    - Frequently, $S_{cand} \sim \mathcal{N}(S_t, \sigma_p)$
    - $\sigma_p$ is the **proposal scale** (more on this later)
3. Compute an acceptance probability $r = \frac{t(S_{cand})}{t(S_0)}$
    - in practice: $r = e^{\log t(S_{cand}) - \log t(S_0)}$
    - accept or reject as in rejection sampling
    - If the candidate is better, $r > 1$, always accept
4. Continue; asymptotically, the state of the chain converges on the target distribution

</div>
<div class="right rt">

```{r, echo = FALSE, fig.height=7}
cex.pt = cex.pt/2
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
points(start, weird(start), pch=21, cex=cex.pt, bg="#b6e8ea")
points(cand, weird(cand), pch=21, cex=cex.pt, bg="#00BFC4")
samples = numeric(51)
samples[1] = start
samples[2] = cand
for(i in 3:51) {
  samples[i] = rnorm(1, samples[i-1], 0.3)
  points(samples[i], weird(samples[i]), pch=21, cex=cex.pt, bg="#00BFC4")
}
plot(samples, 0:(-50), ylab = "Time", xlab = "x", 
     xaxt='n', yaxt='n', type='b', pch=21, cex=cex.pt, 
     bg="#00BFC4", ylim=c(-50, 0), xlim=range(xpr))
```
</div>




## Metropolis-Hastings: results

<div class="left lt">
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value $S_0$
2. Propose a candidate $S_{cand}$ by sampling from a proposal distribution **centred around $S_0$**
    - Frequently, $S_{cand} \sim \mathcal{N}(S_t, \sigma_p)$
    - $\sigma_p$ is the **proposal scale** (more on this later)
3. Compute an acceptance probability $r = \frac{t(S_{cand})}{t(S_0)}$
    - in practice: $r = e^{\log t(S_{cand}) - \log t(S_0)}$
    - accept or reject as in rejection sampling
    - If the candidate is better, $r > 1$, always accept
4. Continue; asymptotically, the state of the chain converges on the target distribution

</div>
<div class="right rt">

```{r, echo = FALSE, fig.height=7}

## Commented out, because this is slow
## uncomment if you want to try it

# samples = numeric(3e5)
# samples[1] = start
# accept = 0
# for(i in 2:length(samples)) {
#   pr = rnorm(1, samples[i-1], 25)
#   r = exp(weird(pr, log = TRUE) - weird(samples[i-1], log=TRUE))
#   U = runif(1)
#   if(r > U) {
#     accept = accept + 1
#     samples[i] = pr
#   } else {
#     samples[i] = samples[i-1]
#   }
# }
# saveRDS(samples, "misc/metrop_ex_samples.rds")

samples = readRDS("misc/metrop_ex_samples.rds")
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(samples[1:500], type='l', xaxt='n', xlab='time', yaxt='n', ylab='x', xlim=c(1, 500))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', xlim=range(xpr),
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
par(new=TRUE)
hist(samples, breaks = 300, freq=FALSE, axes=FALSE, xlim=range(xpr), main="",
     col="#C77CFF66", border = "#C77CFF", xlab="", ylab="")
```
</div>


## Metropolis algorithm summary

```
Define t(x): log unnormalized posterior (i.e, "target") distribution
Define p(x): the proposal distribution
	common: rnorm(n = 1, mean = x, sd = proposal_scale)
Choose state[0] (the starting value)
for i in 1:n_samples
   candidate = p(state[i-1], proposal_scale)
   r = exp( t(candidate) - t(state[i-1])) ## acceptance probability
   if r > runif(1)  ## coin flip to see if we accept or not
      state[i] = candidate
   else
      state[i] = chain[i-1]
```

## Multivariate Metropolis-Hastings
* Logic is the same
* We cannot easily sample from the joint posterior $pr(\theta_1, \theta_2 | X)$
* We can sample from conditional posteriors $pr(\theta_1, | \theta_2, X)$
* Simply sample from parameters one at a time, in random order (see [sample code](https://github.com/mtalluto/vu_advanced_statistics/blob/main/r/mh.r))


## Tuning Metropolis-Hastings
* Proposal variance is essential for efficiency
* Ideally we want acceptance rates around 0.235 for high dimensional problems, closer to 0.5 for univariate
* Can use adaptive samplers (see sample code) to automate selection of proposal variance
* A simple way to implement adaptation:
   - run a chain, at each step, if accepted, scale = scale * 1.1; if rejected scale = scale / 1.1
   - must discard these samples, start chain over with constant scale


## Traceplots
* Traceplots are an important indicator that chains are working efficiently
* Other helpful plots: `mcmc_hist()` and `mcmc_pairs` (multivariate)

```{r, echo=FALSE}
# samples = matrix(NA, nrow=1e4, ncol=3)
# samples[1,] = start
# accept = c(0,0,0)
# sc = c(0.5, 15, 2000)
# for(i in 2:nrow(samples)) {
#   for(j in 1:3) {
#     pr = rnorm(1, samples[i-1, j], sc[j])
#     r = exp(weird(pr, log = TRUE) - weird(samples[i-1,j], log=TRUE))
#     U = runif(1)
#     if(r > U) {
#       accept[j] = accept[j] + 1
#       samples[i,j] = pr
#     } else {
#       samples[i,j] = samples[i-1,j]
#     }
#   }
# }
# colnames(samples) = c("too small", "just right", "too big")
# saveRDS(samples, "misc/traceplt_ex.rds")
```

```{r, echo=TRUE, message=FALSE, fig.width=10}
library(bayesplot)
samples = readRDS("misc/traceplt_ex.rds")
mcmc_trace(samples)

```

## Hamiltonian monte carlo
- Only gets a mention here, we won't implement this
- We will use this when we start using Stan
- HMC imagines the posterior density is a frictionless bowl; more probable locations are lower
- Place a ball on this bowl, give it a shove in a random direction, record everywhere it goes
- The ball curves around the surface, sample from the path
- once the ball slows down past a certain threshold, drop in a new place and shove it again

