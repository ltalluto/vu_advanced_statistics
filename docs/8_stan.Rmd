---
title: "Intro to Stan"
author: "Matthew Talluto"
date: "25.11.2021"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
---


```{r setup, include=FALSE, results = "hide"}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png")
# library(rethinking)
library(ggplot2)
library(igraph)
library(ggnetwork)
library(data.table)

cols = c("#F8766D", "#7CAE00", "#00BFC4", "#C77CFF")

```

## What is Stan? 
<div class="left lt">

* Stan is a modelling language for scientific computing
* Stan is a *probabilistic programming language*
   - deterministic variables: `mu = a + b * x`
   - Stochastic variables: `y ~ normal(mu, sigma)`
* Samples from the posterior using Hamiltonian Monte Carlo

```{stan output.var = "noeval", eval=FALSE}
mu = a + b*x;
y ~ normal(mu, s);
a ~ normal(0, 20);
b ~ normal(0, 5);
s ~ exponential(0.2);
```


1. Write a Stan model in a `.stan` file
2. Prepare all data in R
3. Use the `rstan` package to invoke the Stan interpreter
   - Translates your model into a C++ program w/ an HMC sampler, then compiles for your computer
4. Run the program from R using `rstan`.
5. Perform posterior inference using various R packages.

</div>

<div class="right rt">


```{r, echo = FALSE}
gr = graph_from_literal(a-+"mu", b-+"mu", s-+y, x-+"mu", "mu"-+y)
V(gr)$type = c("random", "deterministic", "random", "random", "random", "deterministic")
V(gr)$source = c("unknown", "unknown", "unknown", "unknown", "known", "known")
layout = matrix(c(0.5,2,  0.5,1,  1,2,  1.5,2,  1.25,0, 0,2), byrow=TRUE, ncol=2)


n = ggnetwork(gr, layout=layout)
pl = ggplot(n, aes(x = x, y = y, xend = xend, yend = yend)) + 
   geom_edges(colour="gray50", arrow=arrow(length = unit(6, "pt"), type = "closed")) + 
   theme_blank() + geom_nodes(aes(color=type, shape = source), size=6) + 
   geom_nodelabel(aes(label = name), fontface = "bold", nudge_x=-0.1)
pl
```
</div>

## Why Stan?

* For many models, faster than other MCMC samplers
* Fast convergence
* Very concise and natural model specification

<div class="left lt">
```{r eval=FALSE}
log_posterior = function(params, data) {
   a = params['a']
   B = params[grep('B', names(params))]
   s = params['s']
   y = data[['y']]
   X = data[['X']]
   if(s <= 0)
      return(-Inf)
   sum(dnorm(y, a + X %*% B, s, log = TRUE)) + 
      dnorm(a, 0, 10, log=TRUE) + 
      sum(dnorm(B, 0, 5, log=TRUE)) + 
      dexp(s, 0.1, log=TRUE)
}
fit = optim(c(a=0,B1=0, B2=0, s = 1), log_posterior, data = data, control=list(fnscale=-1), hessian=TRUE)
vcv = solve(-fit$hessian)
samples = mvtnorm::rmvnorm(5000, fit$par, vcv)
```

</div>

<div class="right rt">
```{stan output.var="stancode", eval=FALSE}
data {
   int n;
   int k;
   vector [n] y;
   matrix [n,k] X;
}
parameters {
   real a;
   vector [k] B;
   real <lower=0> s;
}
model {
   y ~ normal(a + X * B, s);
   a ~ normal(0, 10);
   B ~ normal(0, 5);
   s ~ exponential(0.1);
}
```

```{r eval=FALSE}
library(rstan)
standata = list(
	y = y,
	X = as.matrix(X),
	n = nrow(X),
	k = ncol(X)
)
fit = stan(stancode, iter=5000, data = standata)
samples = as.matrix(fit)
```

</div>



## Variables in Stan

<div class="left lt">
* Stan variables are less flexible than in R (*strongly-typed*)
* Variables must be declared, types are fixed
* Vector/matrix/array dimensions must be specified ahead of time
* Variables must be declared in particular places
* Variables can have **constraints**

<br/>

**Scalar types**

* `int`
* `real`

<br/>

**Containers**

* `vector`: 1 dimension, real numbers only
* `matrix`: 2 dimensions, real numbers only
* `array`: any dimensions, any data type (including other containers!)

</div>


<div class="right rt">

```{stan output.var="garbage", eval=FALSE}
int my_int;
real my_real;
real <lower=0> my_positive_real;
real <lower=0, upper=1> my_proportion;

vector [10] my_ten_reals;
matrix [n,k] my_matrix;
vector <lower=0> [n] n_positive_numbers;

int ten_integers [10]; // array of ints
real ten_reals [10]; // array of reals
vector <lower=0> [10] array_of_vectors [5]; // 5 vectors, each of length 10
```

</div>


## Program blocks
<div class="left lt">
* A Stan program is divided into blocks
* Blocks *must* be specified in the order here
* Required blocks for a useful model are `data`, `parameters`, and `model`
</div>


<div class="right rt">

```{stan output.var="garbage", eval=FALSE}
functions {
   // for user-defined functions; 
   // see the manual for how to format these
}
data {
   // declare all the data you will pass to Stan
}
transformed data {
   // In this block, you can code any transformations to 
   //     your data that you want to do within Stan
}
parameters {
   // declare the names and dimensions of all parameters
   //     in your model
   // a parameter is normally any stochastic-unknown on
   //     your model graph
}
transformed parameters {
   // transformations for your parameters go here
   // in many cases, these will be for 
   //    deterministic-unknowns
}
model {
   // probability statements, including 
   //    likelihood and priors
}
generated quantities {
   // optional, used e.g., for posterior 
   //    predictive simulations
}
```

</div>

## Scope

<div class="left lt">
* All variables have a scope (where the variable is visible)
* Variables declared at the top of a block are visible in that block, and all blocks below
</div>


<div class="right rt">

```{stan output.var="garbage", eval=FALSE}
data {
   // declare all the data you will pass to Stan
}
transformed data {
   print(param1); // ERROR!
}
parameters {
   real param1;
}
model {
   print(param1); // ok!
}

```

</div>


## Scope

<div class="left lt">
* All variables have a scope (where the variable is visible)
* Variables declared at the top of a block are visible in that block, and all blocks below
* Variables declared later in a block must come at the beginning of a block denoted by {}
* These variables are only visible within that block
</div>


<div class="right rt">

```{stan output.var="garbage", eval=FALSE}

transformed data {
   vector[n] x;
   for(i in 1:n) {
      real x2 = 0;
      x2 = x2 + x[i]^2
      print(x2); //ok !
   }
   print(x2); // error!
}

```

</div>



## Probability statements
* In the `model` block, we write statements describing how to evaluate the log probability of the model.
* This works using the `~` symbol
* Here, the model evaluates $pr(y | \mu, \sigma)$ using the **normal PDF**
* This quantity is then added to the total log probability of the model
* This works if y is a vector or a single value

```{stan output.var="garbage", eval=FALSE}
model {
   y ~ normal(mu, sigma);
}
```


## German Tank problem: Stan model

<div class="left lt">
2. Recall the German tank problem presented in lecture. Use the following captured serial numbers:

```{r}
s = c(147, 126, 183, 88, 9, 203, 16, 10, 112, 205)
```

a. Write a log likelihood function that computes $\log pr(s | N_{max})$. What distribution is appropriate? Assume that the observations in $s$ are independent and random; $s$ can be any number between 1 and $N_{max}$, and the probability of any number in that range being observed is the same as for any other number.
b. Write a log prior for $N_{max}$, and combine it with the log likelihood into a log posterior function.

$$
\begin{aligned}
s & \sim \mathrm{U}(1, N_{max}) \\
N_{max} & \sim \mathrm{Gamma}(\alpha=0.001, \beta=0.001)
\end{aligned}
$$



</div>

<div class="right rt">

```{stan output.var="stan_tanks", cache = TRUE}
data {
   int <lower = 1> n;
   vector <lower=1> [n] s;
}
parameters {
   real <lower = max(s)> Nmax;
}
model {
   s ~ uniform(1, Nmax); // likelihood
   // this is a super vague prior!
   Nmax ~ gamma(0.001, 0.001);  
}
```

```{r, echo=FALSE, fig.height=5, fig.width=7}
gr = graph_from_literal(s+-Nmax, Nmax+-"α=0.001", Nmax+-"β=0.001")
V(gr)$type = c("random", "random", "deterministic", "deterministic")
V(gr)$source = c("known", "unknown", "known", "known")
layout = matrix(c(0,2,  0,1, -0.5,0, 0.5,0), byrow=TRUE, ncol=2)
nt = ggnetwork(gr, layout=layout)
grpl = ggplot(nt, aes(x = x, y = y, xend = xend, yend = yend)) + 
   geom_edges(colour="gray50", arrow=arrow(length = unit(6, "pt"), type = "closed")) + 
   theme_blank() + geom_nodes(aes(color=type, shape = source), size=6) + 
   geom_nodelabel(aes(label = name), fontface = "bold", nudge_x=-0.25) + 
   annotate(geom="label", x = 1.3, y = -0.1, label = "Hyperparameter layer") + 
   annotate(geom="label", x = 1.3, y = 0.5, label = "Parameter layer") + 
   annotate(geom="label", x = 1.3, y = 1, label = "Observation layer") + 
   xlim(-0.5, 1.5)
grpl

```

</div>


## German Tank problem: MAP estimate

<div class="left lt">
c. This posterior distribution cannot be estimated using Laplace approximation. However you can still use optim to get a MAP estimate. Get the MAP estimate, then plot the log_posterior against many values of $N_{max}$. Does your MAP estiamte make sense?


```{r}
log_posterior = function(params, data) {
	# here we 'unpack' the objects to make the names clearer
	Nmax = params[1]
	s = data[['s']]

	# the sampler sometimes selects impossible values for Nmax
   # need to rule them out
	if(Nmax < 1) {
		return(-Inf)
	}

	log_liklihood = sum(dunif(s, 1, Nmax, log=TRUE))
   ## this is a super vague prior!
	log_prior = dgamma(Nmax, 0.001, 0.001, log = TRUE)  
	# log_prior = 0
	return(log_liklihood + log_prior)
}
```

</div>

<div class="right rt">

```{r message=FALSE, warning=FALSE}
library(rstan)
standat = list(n = length(s), s = s)
fit_map = optimizing(stan_tanks, data = standat)
fit_map$par
```

```{r message=FALSE, warning=FALSE, echo = FALSE}
pldat = data.frame(x = 150:300, y = sapply(150:300, function(n) exp(log_posterior(n, standat))))
ggplot(pldat, aes(x=x, y=y)) + geom_line() + xlab(expression(N_max)) + ylab("Pr(N | s)") + 
   geom_point(data = data.frame(x=fit_map$par, y = exp(log_posterior(fit_map$par, standat))), aes(x=x, y=y), size=2, colour='red')

```

</div>


## German Tank problem: inference
<div class="left lt">

3. Use a metropolis sampler to generate 20000 samples from the posterior distribution for the German tank problem. You can either use the sampler you wrote for question 1, or use one provided for you. It can be found in `r/metrop.r` in this repository.

```{r message=FALSE, cache = TRUE}
fit = sampling(stan_tanks, data = standat, iter=20000, chains = 1, refresh = 0)

## use if running from a separate .stan file
# fit = stan("path/to/code.stan", data = standat, iter=20000) 
samples = as.matrix(fit)
head(samples)
```

</div>

<div class="right rt">

```{r message = FALSE, fig.height=4.5, fig.width=7}
source("../r/metrop.r")
library(bayesplot)
mcmc_combo(samples, c("hist", "trace"), pars = "Nmax")
rbind(hdpi = hpdi(samples[,1]), 
      quantile = quantile(samples[,1], c(0.05, 0.95)))
```

</div>


## Bayesian analysis workflow
1. Specify joint posterior graphically, mathematically, and in code

## Bayesian analysis workflow
1. Specify joint posterior graphically, mathematically, and in code
2. Draw samples from the joint posterior distribution

## Bayesian analysis workflow
1. Specify joint posterior graphically, mathematically, and in code
2. Draw samples from the joint posterior distribution
3. Evaluate/diagnose the model's performance

## Bayesian analysis workflow
1. Specify joint posterior graphically, mathematically, and in code
2. Draw samples from the joint posterior distribution
3. Evaluate/diagnose the model's performance
4. Perform posterior inference


## 1. Joint posterior: likelihood

* Liklihood is a **generative model** (the distribution from which the observations are generated)
  - whenever possible use knowledge of the appropriate processes
  - transform parameters and sample in transformed space to improve behaviour
  - e.g. use log($\sigma$) instead of $\sigma$ to avoid impossible negative variance in sampler
  - in Stan, this is done by adding variable **constraints** (`real <lower=0> sigma`)
  
## 1. Joint posterior: likelihood

* Liklihood is a **generative model** (the distribution from which the observations are generated)
  - whenever possible use knowledge of the appropriate processes
  - transform parameters and sample in transformed space to improve behaviour
  - e.g. use log($/sigma$) instead of $/sigma$ to avoid impossible negative variance in sampler
  - in Stan, this is done by adding variable **constraints** (`real <lower=0> sigma`)
* Models should be specified to be scale independent
  - Easy: scaling your predictors to have mean=0, sd=1
  - Hard: scaling outcomes (y); this can change the generative model!
  - Prefer to transform outcomes with the **link function**
  
## 1. Joint posterior: priors
* Any unknowns must have a **prior**
   - (or possibly a hierarchical generative model and hyperpriors)
* Prefer regularising priors to vague priors
   - Normal(0, 5) instead of Normal(0,500)
* Avoid improper priors: Uniform(-Inf, Inf)
* Forget conjugacy unless you know what you are doing and why


## 1. Joint posterior: priors
* Avoid hard boundaries
   - **No**: Uniform(0, 1000)
   - **Yes**: Exponential(0.1)
* With informative priors, specify reasonable initial values
* Begin sampling with weaker priors, gradually strengthen once you know the model is working
* Specify priors for everything—avoid defaults
* Draw out your model as a digraph to make sure you don’t miss anything

## GLM in Stan

For this exercise, you will use the [birddiv](https://raw.githubusercontent.com/mtalluto/vu_advanced_statistics/main/docs/exercises/data/birddiv.csv) (in `docs/exercises/data/birddiv.csv`) dataset; you can load it directly from github using `data.table::fread()`. Bird diversity was measured in 1-km^2 plots in multiple countries of Europe, investigating the effects of habitat fragmentation and productivity on diversity. We will consider a subset of the data. Specificially, we will ask how various covariates are associated with the diversity of birds specializing on different habitat types. The data have the following potential predictors:

* **Grow.degd**: growing degree days, a proxy for how warm the climate is.
* **For.cover**: Forest cover near the sampling location
* **NDVI**: normalized difference vegetation index, a proxy for productivity
* **For.diver**: Forest diversity in the forested area nearby
* **Agr.diver**: Diversity of agricultural landscapes
* **For.fragm**: Index of forest fragmentation

All of the above variables are standardized to a 0-100 scale. Consider this when choosing priors.

Your response variable will be **richness**, the bird species richness in the plot. Additionally, you have an indicator variable **hab_type**. This is not telling you what habitat type was sampled (plots included multiple habitats). Rather, this is telling you what type of bird species were counted for the richness measurement: so `hab_type == "forest" & richness == 7` indicates that 7 forest specialists were observed in that plot.

Build one or more generalised linear models for bird richness. Your task should be to describe two things: (1) how does richness vary with climate, productivity, fragmentation, or habitat diversity, and (2) do these relationships vary depending on what habitat bird species specialize on. 


## 1. Specify joint posterior

<div class="left lt">
* We should specify a **generative model**
* Best to graph the model, ensure graph and stan code match
* Strive for independence of the scale of the x-variables
* Carefully choose priors
    - With no prior information, prefer **regularising priors**
    - Avoid priors that give probability mass to impossible values (e.g., normal(0,1) for a standard deviation)
    - Avoid flat priors or very long tails

```{r, echo=FALSE, fig.height=5, fig.width=7}
gr = graph_from_literal(richness+-"λ", "λ"+-a, "λ"+-B, "λ"+-X, a+-"μa=0", a+-"σa=10", B+-"μB=0", B+-"σB=5")
V(gr)$type = c("random", "deterministic", "random", "random", rep("deterministic", 5))
V(gr)$source = c("known", "unknown", "unknown", "unknown", rep("known", 5))
layout = matrix(c(-0.5,3,   0,2,   -0.5,1,   0.5,1,   0.5,3,   -0.7,0,   -0.3,0,   0.3,0,   0.7,0), byrow=TRUE, ncol=2)
nt = ggnetwork(gr, layout=layout)
grpl = ggplot(nt, aes(x = x, y = y, xend = xend, yend = yend)) + 
   geom_edges(colour="gray50", arrow=arrow(length = unit(6, "pt"), type = "closed")) + 
   theme_blank() + geom_nodes(aes(color=type, shape = source), size=6) + 
   geom_nodelabel(aes(label = name), fontface = "bold", nudge_x=-0.1)
grpl

```

</div>

<div class="right rt">

```{stan output.var="garbage", eval = FALSE}
transformed parameters {
	lambda = exp(a + X * B);
}
model {
	richness ~ poisson(lambda);
	a ~ normal(0, 10);
	B ~ normal(0, 5);
}
```


</div>


## 1. Specify joint posterior

<div class="left lt">
* We should specify a **generative model**
* Best to graph the model, ensure graph and stan code match
* Strive for independence of the scale of the x-variables
* Carefully choose priors
    - With no prior information, prefer **regularising priors**
    - Avoid priors that give probability mass to impossible values (e.g., normal(0,1) for a standard deviation)
    - Avoid flat priors or very long tails

```{r, echo=FALSE, fig.height=5, fig.width=7}
grpl

```

</div>

<div class="right rt">

```{stan output.var="stan_glm", cache=TRUE}
data {
	int <lower=0> n; // number of data points
	int <lower=0> k; // number of x-variables
	int <lower=0> richness [n];
	matrix [n,k] X;
}
parameters {
	real a;
	vector [k] B;
}
transformed parameters {
	vector <lower=0> [n] lambda;
	lambda = exp(a + X * B);
}
model {
	richness ~ poisson(lambda);
	a ~ normal(0, 10);
	B ~ normal(0, 5);
}
generated quantities {
	int r_predict [n];
	for(i in 1:n)
		r_predict[i] = poisson_rng(lambda[i]);
	r_predict = poisson_rng(lambda);
}
```


</div>


## 2. Sample from joint posterior

* Start with a small run (10s-100s of iterations) to catch errors, make sure nothing surprising happens
* Increase to a few thousand to view convergence
* Select starting values and run until convergence (1.000s for Stan, 10.000s—100.000s or more for Metropolis)


## 2. Sample from the joint posterior

<div class="left lt">

```{r, message=FALSE, cache = TRUE}
library(data.table)
birds = fread("exercises/data/birddiv.csv")

# stan barfs if you give it NAs
birds = birds[complete.cases(birds)]

## the 0-100 scaling is not super helpful
## here we pull out the x-variables
## and rescale them to have mean = 0 and sd = 1
X_scaled = as.matrix(birds[,c(2:7)])
X_scaled = scale(X_scaled)

## I will try two models
## First, simple model, only forest birds in relation to forest cover
for_i = which(birds$hab_type == "forest")
standat1 = list(
	n = length(for_i), 
	k = 1,
	richness = birds$richness[for_i],
	X = X_scaled[for_i, "NDVI", drop=FALSE])

fit1 = sampling(stan_glm, data = standat1, iter=2000, 
   chains = 4, refresh = 0)

```

</div>

<div class="right rt">

```{r, message=FALSE, cache = TRUE}
# Second, looking at how two variables influence birds of different types

# grab two variables
X = X_scaled[, c("For.cover", "NDVI")]

# add a categorical variable for bird type
X = cbind(X, open=ifelse(birds$hab_type == "open",1, 0))
X = cbind(X, generalist=ifelse(birds$hab_type == "generalist",1, 0))

# add interaction terms with the categories
X = cbind(X, op_forCov=X[,1]*X[,3], op_NDVI=X[,1]*X[,4], 
   ge_forCov=X[,2]*X[,3], ge_NDVI=X[,2]*X[,4])

head(X)

standat2 = with(birds, list(
	n = length(richness), 
	k = ncol(X),
	richness = richness,
	X = X))
fit2 = sampling(stan_glm, data = standat2, iter=2000, 
   chains = 4, refresh = 0)
```


</div>



## 3. Evaluate the model fit

<div class="left lt">
* Traceplots can tell you about model convergence and efficiency
* Histograms can alert you to problems with multi-modality
* Run multiple chains to help with diagnostics

```{r message = FALSE, fig.width=7}
## Use as.array if you want to keep different mcmc chains together
## This is ideal for diagnostics
## For inference, you usually want to lump all chains
## In this case, you use as.matrix
samp1_pars = as.array(fit1, pars=c('a', 'B'))
mcmc_combo(samp1_pars, c("hist", "trace"))

```

</div>

<div class="right rt">

* Printing the model also gives useful metrics
* Can filter by parameters of interest
* n_eff: Effective sample size (after removing autocorrelation)
    - This gives you an indication of how much precision in the tails of the posterior you have
* Rhat: convergence diagnostic, available with multiple chains
    - ideally, Rhat = 1
    - Worry about for "real" parameters (not hierarchical, not deterministic)
    - Rhat > 1.1 for a real parameter is a problem
    - Rhat < 1.05 is probably ok

```{r, message=FALSE}
print(fit1, pars = c('a', 'B'))
```

</div>



## 4. Inference: parameter estimates

<div class="left lt">
```{r message = FALSE, fig.width=5}
mcmc_intervals(samp1_pars)
```
</div>

<div class="right rt">
```{r message = FALSE, fig.width=7}
samp2_pars = as.array(fit2, pars=c('a', 'B'))
mcmc_intervals(samp2_pars)
```
</div>



## 4. Inference: Retrodiction

<div class="left lt">
* How close is the model to the original data?
* How well does our generative model describe the data

```{r message = FALSE, fig.width=9, echo=FALSE}
library(gridExtra)
# this gives us posterior samples for:
#    -lambda (the expected value of richness)
#    -r_predict (posterior predictive simulations)
# for each original data point
samp1_pr = as.matrix(fit1, pars=c('lambda', 'r_predict'))

# now we compute the median and 90% quantiles for lambda (the expected value)
# and for r (the posterior predictive sim for richness) for each data point
samp1_intervals = apply(samp1_pr, 2, quantile, c(0.5, 0.05, 0.9))


## now we reshape these a bit to get them into a data frame for visualisation
## want everything alongside the original data
pldat1 = data.table(standat1$X)
pldat1 = cbind(pldat1, data.table(
	richness = standat1$richness,
	lambda = samp1_intervals[1, grep("lambda", colnames(samp1_intervals))],
	lambda_l = samp1_intervals[2, grep("lambda", colnames(samp1_intervals))],
	lambda_u = samp1_intervals[3, grep("lambda", colnames(samp1_intervals))],
	rpr = samp1_intervals[1, grep("r_predict", colnames(samp1_intervals))],
	rpr_l = samp1_intervals[2, grep("r_predict", colnames(samp1_intervals))],
	rpr_u = samp1_intervals[3, grep("r_predict", colnames(samp1_intervals))]))
pldat1 = pldat1[order(NDVI)]

pl_left = ggplot(pldat1) + geom_ribbon(aes(x=NDVI, ymin=rpr_l, ymax=rpr_u), fill=cols[2], alpha=0.3) + 
	geom_ribbon(aes(x=NDVI, ymin=lambda_l, ymax=lambda_u), fill=cols[3], alpha=0.5) + 
	geom_line(aes(x=NDVI, y=lambda), col=cols[3]) + 
	geom_point(aes(x = NDVI, y = richness)) + 
	theme_minimal() + ylab("Forest bird richness")

pl_right = ggplot(pldat1) + xlab("Observed Richness") + 
	geom_errorbar(aes(x=richness, ymin=rpr_l, ymax=rpr_u), size=1.5, col=cols[2], width=0, alpha=0.3) + 
	geom_errorbar(aes(x=richness, ymin=lambda_l, ymax=lambda_u), col=cols[3], width=0, alpha=0.6) + 
	geom_point(aes(x=richness, y=lambda)) +
	geom_abline(intercept=0, slope=1, lty=2) + theme_minimal() + xlim(0, 23) + ylim(0, 23)

grid.arrange(pl_left, pl_right, ncol=2, nrow=1)
```
</div>

<div class="right rt">

```{r message = FALSE, fig.width=7}
# compute posterior distribution of the residual sum of squares
samp1_lam = as.matrix(fit1, pars='lambda')
sq_resid1 = apply(samp1_lam, 1, function(x) (standat1$richness - x)^2)

# compute posterior distribution of dispersion parameter, which is just 
# sum(squared residuals)/(n - k)
# here k is 2, we have an intercept and one slope
# if phi > 1, we have overdispersion and need a better model
phi = apply(sq_resid1, 2, function(x) sum(x)/(length(x) - 2))
quantile(phi, c(0.5, 0.05, 0.95))

```
</div>





## 4. Inference: Improve the model

<div class="left lt">

```{r message = FALSE, fig.width=9, echo=FALSE}
library(gridExtra)
# this gives us posterior samples for:
#    -lambda (the expected value of richness)
#    -r_predict (posterior predictive simulations)
# for each original data point
samp2_pr = as.matrix(fit2, pars=c('lambda', 'r_predict'))

# now we compute the median and 90% quantiles for lambda (the expected value)
# and for r (the posterior predictive sim for richness) for each data point
samp2_intervals = apply(samp2_pr, 2, quantile, c(0.5, 0.05, 0.9))


## now we reshape these a bit to get them into a data frame for visualisation
## want everything alongside the original data
pldat2 = data.table(standat2$X)
pldat2 = cbind(pldat2, data.table(
	richness = standat2$richness,
	lambda = samp2_intervals[1, grep("lambda", colnames(samp2_intervals))],
	lambda_l = samp2_intervals[2, grep("lambda", colnames(samp2_intervals))],
	lambda_u = samp2_intervals[3, grep("lambda", colnames(samp2_intervals))],
	rpr = samp2_intervals[1, grep("r_predict", colnames(samp2_intervals))],
	rpr_l = samp2_intervals[2, grep("r_predict", colnames(samp2_intervals))],
	rpr_u = samp2_intervals[3, grep("r_predict", colnames(samp2_intervals))]))

## add an indicator as to what kind of bird we are talking about
pldat2[, bird := ifelse(open == 1, "open", ifelse(generalist == 1, "generalist", "forest"))]


ggplot(pldat2) + xlab("Observed Richness") + 
	geom_errorbar(aes(x=richness, ymin=rpr_l, ymax=rpr_u), size=1.5, col=cols[4], width=0, alpha=0.3) + 
	geom_errorbar(aes(x=richness, ymin=lambda_l, ymax=lambda_u), width=0, alpha=0.6) + 
	geom_point(aes(x=richness, y=lambda, colour = bird)) +
	facet_grid(.~bird) + 
	geom_abline(intercept=0, slope=1, lty=2) + theme_minimal() + xlim(0, 23) + ylim(0, 23) + 
	labs(colour="Type of bird") 

```
</div>

<div class="right rt">

```{r message = FALSE, fig.width=7}
# compute posterior distribution of the residual sum of squares
samp2_lam = as.matrix(fit2, pars='lambda')
sq_resid2 = apply(samp2_lam, 1, function(x) (standat2$richness - x)^2)

# compute posterior distribution of dispersion parameter, which is just 
# sum(squared residuals)/(n - k)
# here k is 2, we have an intercept and one slope
# if phi > 1, we have overdispersion and need a better model
phi = apply(sq_resid2, 2, function(x) sum(x)/(length(x) - 2))
quantile(phi, c(0.5, 0.05, 0.95))

```

* This model is still quite overdispersed
   - Consider more variables
   - Consider other likelihoods (e.g., Negative Binomial)

</div>





## 4. Inference: Partial Response Curves

* How does richness respond to the individual variables, holding other variables constant?

```{r, message=FALSE, echo = FALSE, fig.width=10}
## generate a dataset to predict a line for all combinations
xx = seq(-1.8,1.8, length.out = 200)
predict_dat = rbind(data.table(forcover = xx, ndvi = 0, open=0, gen = 0, op_fc = 0, op_nd = 0, ge_fc = 0, ge_ndvi = 0), 
			data.table(forcover = xx, ndvi = 0, open=1, gen = 0, op_fc = xx, op_nd = 0, ge_fc = 0, ge_ndvi = 0),
			data.table(forcover = xx, ndvi = 0, open=0, gen = 1, op_fc = 0, op_nd = 0, ge_fc = xx, ge_ndvi = 0),
			data.table(forcover = 0, ndvi = xx, open=0, gen = 0, op_fc = 0, op_nd = 0, ge_fc = 0, ge_ndvi = 0),
			data.table(forcover = 0, ndvi = xx, open=1, gen = 0, op_fc = 0, op_nd = 1, ge_fc = 0, ge_ndvi = 0),
			data.table(forcover = 0, ndvi = xx, open=0, gen = 1, op_fc = 0, op_nd = 0, ge_fc = 0, ge_ndvi = 1))
predict_dat = cbind(intercept=1, predict_dat)

## this computes a posterior distribution for E(y) | predict_dat
## in other words, for the x-values we have chosen for visualisation, what
## is the distribution of the average of y at those x-values
y = exp(as.matrix(fit2, pars = c('a', 'B')) %*% t(as.matrix(predict_dat)))

## Compute median and 90% quantile interals for E(y) and add to the data frame for ggplot
predict_dat$y_med = apply(y, 2, median)
predict_dat$y_upper = apply(y, 2, quantile, 0.95)
predict_dat$y_lower = apply(y, 2, quantile, 0.05)

## Create some nice labels for ggplot
predict_dat$bird = with(predict_dat, ifelse(open == 1, "open", ifelse(gen == 1, "generalist", "forest")))
predict_dat$panel = "Forest Cover | NDVI=0"
predict_dat$panel[601:1200] = "NDVI | Forest Cover=0"

## Create a combined x-variable for ggplot; this works because forcover
## and ndvi are transformed to mean = 0, and the predictions are conditional; 
## if one variable is nonzero, the other must be zero
predict_dat$x = predict_dat$forcover + predict_dat$ndvi

ggplot(predict_dat, aes(x=x, y=y_med, col=bird)) + 
	geom_ribbon(aes(x=x, ymin=y_lower, ymax=y_upper, fill=bird), alpha=0.5) + 
	geom_line() + facet_grid(.~panel) + 
	theme_minimal() + ylab("Species Richness") + xlab(expression(sigma)) + 
	labs(fill="Type of bird", colour="Type of bird") + 
	xlim(-1.8, 1.8)

```


## 4. Inference: Response Surfaces
```{r, message=FALSE, echo = FALSE, fig.width=15}
xy = data.table(expand.grid(forcover=seq(-1.8,1.8, length.out = 50), ndvi=seq(-1.8,1.8, length.out = 50)))
pr_for = cbind(intercept=1, xy, open=0, gen = 0, op_fc = 0, op_nd = 0, ge_fc = 0, ge_ndvi = 0)
pr_for$y = apply(exp(as.matrix(fit2, pars = c('a', 'B')) %*% t(as.matrix(pr_for))), 2, median)

pr_open = cbind(intercept=1, xy, open=1, gen = 0, op_fc = xy$forcover, op_nd = xy$ndvi, ge_fc = 0, ge_ndvi = 0)
pr_open$y = apply(exp(as.matrix(fit2, pars = c('a', 'B')) %*% t(as.matrix(pr_open))), 2, median)

pr_gen = cbind(intercept=1, xy, open=0, gen = 1, op_fc = 0, op_nd = 0, ge_fc = xy$forcover, ge_ndvi = xy$ndvi)
pr_gen$y = apply(exp(as.matrix(fit2, pars = c('a', 'B')) %*% t(as.matrix(pr_gen))), 2, median)

pts = data.table(standat2$X)
pts$richness = standat2$richness
pts[, type := ifelse(open==1, "open", ifelse(generalist==0, "generalist", "forest"))]


forpl = ggplot(pr_for, aes(x=forcover, y=ndvi, fill=y)) + geom_tile() + guides(fill="none") +
	scico::scale_fill_scico(palette = "bilbao", limits=c(0,12)) + theme_minimal() + 
	xlab("Forest Cover") + ylab("NDVI") + labs(fill="Predicted richness") + ggtitle("Forest Birds") + 
	geom_point(data=pts[type == "forest"], aes(x=For.cover, y=NDVI, size=richness), fill="white", alpha=0.5) +
	guides(size="none") + scale_size(limits=c(0, 20), range=c(0.1, 7))
openpl = ggplot(pr_open, aes(x=forcover, y=ndvi, fill=y)) + geom_tile() + guides(fill="none") +
	scico::scale_fill_scico(palette = "bilbao", limits=c(0,12)) + theme_minimal() + 
	xlab("Forest Cover") + ylab("") + labs(fill="Predicted richness") + ggtitle("Open Birds") + 
	geom_point(data=pts[type == "open"], aes(x=For.cover, y=NDVI, size=richness), fill="white", alpha=0.5) +
	guides(size="none") + scale_size(limits=c(0, 20), range=c(0.1, 4))
genpl = ggplot(pr_gen, aes(x=forcover, y=ndvi, fill=y)) + geom_tile() + 
	scico::scale_fill_scico(palette = "bilbao", limits=c(0,12)) + theme_minimal() + 
	xlab("Forest Cover") + ylab("") + labs(fill="Predicted richness") + ggtitle("Generalist Birds") + 
	geom_point(data=pts[type == "generalist"], aes(x=For.cover, y=NDVI, size=richness), fill="white", alpha=0.5) +
	guides(size="none") + scale_size(limits=c(0, 20), range=c(0.1, 4))

grid.arrange(forpl, openpl, genpl, nrow=1, ncol=3, widths=c(1,1,1.3))
```

