---
title: "Sampling for Posterior Inference"
author: "Matthew Talluto"
date: "04.05.2022"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
  beamer_presentation: default
---


```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png")
library(ggplot2)
library(data.table)
library(mvtnorm)
options(digits=4)

# code from the previous lecture
log_liklihood = function(a, b, s, x, y) {
  # compute mu, the expectation of y
  mu = a + b*x
  
  # liklihood of y|x,a,b,s
  sum(dnorm(y, mu, s, log=TRUE))
}
log_prior = function(a, b, s) {
  ## here the prior hyperparameters are hard-coded
  dnorm(a, 0, 10, log=TRUE) + dnorm(b, 0, 5, log=TRUE) + dexp(s, 0.2, log=TRUE)  
}

log_posterior_la = function(params, data) {
  # unpack params and data
  x = data[['x']]
  y = data[['y']]
  a = params['a']
  b = params['b']
  s = exp(params['log_s'])

  log_liklihood(a, b, s, x, y) + log_prior(a, b, s)
}


data('iris')
iris = iris[iris$Species != "setosa",]
data = with(iris, data.frame(x = Sepal.Length, y = Petal.Length))
param_init = c(a=0, b=0, log_s=0)
fit = optim(param_init, log_posterior_la, data = data, method="Nelder-Mead", 
            control=list(fnscale=-1), hessian = TRUE)
vcv = solve(-fit$hessian)
sds = sqrt(diag(vcv))
```




## Sampling the unknown

* Why *can't* we just look at the conditional distributions?
* We have a mean and standard deviation



## Sampling the unknown

* Why *can't* we just look at the conditional distributions?
* We have a mean and standard deviation

<div class="left lt">

```{r}
bad_sample = data.frame(a = rnorm(1e4, fit$par[1], sds[1]), 
                        b = rnorm(1e4, fit$par[2], sds[2]))
ggplot(bad_sample, aes(x=a, y=b)) + geom_point(size = 0.4) + 
  theme_minimal()
```
</div>


<div class="right rt">

```{r fig.width = 7, fig.height=5}
posterior_grid = expand.grid(a=seq(-3, 0, length.out=100), 
    b = seq(0.8, 1.3, length.out = 100), log_s = fit$par[3])
posterior_grid$pr = dmvnorm(as.matrix(posterior_grid[,1:3]), 
    mean = fit$par, sigma=vcv, log=FALSE)
pos_dens_pl = ggplot(posterior_grid, aes(x=a, y=b, fill=pr)) + 
  geom_tile(size=0.5) + theme_minimal() + 
  scico::scale_fill_scico(palette = "lajolla") + 
  labs(fill="Posterior density")
pos_dens_pl
```
</div>


## Sampling the unknown

<div class="left lt">
* This technique of sampling to learn about something is quite useful
* We can learn more about our posterior by sampling from it
* This includes marginal distributions

```{r echo=FALSE, fig.width = 7, fig.height=5}
pos_dens_pl
```
</div>


<div class="right rt">

```{r fig.width=3.5, fig.height=3.5, fig}
good_sample = data.frame(rmvnorm(1e4, mean = fit$par, 
  sigma = vcv))
ggplot(good_sample, aes(x=a, y=b)) + geom_point(size = 0.4) + 
  theme_minimal()
```

```{r fig.width=5.5, fig.height=3.5}
good_sample_pl = reshape2::melt(good_sample, id.vars = NULL)
ggplot(good_sample_pl, aes(x = value)) + 
  geom_histogram(fill = "#00BFC4", bins=40) + 
  facet_grid(~variable, scales="free") + theme_minimal()

```
</div>

## Summarizing a posterior distribution

Often we want to know something about some parameter $\theta$

* What is the probability that $\theta$ is less than some value---e.g., $pr(b \le 0)$

```{r}
with(good_sample, sum(b <= 0)/length(b))

# we have 1000 samples, so we can confidently say
# pr(b <= 0) < 0.001
```

## Summarizing a posterior distribution

Often we want to know something about some parameter $\theta$

* What is the probability that $\theta$ is less than some value---e.g., $pr(b \le 0)$
* What is the probability that $\theta$ is between two particular values

```{r}
with(good_sample, sum(b >= 1.0 & b <= 1.2)/length(b))
```

## Summarizing a posterior distribution

Often we want to know something about some parameter $\theta$

* What is the probability that $\theta$ is less than some value---e.g., $pr(b \le 0)$
* What is the probability that $\theta$ is between two particular values
* What interval encompasses 90% or 95% of the probability mass? **90% Credible Interval**

```{r}
t(apply(good_sample, 2, quantile, c(0.05, 0.95)))
```


## Summarizing a posterior distribution

Often we want to know something about some parameter $\theta$

* What is the probability that $\theta$ is less than some value---e.g., $pr(b \le 0)$
* What is the probability that $\theta$ is between two particular values
* What interval encompasses 90% or 95% of the probability mass? **90% Credible Interval**
* What is the most likely value or "best" guess of theta?

```{r, results = 'asis'}
mn = colMeans(as.matrix(good_sample))
med = apply(as.matrix(good_sample), 2, median)
tab = matrix(c(mn, med, fit$par), ncol=3, byrow=TRUE, dimnames = list(c("mean", "median", "mode"), names(fit$par)))
```

```{r, results = 'asis', echo=FALSE}
knitr::kable(tab, format='html')
```

* MAP estimates the posterior mode. 
* For normal distribution, mode, mean, median all the same.
* In other cases, median performs best


## Posterior prediction

<div class="left lt">

* Posterior distributions are *transitive*

> If $\hat{\theta}$ is a set of samples approximating the posterior distribution of $\theta$, and if some desired variable $Z = f(\theta)$, then $f(\hat{\theta})$ approximates the posterior distribution of $Z$

* Each posterior sample represents a plausible line, describing the expected value of $y$
</div>

<div class="right rt">

```{r}
plot(data$x, data$y, type='n', xlab='Sepal Length', ylab = "Petal Length", bty='n')
points(data$x, data$y, pch=16, cex=0.7, col='#80b1d3')

# draw 100 random sample lines and draw them
rows = sample(nrow(good_sample), 100)
apply(as.matrix(good_sample)[rows,], 1, function(x) abline(a=x['a'], b=x['b'], col="#fb807255", lwd=0.5))

# draw the MAP line as well
abline(a=fit$par['a'], b=fit$par['b'], lwd=3, col="#c45d52")

```
</div>





## Regression credible interval

<div class="left lt">

* Posterior distributions are *transitive*

> If $\hat{\theta}$ is a set of samples approximating the posterior distribution of $\theta$, and if some desired variable $Z = f(\theta)$, then $f(\hat{\theta})$ approximates the posterior distribution of $Z$

* Each posterior sample represents a plausible line, describing the expected value of $y$

* In our regression model, we compute $\mu$, which is the expected value of $y$ if we know the value of $x$

$$
\mu = a + bx
$$

* Because we have samples from the joint posterior $pr(a,b|x)$, we can compute the posterior for mu
* Each sample predicts a line, all plausible, some more probable than others
</div>

<div class="right rt">

```{r fig.width=4, fig.height=4}
# here we compute a credible interval for y for each value in
# x_predict. Why include the column of 1's?
x_predict = cbind(intercept=1, x=seq(min(data$x)-1, 
  max(data$x)+1, length.out=100))

# shortcut using matrix multiplication; this is 1000s of times
# faster than other methods
# look over this code carefully, what is %*% doing? 
# what about t? 
# what are the dims of mu_sample? what does this represent?
mu_samples = as.matrix(good_sample[,1:2]) %*% t(x_predict)
mu_ci = apply(mu_samples, 2, quantile, probs=c(0.05, 0.90))

# organize things into a dataframe for ggplot
plDat = data.frame(x = x_predict[,2], lower = mu_ci[1,], 
  upper=mu_ci[2,])
plDat$median = fit$par['a'] + plDat$x*fit$par['b']
ggplot(plDat, aes(x=x, y = median)) + 
  geom_line(colour="#c45d52") + 
  geom_ribbon(aes(ymin=lower, ymax=upper), alpha = 0.4, 
    fill = "#fb8072") + 
  geom_point(data = data, aes(x=x, y=y), size=0.7, 
    colour='#80b1d3') +
  theme_minimal() + ylab("Petal Length") + xlab('Sepal Length')

```
</div>





## Posterior predictive distribution

<div class="left lt">
* Our Bayesian model is **generative**
* It postulates a *statistical* process (not mechanistic) by which the outcomes $y$ are created
* Posterior predictive simulations tells you the distribution of new *outcomes*
* For a given value of $x$, the interval tells you where 90% of the values of $y$ will fall
* To do this:
   - for each sample of $a$, $b$, and $s$
   - comptue the expected value $\mu$ for a **prediction dataset** $\hat{x}$
   - simulate a new dataset $\hat{y}$ from $\mu$ and $s$
   - compute quantiles for $\hat{y} | \hat{x}$
</div>

<div class = "right rt">

```{r}
# function that takes a single a, b, and s, and a vector of x
# generates a new dataset of y's for each x
sim = function(a, b, s, xhat) {
  mu = a + b * xhat
  rnorm(length(xhat), mu, s)
}
prsims = mapply(sim, a=good_sample$a, b=good_sample$b, 
  s=exp(good_sample$log_s), MoreArgs=list(x=x_predict[,'x']))
po_pr_interval = apply(prsims, 1, quantile, c(0.05, 0.95))
```

```{r echo = FALSE, fig.height=4, fig.width=4}
plDat$ppi_lower = po_pr_interval[1,]
plDat$ppi_upper = po_pr_interval[2,]
ggplot(plDat, aes(x=x, y = median)) + geom_line(colour="#c45d52") + 
  geom_ribbon(aes(ymin=ppi_lower, ymax=ppi_upper), alpha = 0.4, fill = "#96bad3") + 
  geom_point(data = data, aes(x=x, y=y), size=0.7, colour='#555555') +
  theme_minimal() + ylab("Petal Length") + xlab('Sepal Length')
```

</div>
