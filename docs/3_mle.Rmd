---
title: "Maximum Likelihood & Optimisation"
author: "Matthew Talluto"
date: "05.11.2020"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
  beamer_presentation: default
---


```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="svg")
library(ggplot2)
library(xtable)
```

## What is a likelihood?

<div class="left lt">
* In the Zombie example, we knew the probability of being a zombie $p = 0.3$
* We wanted to know the probability of observing some number of zombies $k$ in a sample of $n=10$
* More frequently, we observe $k$ and $n$, and want to know $p$.
</div>

<div class="right rt">

```{r echo = FALSE}
dat = data.frame(k = seq(0,10))
dat$Probability = dbinom(dat$k, 10, 0.3)
ym = 0.4
pt = function(p) p + geom_bar(aes(y = Probability, x = k), stat='identity') + ylim(0,ym) + 
  scale_x_continuous(breaks=0:10, labels=0:10)
pl = ggplot(data = dat)
pt(pl)
```

</div>


## Sampling a population

<div class="left lt">
* We sampled a large population to determine the rate of zombism.
* Assume samples were random, *iid* 
* Given a sample of $n=25$, we observed $k=7$ zombies.
* Estimate $p$, the proportion of the population that is a zombie.
</div>

<div class="right rt">
</div>



## Sampling a population

<div class="left lt">
* We sampled a large population to determine the rate of zombism.
* Assume samples were random, *iid* 
* Given a sample of $n=25$, we observed $k=7$ zombies.
* Estimate $p$, the proportion of the population that is a zombie.
* *Intuitively*, we know the best estimate is $p = 7/25 = 0.28.$ Why?
</div>

<div class="right rt">
```{r echo = FALSE, warning=FALSE}
n = 25
dat = data.frame(k = rep(0:25, 2))
dat$p = c(rep(7/25, 26), rep(10/25, 26))
dat$Probability = dbinom(dat$k, 25, dat$p)
dat$col = paste0("p=",dat$p)
al = 0.85
pl = ggplot(data = dat[dat$p == 7/25,], aes(y = Probability, x = k, fill=col)) + geom_col(alpha = al) + 
  scale_x_continuous(breaks=0:25, labels=0:25)+ theme_minimal() + ylim(0,0.2) + labs(fill = "")
pl
```
</div>


## Sampling a population

<div class="left lt">
* We sampled a large population to determine the rate of zombism.
* Assume samples were random, *iid* 
* Given a sample of $n=25$, we observed $k=7$ zombies.
* Estimate $p$, the proportion of the population that is a zombie.
* *Intuitively*, we know the best estimate is $p = 7/25 = 0.28.$ Why?
* Many other values of $p$ are also possible.
</div>

<div class="right rt">
```{r echo = FALSE}
pl = ggplot(data = dat, aes(y = Probability, x = k, fill=col)) + geom_col(alpha = al, position = 'dodge') +
  scale_x_continuous(breaks=0:25, labels=0:25)+ theme_minimal() + ylim(0,0.2) + labs(fill = "")
pl
```
</div>



## Likelihood functions

<div class="left lt">
* The **likelihood function** is a function *f* of the data $X$ and model/parameters $\theta$
* Here $X = \{k,n\}$ and $\theta = \{p\}$
* $f(X,\theta)$ returns the *probability of observing* $X$, given a particular model $\theta$: $pr(X|\theta)$
* Here, the *binomial PMF* is a useful likelihood function
</div>

<div class="right rt">
* $n=25$, $k=7$

```{r}
p = seq(0,1,length.out=200)
n = 25
k = 7
dat = data.frame(p = p, lhood = dbinom(k, n, p))
pl = ggplot(dat, aes(x = p, y = lhood)) + geom_line() + 
  theme_minimal() + ylab("pr(n,k|p)")
pl
```

</div>



## Likelihood functions

<div class="left lt">
* Intuition is fine, but how do we estimate or (in some cases) solve for the maximum likelihood?
</div>

<div class="right rt">
* $n=25$, $k=7$

```{r, echo=FALSE}
pl = pl + geom_point(aes(x = 7/25, y = dbinom(7, 25, 7/25)), size = 2, colour='blue')
pl
```

</div>


## Solving for the MLE


$$
\begin{aligned}
\mathcal{L}(k,n|p) & = {n \choose k} p^k(1-p)^{(n-k)} \\
\frac{d \mathcal{L}(k,n|p)}{dp} & = {n \choose k}kp^{k-1}(1-p)^{(n-k)} - {n \choose k} p^k (n-k)(1-p)^{(n-k-1)} \\
& = 0
\end{aligned}
$$

## Solving for the MLE
$$
\begin{aligned}
{n \choose k} p^k (n-k)(1-p)^{(n-k-1)} & = {n \choose k}kp^{k-1}(1-p)^{(n-k)} \\
p(p^{k-1}) (n-k)(1-p)^{(n-k-1)} & = kp^{k-1}(1-p)(1-p)^{(n-k-1)} \\
p (n-k) & = k(1-p) \\
pn -pk & = k-pk \\
pn & = k \\
p & = \frac{k}{n} \\
\end{aligned}
$$

 

## Generalising to multiple observations
* Remember the product rule: for two independent events, $pr(A,B) = pr(A)pr(B)$
* Likelihoods are probabilities, and we like to assume each data point is independent. Thus:

$$
\mathcal{L}(X_{1..n}|\theta) = \prod_{i=1}^{n} \mathcal{L}(X_i|\theta)
$$

## Optimisation

> - In many (most) cases, analytical solutions are unavailable or impractical.
> - We turn to various algorithms for numerical optimisation


## Optimisation

* In many (most) cases, analytical solutions are unavailable or impractical.
* We turn to various algorithms for numerical optimisation

```{r}
n = 25
k = 7
lfun = function(p, n, k) {
  dbinom(k, n, p, log=TRUE) ## why log??
}
p_init = 0.5
optim(p_init, lfun, method = "Brent", n = n, k = k, control = list(fnscale = -1), lower=0, upper=1)
```


## Why Bayes?
* I want to describe some phenomenon ("model"; $\theta$)
* I have some general ("prior") knowledge about the question: $pr(\theta)$
* I gather additional information ("data"; $X$)

What is the probability that my model is correct given what I already know about it and what Iâ€™ve learned?

$$ pr(\theta | X) $$

## Applying Bayes' Theorem

* We already know an expression for this:

$$
pr(\theta | X) = \frac{pr(X|\theta)pr(\theta)}{pr(X)}
$$

## Applying Bayes' Theorem

* We already know an expression for this:

$$
pr(\theta | X) = \frac{pr(X|\theta)pr(\theta)}{pr(X)}
$$

* The goal, $pr(\theta | X)$, is called the **posterior probability of $\theta$**
* We have already seen $pr(X|\theta)$; this is the **likelihood** of the data
* $pr(\theta)$ is often called the **prior probability of $\theta$**. Could also be called "other information about $\theta$"
* What about $pr(X)$? This is the **normalizing constant**

When we do Bayesian inference, each of these terms is a full **probability distribution**


## The normalizing constant

> - For the zombie example, we had a single observation ("I tested positive"), we were able to add up all of the ways one could test positive.
> - $pr(T) = pr(T|Z)pr(Z) + pr(T|Z')pr(Z')$
> - $pr(X) = \sum_i^n pr(X|\theta_i)pr(\theta_i)$ where all possible models are in the set $n$
> - What about for continuous problems? There are infinitely many possible datasets if X is real-valued, and infinte possible models if $pr(\theta)$ is a continuous PDF.



## The normalizing constant

* For the zombie example, we had a single observation ("I tested positive"), we were able to add up all of the ways one could test positive.
* $pr(T) = pr(T|Z)pr(Z) + pr(T|Z')pr(Z')$
* $pr(X) = \sum_i^n pr(X|\theta_i)pr(\theta_i)$ where all possible models are in the set $n$
* What about for continuous problems? There are infinitely many possible datasets if X is real-valued, and infinte possible models if $pr(\theta)$ is a continuous PDF.

$$
pr(X) = \int_a^b pr(X|\theta)pr(\theta)d \theta
$$

* This integral can be challenging to compute


## Proportional Bayes' Theorem

> - $pr(X)$ is a *constant*; it adjusts the height of the distribution so that the posterior integrates to 1
> - If all we want to do is estimate the maximum value, we can safely ignore it

## Proportional Bayes' Theorem

* $pr(X)$ is a *constant*; it adjusts the height of the distribution so that the posterior integrates to 1
* If all we want to do is estimate the maximum value, we can safely ignore it

$$
pr(\theta|X) \propto pr(X|\theta)pr{\theta}
$$




## Maximum A Posteriori Estimation

* $pr(X)$ is a *constant*; it adjusts the height of the distribution so that the posterior integrates to 1
* If all we want to do is estimate the maximum value, we can safely ignore it

$$
pr(\theta|X) \propto pr(X|\theta)pr{\theta}
$$

* This is know as the **maximum a posteriori** (MAP) estimate, Bayesian equivalent to the MLE

```{r}
n = 25
k = 7
log_lfun = function(p, n, k) {
  dbinom(k, n, p, log=TRUE)
}
log_prior = function(p, alpha, beta) {
  dbeta(p, alpha, beta, log = TRUE)
}
log_posterior = function(p, n, k, alpha, beta) {
  log_lfun(p, n, k) + log_prior(p, alpha, beta)  ## why +?
}

## starting value for the optimisation
p_init = 0.5

## we choose an "uninformative" prior
alpha = 1
beta = 1
```


## Maximum A Posteriori Estimation

<div class="left lt">
```{r}
optim(p_init, log_posterior, method = "Brent", n = n, k = k, alpha = alpha, beta = beta, 
      control = list(fnscale = -1), lower=0, upper=1)
```

This prior has no influence on the posterior
</div>


<div class="right rt">
```{r echo=FALSE, figure.width = 3, figure.height = 3}
p = seq(0,1,length.out=100)
par(mar = c(5,5,0,1), bty='n', mfrow=c(2,2))
plot(p, exp(log_lfun(p, n, k)), xlab = "", ylab = "Likelihood", col = 'red', type='l', lwd=3)
plot(p, exp(log_prior(p, alpha, beta)), xlab = "", ylab = "Prior", col = 'blue', type='l', lwd=3)
plot(p, exp(log_posterior(p, n, k, alpha, beta)), xlab = "", ylab = "Unnormalized Posterior", col = 'purple', type='l', lwd=3)
mtext("p", side = 1, outer=TRUE, line=-2)
```

</div>


## Changing the prior
<div class="left lt">
```{r}
alpha = 2; beta = 2
optim(p_init, log_posterior, method = "Brent", n = n, k = k, alpha = alpha, beta = beta, 
      control = list(fnscale = -1), lower=0, upper=1)$par
```

```{r}
alpha2 = 3; beta2 = 1.5
optim(p_init, log_posterior, method = "Brent", n = n, k = k, alpha = alpha2, beta = beta2, 
      control = list(fnscale = -1), lower=0, upper=1)$par
```
This prior is informative, but relatively weak (our data has the weight equivalent to alpha=7, beta=18)

</div>


<div class="right rt">
```{r echo=FALSE, figure.width = 3, figure.height = 3}
par(mar = c(5,5,0,1), bty='n', mfrow=c(2,2))
plot(p, exp(log_lfun(p, n, k)), xlab = "", ylab = "Likelihood", col = 'red', type='l', lwd=3)
plot(p, exp(log_prior(p, alpha, beta)), xlab = "", ylab = "Prior", col = 'blue', type='l', lwd=3, ylim=c(0, 1.8))
lines(p, exp(log_prior(p, alpha2, beta2)), col = 'blue', lwd=2, lty=2)
plot(p, exp(log_posterior(p, n, k, alpha, beta)), xlab = "", ylab = "Unnormalized Posterior", col = 'purple', type='l', lwd=3)
lines(p, exp(log_posterior(p, n, k, alpha2, beta2)), col = 'purple', lwd=2, lty=2)
mtext("p", side = 1, outer=TRUE, line=-2)
```

</div>



## Changing the prior

<div class="left lt">

What if we had already conducted an identical prior sample, with 20 zombies and 5 normals?

```{r}
alpha3 = 20; beta3 = 5
optim(p_init, log_posterior, method = "Brent", n = n, k = k, alpha = alpha3, beta = beta3, 
      control = list(fnscale = -1), lower=0, upper=1)$par
```
</div>

<div class="right rt">
```{r echo=FALSE, figure.width = 3, figure.height = 3}
par(mar = c(5,5,0,1), bty='n', mfrow=c(2,2))
plot(p, exp(log_lfun(p, n, k)), xlab = "", ylab = "Likelihood", col = 'red', type='l', lwd=3)
plot(p, exp(log_prior(p, alpha3, beta3)), xlab = "", ylab = "Prior", col = 'blue', type='l', lwd=3)
plot(p, exp(log_posterior(p, n, k, alpha3, beta3)), xlab = "", ylab = "Unnormalized Posterior", col = 'purple', type='l', lwd=3)
mtext("p", side = 1, outer=TRUE, line=-2)
```

</div>


## Geting a normalized posterior

> - We often want to know the full posterior distribution
> - For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$

## Geting a normalized posterior

* We often want to know the full posterior distribution
* For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$
* For the beta-binomial, if:

$$
\begin{aligned}
pr(p) & = \mathrm{Beta}(\alpha, \beta) \\
pr(k,n | p) & = \mathrm{Binomial}(k, n, p)
\end{aligned}
$$

## Geting a normalized posterior

* We often want to know the full posterior distribution
* For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$
* For the beta-binomial, if:

$$
\begin{aligned}
pr(p) & = \mathrm{Beta}(\alpha, \beta) \\
pr(k,n | p) & = \mathrm{Binomial}(k, n, p) \\
pr(p | k,n,\alpha, \beta) &= \mathrm{Beta}(\alpha + k, \beta + n - k)
\end{aligned}
$$

## Geting a normalized posterior

* We often want to know the full posterior distribution
* For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$
* In many cases, the shape of the log posterior is appoximately quadratic (the posterior is approximately normal)
* **Laplace approximation** (or Quadratic approximation) is a method for approximating the shape of this curve


## Geting a normalized posterior

* We often want to know the full posterior distribution
* For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$
* In many cases, the shape of the log posterior is appoximately quadratic (the posterior is approximately normal)
* Laplace (or Quadratic) approximation is a method for approximating the shape of this curve

```{r, echo = FALSE}
alpost = alpha3 + k
bepost = beta3 + n - k
plot(p, dbeta(p, alpost, bepost, log=TRUE), type='l', xlab = "p", ylab = "Log Posterior", 
     col = 'purple', lwd=2, xlim=c(0.4, 0.7), ylim=c(-1.5, 2), bty='n')

## quick estimate of the mean/variance of the posterior
stdev = sqrt((alpost*bepost)/((alpost+bepost)^2 * (alpost+bepost+1)))
mu = alpost / (alpost + bepost)
## quadratic estimation
lines(p, dnorm(p, mu, stdev, log=TRUE), col='red', lwd=2, lty=2)
legend("bottomleft", legend = c("Normalized posterior", "Quadratic Estimate"), col=c("purple", "red"), lty=c(1,2), lwd=2, bty='n')
```



## Geting a normalized posterior

* We often want to know the full posterior distribution
* For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$
* In many cases, the shape of the log posterior is appoximately quadratic (the posterior is approximately normal)
* Laplace (or Quadratic) approximation is a method for approximating the shape of this curve
* When the above are unavailable, we can use simulations (e.g., MCMC)
