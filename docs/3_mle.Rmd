---
title: "Maximum Likelihood & Optimisation"
author: "Matthew Talluto"
date: "05.11.2020"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
  beamer_presentation: default
---


```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="svg")
library(ggplot2)
library(xtable)
```

## What is a likelihood?

<div class="left lt">
* In the Zombie example, we knew the probability of being a zombie $p = 0.3$
* We wanted to know the probability of observing some number of zombies $k$ in a sample of $n=10$
* More frequently, we observe $k$ and $n$, and want to know $p$.
</div>

<div class="right rt">

```{r echo = FALSE}
dat = data.frame(k = seq(0,10))
dat$Probability = dbinom(dat$k, 10, 0.3)
ym = 0.4
pt = function(p) p + geom_bar(aes(y = Probability, x = k), stat='identity') + ylim(0,ym) + 
  scale_x_continuous(breaks=0:10, labels=0:10)
pl = ggplot(data = dat)
pt(pl)
```

</div>


## Sampling a population

<div class="left lt">
* We sampled a large population to determine the rate of zombism.
* Assume samples were random, *iid* 
* Given a sample of $n=25$, we observed $k=7$ zombies.
* Estimate $p$, the proportion of the population that is a zombie.
</div>

<div class="right rt">
</div>



## Sampling a population

<div class="left lt">
* We sampled a large population to determine the rate of zombism.
* Assume samples were random, *iid* 
* Given a sample of $n=25$, we observed $k=7$ zombies.
* Estimate $p$, the proportion of the population that is a zombie.
* *Intuitively*, we know the best estimate is $p = 7/25 = 0.28.$ Why?
</div>

<div class="right rt">
```{r echo = FALSE, warning=FALSE}
n = 25
dat = data.frame(k = rep(0:25, 2))
dat$p = c(rep(7/25, 26), rep(10/25, 26))
dat$Probability = dbinom(dat$k, 25, dat$p)
dat$col = paste0("p=",dat$p)
al = 0.85
pl = ggplot(data = dat[dat$p == 7/25,], aes(y = Probability, x = k, fill=col)) + geom_col(alpha = al) + 
  scale_x_continuous(breaks=0:25, labels=0:25)+ theme_minimal() + ylim(0,0.2) + labs(fill = "")
pl
```
</div>


## Sampling a population

<div class="left lt">
* We sampled a large population to determine the rate of zombism.
* Assume samples were random, *iid* 
* Given a sample of $n=25$, we observed $k=7$ zombies.
* Estimate $p$, the proportion of the population that is a zombie.
* *Intuitively*, we know the best estimate is $p = 7/25 = 0.28.$ Why?
* Many other values of $p$ are also possible.
</div>

<div class="right rt">
```{r echo = FALSE}
pl = ggplot(data = dat, aes(y = Probability, x = k, fill=col)) + geom_col(alpha = al, position = 'dodge') +
  scale_x_continuous(breaks=0:25, labels=0:25)+ theme_minimal() + ylim(0,0.2) + labs(fill = "")
pl
```
</div>



## Likelihood functions

<div class="left lt">
* The **likelihood function** is a function *f* of the data $X$ and model/parameters $\theta$
* Here $X = \{k,n\}$ and $\theta = \{p\}$
* $f(X,\theta)$ returns the *probability of observing* $X$, given a particular model $\theta$: $pr(X|\theta)$
* Here, the *binomial PMF* is a useful likelihood function
</div>

<div class="right rt">
* $n=25$, $k=7$

```{r}
p = seq(0,1,length.out=200)
n = 25
k = 7
dat = data.frame(p = p, lhood = dbinom(k, n, p))
pl = ggplot(dat, aes(x = p, y = lhood)) + geom_line() + 
  theme_minimal() + ylab("pr(n,k|p)")
pl
```

</div>



## Likelihood functions

<div class="left lt">
* Intuition is fine, but how do we estimate or (in some cases) solve for the maximum likelihood?
</div>

<div class="right rt">
* $n=25$, $k=7$

```{r, echo=FALSE}
pl = pl + geom_point(aes(x = 7/25, y = dbinom(7, 25, 7/25)), size = 2, colour='blue')
pl
```

</div>




## Solving for the MLE

$$
\begin{aligned}
\mathcal{L}(k,n|p) & = {n \choose k} p^k(1-p)^{(n-k)} \\
\\
\frac{d \mathcal{L}(k,n|p)}{dp} & = {n \choose k}kp^{k-1}(1-p)^{(n-k)} - {n \choose k} p^k (n-k)(1-p)^{(n-k-1)} \\
0 & = {n \choose k}kp^{k-1}(1-p)^{(n-k)} - {n \choose k} p^k (n-k)(1-p)^{(n-k-1)} \\
{n \choose k} p^k (n-k)(1-p)^{(n-k-1)} & = {n \choose k}kp^{k-1}(1-p)^{(n-k)} \\
1 & = \frac{{n \choose k}kp^{k-1}(1-p)^{(n-k)}}{{n \choose k} p^k (n-k)(1-p)^{(n-k-1)}} \\
\end{aligned}
$$ 






## Solving for the MLE

$$
\begin{aligned}
1 & = \frac{{n \choose k}kp^{k-1}(1-p)^{(n-k)}}{{n \choose k} p^k (n-k)(1-p)^{(n-k-1)}} \\
1 & = \frac{k(1-p)}{p(n-k)} \\
pn - pk & = k-pk \\
pn & = k \\
p & = \frac{k}{n} \\
\end{aligned}
$$ 

## Generalising to multiple observations
* Remember the product rule: for two independent events, $pr(A,B) = pr(A)pr(B)$
* Likelihoods are probabilities, and we like to assume each data point is independent. Thus:

$$
\mathcal{L}(X_{1..n}|\theta) = \prod_{i=1}^{n} \mathcal{L}(X_i|\theta)
$$

## Optimisation

> - In many (most) cases, analytical solutions are unavailable or impractical.
> - We turn to various algorithms for numerical optimisation


## Optimisation

* In many (most) cases, analytical solutions are unavailable or impractical.
* We turn to various algorithms for numerical optimisation

```{r}
n = 25
k = 7
lfun = function(p, n, k) {
  dbinom(k, n, p, log=TRUE) ## why log??
}
p_init = 0.5
optim(p_init, lfun, method = "Brent", n = n, k = k, control = list(fnscale = -1), lower=0, upper=1)
```


