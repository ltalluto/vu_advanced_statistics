---
title: "Spatial Models & Gaussian Processes"
author: "Matthew Talluto"
date: "14.05.2021"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
---

```{r setup, include=FALSE, results = "hide"}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png")

library(lme4)
library(ggplot2)
library(sf)
library(mvtnorm)
library(raster)
library(gstat)
library(data.table)
library(rstan)
library(loo)
library(bayesplot)

options(mc.cores = parallel::detectCores())

# # library(igraph)
# # library(ggnetwork)
# 
# # library(gridExtra)
cols = c("#F8766D", "#7CAE00", "#00BFC4", "#C77CFF")

```


## Presenting Bayesian Models: Methods Section
* All detail needed for a non-Bayesian model
    - What is the response, how does it connect to the hypotheses?
    - Distributional assumptions (likelihood distribution, link function)
    - Predictors, how they were chosen
    - Model evaluation, model selection, model comparison, etc
    - Models considered but not presented
    - For hierarchical models, how was model structure chosen, grouping variables
* Additionally, for Bayesian models
    - Choice of prior distributions and hyperparameters
    - Justify prior choice (even if regularising)
    - [Stan manual](https://mc-stan.org/users/documentation/) and [vignettes](https://cran.r-project.org/web/packages/rstanarm/vignettes/priors.html) for `rstanarm` are good starting points
    - Software and algorithm used for fitting the model, with citations
* For MCMC/HMC:
    - How many chains?
    - How many samples?
    - How did you assess convergence/validity?
* different rules for empirical models vs papers creating a new method




## The problem of nonindependence

* Recall that all of our linear models have an **independence assumption**

$$
\begin{aligned}
	\mathrm{L}[\mathbb{E}(y)] & = \alpha + \beta{\mathbf{X}} \\
	\theta & = \mathcal{f}[\mathbb{E}(y)] \\
	y & \sim \mathcal{D}(\theta) \\
	\\
	\mathrm{pr}(y_i | \color{red}{y_{-i}}, \alpha, \beta, \mathbf{X}) & \equiv 
	\mathrm{pr}(y_i | \alpha, \beta, \mathbf{X}) 
\end{aligned}
$$

* This assumption is what allows us to compute the log-likelihood of all the data as the sum of the log-likelihoods of individual data points


## Nonidependence consequences

* We **must** incorporate nonindependence in the model
   - Potentially biased parameter estimates
   - Standard errors will be biased low
   - *p*-values, if used, will also be too small
   - Potential for misspecification (important effects will appear unimportant, unimportant effects appear important)
   

## Reducing nonindependence
> - Add important *x*-variables and remove unimportant ones (but how can we know?)
> - Incorporate known structure into the model using hierarchical terms
> - Model covariance directly, estimating it from the data





## The random intercepts model
<div class="left lt">
* Mixed models allow us to relax the conditional independence
* Individual observations covary by means of shared group-level parameters
</div>


<div class="right rt">


```{r irisglm, fig.height = 5, fig.width = 5, echo=FALSE}
library(ggplot2)
data(iris)
mod = glm(Sepal.Width ~ Sepal.Length, data = iris)
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, colour=Species)) + geom_point() + 
	geom_abline(intercept = coef(mod)[1], slope = coef(mod)[2], size=1.5, colour='black') + 
	scale_colour_manual(values=cols) + theme_minimal()
```

</div>



## The random intercepts model
<div class="left lt">
* Mixed models allow us to relax the conditional independence
* Individual observations covary by means of shared group-level parameters

When observation $i$ is in group $j$

$$
\begin{aligned}
\mathbb{E}(y_i) & = \alpha + \gamma_j + \beta X \\
y & \sim \mathcal{N}\left (\mathbb{E} \left (y \right ), \sigma \right) \\
\gamma & \sim \mathcal{N}(0, \sigma_\gamma)
\end{aligned}
$$

$\gamma$ models an **offset** from the global intercept (hence prior mean of 0)

</div>


<div class="right rt">


```{r irisglmm, fig.height = 5, fig.width = 5, echo=FALSE}
mod = lmer(Sepal.Width ~ Sepal.Length + (1|Species), data = iris)
a_mm = coef(mod)$Species[,1]
b_mm = coef(mod)$Species[1,2]
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, colour=Species)) + geom_point() + 
	geom_segment(aes(x = 4.0, xend = 6.0, y = a_mm[1] + b_mm*4.0, yend = a_mm[1] + b_mm*6.0), size=0.8, colour=cols[1]) + 
	geom_segment(aes(x = 4.6, xend = 7.1, y = a_mm[2] + b_mm*4.6, yend = a_mm[2] + b_mm*7.1), size=0.8, colour=cols[2]) + 
	geom_segment(aes(x = 4.6, xend = 8.0, y = a_mm[3] + b_mm*4.6, yend = a_mm[3] + b_mm*8.0), size=0.8, colour=cols[3]) + 
	scale_colour_manual(values=cols) + theme_minimal()
```

</div>



## Group membership via spatial neighbours
<div class="left lt">
* We can define a hierarchical group as an observation + its neighbours
* Every data point now has a separate group and a different spatial random effect!
* We define a **neighbourhood matrix** $w$, such that $w_{ij} = 1$ if $i$ and $j$ are neighbours
* The spatial random effect of observation $i$ has a mean equal to the average spatial random effect of its neighbours
* The variance for an individual spatial unit decreases with the number of neighbours $\nu_i$

</div>


<div class="right rt">
$$ 
\begin{aligned}
\mathbb{E}(y_i) & = \alpha + \gamma_\color{red}{i} + \beta X \\
y & \sim \mathcal{N}\left (\mathbb{E} \left (y \right ), \sigma \right) \\
\gamma_i & \sim \mathcal{N} \left( \frac{\sum w_{ij} \gamma_j}{\nu_i}, \frac{\sigma_\gamma}{\nu_i} \right)
\end{aligned}
$$


```{r getscotlip, include=FALSE}
scotlip = st_read("exercises/data/scotlip/", "scotlip")
```


```{r scotland, echo=FALSE, message=FALSE, warning=FALSE}
pl = ggplot(scotlip, aes(fill = CANCER)) + geom_sf(color='white') + 
	scico::scale_fill_scico(palette = "bilbao") + 
	theme_minimal() + 
	theme(axis.line=element_blank(), axis.text.x=element_blank(), axis.text.y=element_blank(),
		  axis.ticks=element_blank(), axis.title.x=element_blank(), axis.title.y=element_blank(),
		  panel.background=element_blank(), panel.border=element_blank(), 
		  panel.grid.major=element_blank(),
	      panel.grid.minor=element_blank(), plot.background=element_blank()) + 
	labs(fill="Lip Cancer Incidence")
pl
```


</div>


## Continuous spatial models
<div class="left lt">

* With point data, all points are neighbours, but some neighbours are more important than others
* The weights matrix $w$ now has no zeros, instead we weight based on some function of the distance
* Here, I use $w_{ij} = \frac{1}{d_ij}$

```{r wtsmat, include=FALSE}
set.seed(123)
n = 10
dat = data.frame(
	name = 1:n,
	x = runif(n, -1, 1),
	y = runif(n, -1, 1))
dmat = as.matrix(dist(dat))

s = 0.5

matern = function(d, s = 0.5, rho = 0.8) {
	s^2 * (1 + (sqrt(3) * d)/rho) * exp(-(sqrt(3)*d)/rho)
}

sig = matern(dmat, s = s)
dat$z = as.vector(rmvnorm(1, rep(0, n), sig))

# xx(1/dmat, e=1, mtype="pmatrix")

```
<span style="font-size: small;">
$$ w =
 \begin{pmatrix}
 Inf & 0.58 & 0.48 & 0.30 & 0.22 & 0.20 & 0.16 & 0.14 & 0.12 & 0.11 \\ 
 0.58 & Inf & 0.75 & 0.49 & 0.32 & 0.23 & 0.20 & 0.17 & 0.14 & 0.12 \\ 
 0.48 & 0.75 & Inf & 0.72 & 0.39 & 0.32 & 0.24 & 0.19 & 0.17 & 0.14 \\ 
 0.30 & 0.49 & 0.72 & Inf & 0.73 & 0.37 & 0.32 & 0.24 & 0.20 & 0.16 \\ 
 0.22 & 0.32 & 0.39 & 0.73 & Inf & 0.39 & 0.46 & 0.33 & 0.24 & 0.19 \\ 
 0.20 & 0.23 & 0.32 & 0.37 & 0.39 & Inf & 0.52 & 0.32 & 0.30 & 0.24 \\ 
 0.16 & 0.20 & 0.24 & 0.32 & 0.46 & 0.52 & Inf & 0.77 & 0.50 & 0.30 \\ 
 0.14 & 0.17 & 0.19 & 0.24 & 0.33 & 0.32 & 0.77 & Inf & 0.75 & 0.35 \\ 
 0.12 & 0.14 & 0.17 & 0.20 & 0.24 & 0.30 & 0.50 & 0.75 & Inf & 0.62 \\ 
 0.11 & 0.12 & 0.14 & 0.16 & 0.19 & 0.24 & 0.30 & 0.35 & 0.62 & Inf \\ 
 \end{pmatrix} $$
 </span>
 
 
* This can be applied to covariance in many situations!
    - Genetic/phylogenetic relatedness
    - Temporal autocorrelation
    - Functional similarity
* Prior mixed models had **unordered** (i.e., nominal) groups
* Now the grouping variable is continuous
</div>

<div class="right rt">

```{r cspatialmod, echo=FALSE, fig.height = 5, fig.width = 5.5}
make_line = function(pl, i, j, dat) {
	mpx = mean(dat$x[c(i,j)])
	mpy = mean(dat$y[c(i,j)])
	len = round(sqrt((dat$x[i] - dat$x[j])^2 + (dat$y[i] - dat$y[j])^2),2)
	pl = pl + geom_segment(aes(x = x[i], y = y[i], xend = x[j], yend = y[j]), linetype='dashed')
	pl + geom_text(x = mpx-0.04, y = mpy + 0.04, label = len, size = 2.1, colour = '#fb9a99')
}

pl = ggplot(dat, aes(x=x, y=y, color=z))
i = 6
for(j in c(1:5, 7:10))
	pl = make_line(pl, i, j, dat)

pl = pl + geom_point(size=2.5) +
	geom_text(label=dat$name, colour='black', nudge_x=-0.04, size=2.5) + 
	scico::scale_colour_scico(palette = "lajolla") + theme_minimal() + xlab("Longitude") + 
	ylab("Latitude")
pl
```

</div>


## Fully parameterized CAR
<div class="left lt">

$$ 
\begin{aligned}
\mathbb{E}(y_i) & = \alpha + \gamma_{i} + \beta X \\
y & \sim \mathcal{N}\left (\mathbb{E} \left (y \right ), \sigma \right) \\
\gamma_i & \sim \mathcal{N} \left( \frac{\sum_{j=1}^{n} w_{ij} \gamma_j}{\sum_{j=1}^{n}w_{ij}}, \sigma_\gamma \right)
\end{aligned}
$$

**Problem**

* Computing $\gamma$ becomes problematic as $n$ increases
* Too many parameters!
   - $n=10$: 40 (pseudo-)parameters
   - $n=100$: 4900
   - $n=1000$: ~5e5
* We need another hierarchical layer to reduce computation

</div>

<div class="right rt">

```{r cspatialmod2, echo=FALSE, fig.height = 5, fig.width = 5.5}
pl
```
</div>


## Multivariate normal parameterization

* The previous model can be reparameterized with a multivariate normal
* The multivariate normal is parameterized with a **mean vector** and a **variance-covariance** matrix
* Diagonal elements are the variance (usually assumed to be constant for all points)
* Off diagonals are covariance between two points
* The outcomes here are the result of a **Gaussian Process**, and the model is called **GP Regression**


$$
\begin{align}
	\mathbb{E}(y) & = \alpha + \beta \mathbf{X} \\
	y & \sim \mathcal{MN} \left( \mathbb{E}(y), \Sigma \right) \\
	\Sigma_{ij} & = \frac{\rho_{ij}}{d_{ij}}
\end{align}
$$

* **Note**: The GLM is a special case of a GP with covariance = 0!





## Covariance functions
<div class="left lt">

* We can add a hyperparemeter layer to reduce the number of parameters we need for $\Sigma$
* In effect, we compute a regression model with $\Sigma$ as the response and the spatial (or other) distance as the predictor
* Instead of treating covariance as a **random** variable, we insert the **expectation** of this regression model (which is a deterministic function of its parameters)



</div>

<div class="right rt">
```{r fig.height=4, fig.width=4, echo=FALSE}
x = seq(0,10,0.01)
plot(x, matern(x), bty='n', xlab="Distance", ylab = "Covariance", type='l', lwd=1.5)
```


</div>



## Covariance kernel functions
<div class="left lt">


* We usually use **kernel** functions to describe the shape of the covariance-distance relationship
* A common kernel for spatial models is the Matérn$^{3/2}$ function:

* $\sigma$: standard deviation 
* $\rho$: lengthscale or correlation length
	- controls how quickly covariance decays with distance

$$
	\Sigma_{ij} = \sigma^2 \left( 1 + \frac{\sqrt{3}d_{ij}}{\rho}\right)\left(\mathrm{e}^\frac{-\sqrt{3}d_{ij}}{\rho} \right)
$$

* Note that this only requires 2 hyperparameters!


</div>

<div class="right rt">
```{r fig.height=4, fig.width=4, echo=FALSE}
x = seq(0,10,0.01)
plot(x, matern(x), bty='n', xlab="Distance", ylab = "Covariance", type='l', lwd=1.5)
```

</div>



## GP GLMs

<div class="left lt">
* The previous model assumed a multivariate normal for the response
* What do do when $y$ is not normal?
</div>

## GP GLMs

<div class="left lt">

* The previous model assumed a multivariate normal for the response
* What do do when $y$ is not normal?
* We can reparameterize again, adding a latent variable in the form of a **Gaussian Random Field**

$$
\begin{align}
	\mathrm{L}[\mathbb{E}(y)] & = \alpha + \beta \mathbf{X} + \gamma \\
	\theta &= \mathcal{f}[\mathbb{E}(y), \phi] \\
	y & \sim \mathcal{D}(\theta) \\
	\gamma & \sim \mathcal{MN} \left( \mathbf{0}, \Sigma \right) \\
	\Sigma_{ij} & = \sigma^2 \left( 1 + \frac{\sqrt{3}d_{ij}}{\rho}\right)\left(\mathrm{e}^\frac{-\sqrt{3}d_{ij}}{\rho} \right)
\end{align}
$$

</div>

<div class="right rt">

```{r include=FALSE}
output = raster(nrows=100, ncol=100, xmn=-1, xmx=1, ymn=-1, ymx=1)
out_pts = coordinates(output)
out_pts = st_as_sf(data.frame(out_pts), coords=c('x', 'y'))
dat_sf = st_as_sf(dat, coords=c('x', 'y'))
out_z = krige(z~1, locations=dat_sf, newdata = out_pts)
outdat = data.frame(x = st_coordinates(out_z)[,1], y = st_coordinates(out_z)[,2], phi = out_z$var1.pred)
```


```{r fig.height=3.5, fig.width=4.5, echo=FALSE}
ggplot(outdat, aes(x=x,y=y)) + geom_raster(aes(fill=phi)) + 
	scico::scale_fill_scico(palette = "bilbao") + 
	geom_point(data=dat, aes(x=x, y=y, fill=z), colour="black", pch=21, size=3) + 
	labs(fill="γ") + theme_minimal()
```

```{r fig.height=3.5, fig.width=4.5, echo=FALSE}

pl = ggplot(dat, aes(x=x, y=y, color=z))
pl = pl + geom_point(size=2.5) +
	scico::scale_colour_scico(palette = "bilbao")+
	xlab("Longitude") + ylab("Latitude") + labs(fill="y") + theme_minimal()
pl

```

</div>



## Implementation

* The GP in Stan generally performs well
* Feasible for up to several hundred data points
* We add a `functions` block before `data` to implement our covariance function
* We pass a coordinate matrix to stan, then compute distance weights
* We employ a few "tricks" to keep the model numerically stable

## Implementation

```{stan output.var="tsuga_gp", eval=FALSE}
functions {
	matrix matern_cov (matrix dist, real rho, real sigma, real delta) {
		int NR = rows(dist);
		matrix [NR,NR] result;
		
		for(i in 1:(NR-1)) {
			result[i,i] = sigma^2 + delta;
			for(j in (i+1):NR) {
				real mpar = (sqrt(3.0) * dist[i,j])/(rho);
				result[i,j] = sigma^2 * (1 + (mpar)) * (exp(-1*mpar));
				result[j,i] = result[i,j];
			}
		}
		result[NR,NR] =  sigma^2 + delta;
		return(result);
	}
}
data {
	int <lower=1> n; // number of sites 
	int <lower=1> k; // number of predictors for the non-spatial process

	vector [n] y; // response
	matrix [n,k] x; // predictor variables
	matrix [n,2] coords; // spatial coordinates, x first, then y
}
transformed data {
	// this is a fairly memory-intensive parameterisation
	// with large datasets, other parameterisations will be needed
	// we only fill in the upper triangle, we only need that part of the matrix
	matrix [n,n] d; // euclidean distance matrix
	real delta = 1e-9; // small addition to prevent numerical problems

	for(i in 1:(n-1)) {
		for(j in (i+1):n) {
			d[i,j] = sqrt((coords[i,1] - coords[j,1])^2 + (coords[i,2] - coords[j,2])^2);
		}
	}
}
parameters {
	// hyperparameters
	real <lower=0> rho; // spatial process lengthscale
	real <lower=0> sigma; // error term in VCV matrix
	real <lower=0> sig_y; //gaussian error term for y

	// regression parameters
	real alpha;
	vector [k] beta;
	
	// scaled latent GP effect
	vector [n] eta;
}
transformed parameters {
	vector [n] mu; // expectation of y
	{
		// compute variance-covariance matrix due to euclidean distance
		matrix[n, n] Sig; // variance-covariance matrix
		matrix[n, n] L_Sig; // cholesky decomposition of VCV matrix
		vector [n] gamma; // additive effect of GP, at response scale
		
		Sig = matern_cov(d, rho, sigma, delta);
		L_Sig = cholesky_decompose(Sig);
		gamma = L_Sig * eta;
		mu = alpha + x * beta + gamma;
	}
}
model {
	rho ~ inv_gamma(5, 5);
	alpha ~ normal(0, 10);
	beta ~ normal(0, 5);
	sigma ~ normal(0, 5);
	eta ~ std_normal();

	y ~ normal(mu, sig_y);
}
```


