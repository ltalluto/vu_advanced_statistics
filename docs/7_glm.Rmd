---
title: "Generalised Linear Modelling"
author: "Matthew Talluto"
date: "05.05.2022"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
  beamer_presentation: default
---


```{r setup, include=FALSE, results = "hide"}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png")

library(rethinking)
data(Howell1)
library(ggplot2)
library(igraph)
library(ggplot2)
library(ggnetwork)
library(data.table)
Howell1 = as.data.table(Howell1)

cols = c("#F8766D", "#7CAE00", "#00BFC4", "#C77CFF")

```

## More linear models

<div class="left lt">

* Our linear model used a single x-variable: $\mathbb{E}(y) = a + bx$
* It is trivial to add additional predictors to a model: $\mathbb{E}(y) = a + b_1x_1 + b_2x_2 + \dots + b_nx_n$

</div>

## More linear models

<div class="left lt">
* Our linear model used a single x-variable: $\mathbb{E}(y) = a + bx$
* It is trivial to add additional predictors to a model: $\mathbb{E}(y) = a + b_1x_1 + b_2x_2 + \dots + b_nx_n$
* You can also easily add a "curve" in the relationship between x and y by transforming x:

$$
\mathbb{E}(y) = a + b_1x + b_2x^2
$$

* Use caution: curves can predict silly things
   - does it make sense that height decreases after a certain weight?
* This is still a linear model. Why?

</div>

<div class="right rt">
```{r, echo = FALSE}
fit = lm(height ~ weight + I(weight^2), data = Howell1)
pldat = data.frame(weight = seq(min(Howell1$weight), max(Howell1$weight), length.out=200))
pldat$height = predict(fit, newdata = pldat)
ggplot(Howell1, aes(x = weight, y = height)) + geom_point() + 
  geom_line(data = pldat, aes(x = weight, y = height), size=1.5, col='blue') + theme_minimal()
```
</div>


## Categorical Variables

<div class="left lt">
* Perhaps we want to model a categorical variable based on age.


```{r}
Howell1[, age_c := factor(ifelse(age <= 12, "child", 
          ifelse(age > 22, "adult", "young")))]
Howell1
```

</div>

<div class="right rt">

```{r echo = FALSE}
pl = ggplot(Howell1, aes(x=weight, y = height, colour=age_c)) + 
          geom_point() + theme_minimal()
pl
```

</div>




## Categorical Variables

<div class="left lt">
* Perhaps we want to model a categorical variable based on age.
* `lm` and other tools often handle this automatically. 
* To reproduce what they do, for $k$ categories we need to use $k-1$ *dummy variables* with 0-1 coding
* Add parameters to our linear model for each dummy variable


```{r}
Howell1[, ac_child := as.integer(age_c == "child")]
Howell1[, ac_young := as.integer(age_c == "young")]
Howell1

```

</div>





<div class="right rt">

```{r}
## equivalent of lm(height ~ age_cat + weight)
log_liklihood1 = function(params, data) {
  mu = params['a'] + params['b_child']*data[['ac_child']] + 
    params['b_young']*data[['ac_young']] + 
    params['b_weight']*data[['weight']]
  sum(dnorm(data[['height']], mu, params['s'], log=TRUE))
}

## equivalent of lm(height ~ age_cat * weight)
log_liklihood2 = function(params, data) {
  mu = params['a'] + params['b_child']*data[['ac_child']] + 
    params['b_young']*data[['ac_young']] + 
    params['b_weight']*data[['weight']] + 
    params['b_cw']*data[['ac_child']]*data[['weight']] + 
    params['b_yw']*data[['ac_young']]*data[['weight']]
  sum(dnorm(data[['height']], mu, params['s'], log=TRUE))
}
```

</div>




## A general statement for mu

* Model statements, function signatures get ugly fast
* We can simplify the notation (and the computation) a lot using matrices
* $\mathbf{X}$ is now a **matrix** with $n$ rows (one per data point) and $k$ columns (one per predictor)
* This includes transformations; if we want $weight^2$ in our model, we add a column in $\mathbf{X}$ for it
* $\mathbf{B}$ is a parameter vector of length $k$, one parameter to go with each x-variable

$$
\mathbb{E}(y) = a + \mathbf{B}\mathbf{X}
$$


## A general statement for mu

* Model statements, function signatures get ugly fast
* We can simplify the notation (and the computation) a lot using matrices
* $\mathbf{X}$ is now a **matrix** with $n$ rows (one per data point) and $k$ columns (one per predictor)
* This includes transformations; if we want $weight^2$ in our model, we add a column in $\mathbf{X}$ for it
* $\mathbf{B}$ is a parameter vector of length $k$, one parameter to go with each x-variable
* This likelihood is now general; can use it with any $X$, any set of predictors

```{r}
log_liklihood3 = function(a, B, s, y, X) {
  mu = a + X %*% B  ## %*% is matrix multiplication, returns a vector length n
  sum(dnorm(height, mu, s, log=TRUE))
}
```



## Changing the distribution of y

* What if $y$ doesn't come from a normal distribution?
* There is nothing saying this is fixed
* For some generic distribution $\mathcal{D}$, we can say:

$$
\begin{aligned}
\mathbb{E}(y) & = a + \mathbf{B}\mathbf{X} \\
y & \sim \mathcal{D}(\mathbb{E}(y), ...)
\end{aligned}
$$




## Example: Tree mortality as a function of temperature

* We already modelled tree mortality using a binomial likelihood. It's easy to add a linear model

<div class="left lt">

```{r}
tsuga = readRDS("exercises/data/tsuga.rds")
tsuga[, 4:8]

```
</div>

<div class="right rt">

```{r, warning=FALSE, echo=FALSE}
pl = ggplot(tsuga, aes(x=annual_mean_temp, y=died/n)) + geom_point(aes(size=n)) + theme_minimal() +
  scale_size_continuous(range = c(0,3)) + xlab("Annual Mean Temperature") + ylab("Proportion dying")
pl
```
</div>



## Tree mortality log posterior


<div class="left lt">
$$
\begin{aligned}
\mathbb{E}(died/n) &= p  = a + bT \\
died & \sim \mathrm{Binomial}(p, n)
\end{aligned}
$$

```{r}
log_posterior = function(params, data) {
  # parameters for the regression model
  a = params['a']
  b = params['b']
  n = data[['n']]
  died = data[['died']]
  temperature = data[['annual_mean_temp']]
  
  # the probability of death changes with temperature
  p = a + b*temperature

  # liklihood
  logpr = sum(dbinom(died, n, p, log=TRUE))

  # priors
  logpr = logpr + dnorm(a, 0, 20, log=TRUE) + dnorm(b, 0, 5, log=TRUE)
  logpr
}
```
</div>

<div class="right rt">

```{r, warning=FALSE, echo=FALSE}
gr = graph_from_literal(a-+p, b-+p, T-+p, p-+died, n-+died)
V(gr)$type = c("random", "deterministic", "random", "deterministic", "random", "deterministic")
V(gr)$source = c("unknown", "unknown", "unknown", "known", "known", "known")
layout = matrix(c(0,2,  0.5,1, 0.5,2, 1,2, 1.25,0, 1.5,2), byrow=TRUE, ncol=2)
nt = ggnetwork(gr, layout=layout)
grpl = ggplot(nt, aes(x = x, y = y, xend = xend, yend = yend)) + 
   geom_edges(colour="gray50", arrow=arrow(length = unit(6, "pt"), type = "closed")) + 
   theme_blank() + geom_nodes(aes(color=type, shape = source), size=6) + 
   geom_nodelabel(aes(label = name), fontface = "bold", nudge_x=-0.1)
grpl


```
</div>




## Fitting the model


<div class="left lt">
$$
\begin{aligned}
\mathbb{E}(died/n) &= p  = a + bT \\
died & \sim \mathrm{Binomial}(p, n)
\end{aligned}
$$

* Running optim produces warnings, but seems to work.

```{r}
fit = optim(c(a=0.5, b=0), log_posterior, data = tsuga[n > 0], 
            method = "Nelder-Mead", control=list(fnscale=-1))

```
</div>

<div class="right rt">

```{r, warning=FALSE, echo=FALSE, fig.height=4, fig.width=4.5}
pl = pl + geom_abline(intercept = fit$par['a'], slope=fit$par['b'], colour=cols[3], size = 1.5) + 
  xlim(0, 25)
pl
```

```{r, warning=FALSE, echo=FALSE, fig.height=4, fig.width=4.5}
grpl

```
</div>



## Link functions

<div class="left lt">

* We need a way to map (or *link*) sensible values for $\mathbb{E}(y)$ to a linear equation $a + \mathbf{BX}$
* The linear equation is defined for all real numbers: $(-\infty, \infty)$, while $\mathbb{E}(y)$ is in $[0,1]$
* For probabilities, the *logit* function works well

$$
\begin{aligned}
\log \frac{p}{1-p} & = a + \mathbf{BX} \\
\mathrm{logit} (p) & = a + \mathbf{BX} \\
p & = \mathrm{logit}^{-1} (a + \mathbf{BX}) \\
p & = \frac{e^{a + \mathbf{BX}}}{1 + e^{a + \mathbf{BX}}}
\end{aligned}
$$

* In R, we can use `p = plogis(a + X %*% B)`

</div>

<div class="right rt">
```{r echo=FALSE}

xx = seq(-7, 7, length.out=200)
pp = plogis(xx)
par(mar=c(4.5, 5, 0.5, 0.5))
plot(xx, pp, xlab = "X", ylab = expression(logit^{-1}~X), type='l', bty='n')
```
</div>


## Binomial-logistic regression

<div class="left lt">

```{r}
log_posterior = function(params, data) {
  # parameters for the regression model
  a = params['a']
  b = params['b']
  n = data[['n']]
  died = data[['died']]
  temperature = data[['annual_mean_temp']]
  
  # the probability of death changes with temperature
  p = plogis(a + b*temperature)

  # liklihood
  logpr = sum(dbinom(died, n, p, log=TRUE))

  # priors
  logpr = logpr + dnorm(a, 0, 20, log=TRUE) + dnorm(b, 0, 5, log=TRUE)
  logpr
}
fit = optim(c(a=0.5, b=0), log_posterior, data = tsuga[n > 0], 
            method = "Nelder-Mead", control=list(fnscale=-1), hessian=TRUE)

vcv = solve(-fit$hessian)
fit$par

```
</div>

<div class="right rt">

```{r echo = FALSE, warning=FALSE}
library(mvtnorm)
samples = rmvnorm(5000, fit$par, vcv)


predictions = data.frame(x = seq(0, 25, length.out=500))
predictions$y = plogis(fit$par['a'] + fit$par['b'] * predictions$x)
predictions$lower = apply(apply(samples, 1, function(p) plogis(p[1] + p[2] * predictions$x)), 1, quantile, 0.05)
predictions$upper = apply(apply(samples, 1, function(p) plogis(p[1] + p[2] * predictions$x)), 1, quantile, 0.95)
predictions$temperature = NA
predictions$died = NA
predictions$n = NA
predictions = rbind(predictions, data.frame(x = NA, y = NA, lower = NA, upper = NA, 
              temperature = tsuga$annual_mean_temp, died = tsuga$died, n = tsuga$n))

pl = ggplot(predictions) + geom_ribbon(data=predictions, aes(x=x, ymin=lower, ymax=upper), fill=cols[1], alpha = 0.5)
pl = pl + geom_line(aes(x=x, y=y), size = 1.2, colour=cols[1])
pl = pl + geom_point(aes(x=temperature, y=died/n, size=n)) + theme_minimal()
pl = pl + scale_size_continuous(range = c(0,3)) + xlab("Annual Mean Temperature") + ylab("Proportion dying")

pl
```
</div>







## Link functions

* More generally, we can say that our observations $y$ are drawn from some 
distribution $\mathcal{D}$ that has parameters $\theta$
* The distribution parameters $\theta$ are related to the expected value of $y$, $\mathbb{E}(y)$, and
usually a precision/dispersion parameter $\phi$
* There exists a **link function** $\mathrm{L}(y)$ that maps the expectation of $y$ onto the linear model

$$
\begin{aligned}
\mathbb{E}(y) & = \mathrm{L}^{-1}\left (a + \mathbf{B}\mathbf{X} \right )\\
\theta &= \mathcal{f}(\mathbb{E}[y], \phi)\\
y & \sim \mathcal{D}(\theta)
\end{aligned}
$$

* This is the **generalised linear model**, suitable for many applied problems
* Linear regression is a special case of the GLM, where
   - $\mathcal{D}$ is a normal distribution with mean $\mathbb{E}(y)$ and constant variance $\sigma^2 = 1/phi$
   - $\mathrm{L}(y)$ is the **identity function**: $\mathrm{L}(y) = y$



## Canonical link functions

Many distributions have so-called canonical link functions

* **Normal**: identity---`mu = a + X %*% B`
* **Binomial**: logit---`p = plogis(a + X %*% B)`
* **Poisson**: log---`lambda = exp(a + X %*% B)`
* **Exponential**: inverse---`lambda = 1/(a + X %*% B)`

Other distributions have typical link functions, but need reparameterization

* **Negative Binomial**: log---`mu = exp(a + X %*% B)`
* **Beta**: logit, or probit---`mu = pnorm(a + X %*% B)`
* **Gamma**: inverse---`mu = 1/(a + X %*% B)`





## Count Data: Poisson

* For counts where the number of trials is unkown or nonsensical

$$
\begin{aligned}
\lambda & = \exp(a + \mathbf{BX}) \\
y & \sim \mathrm{Poisson}(\lambda)
\end{aligned}
$$

**Constraint:** $\sigma^2_{y|\mathbf{X}} = \mathbb{E}(y|\mathbf{X}) = \lambda$

* Sometimes, *exposure* varies by observation.
   - Different size plots
   - Different observation times
   - We add an exposure variable in this case, **u**
   - In traditional modelling, $\log u$ is called the *offset*
   
$$
y \sim \mathrm{Poisson}(u\lambda)
$$


## Overdispersed counts: Negative Binomial

* For counts where the mean and variance are not equal
* Typical in ecological count data (e.g., abundances)

$$
\begin{aligned}
\mu & = \exp(a + \mathbf{BX}) \\
y & \sim \mathrm{NB}(\mu, \phi)
\end{aligned}
$$

$\phi$ is called the dispersion parameter, and is related to the variance:

$$
\sigma^2_{y|\mathbf{X}} = \mu + \frac{\mu^2}{\phi}
$$



## Proportions: Beta

* When we have true proportions (i.e., a number of trials is unavailable)
* Also used for Binomial problems where the number of trials is missing/unrecorded
* If number of trials is available, **always** prefer Binomial GLM

$$
\begin{aligned}
\mu & = \mathrm{logit}^{-1}(a_\rho + \mathbf{B_\rho X_\rho}) \\
\phi & = \exp(a_\phi + \mathbf{B_\phi X_\phi}) & \mathrm{\small optionally} \\
\alpha & = \mu\phi \\
\beta & = (1 - \mu)\phi \\
y & \sim \mathrm{Beta}(\alpha, \beta)
\end{aligned}
$$

### Caveats & Hints

* $\phi$ is the **precision**; $\sigma^2_{y|\mathbf{X}} = \frac{\mu(1-\mu)}{\phi + 1}$
* Some software allows you to parameterise the beta directly, using $\mu$ and $\phi$
* All *observations* of $y$ must be on (0, 1). Values of exactly 0 or 1 are not allowed
* Finite mixtures can be used if the data contain 0s and 1s.
* $\alpha$: expected number of successes when sampling $\alpha + \beta$ trials
* $\beta$: expected number of failures


## Continuous, strictly positive: Gamma

* Often used for costs, waiting times
   - ecosystem services modelling
   - animal movement
   - life expectency
* Generally assume the coefficient of variation is constant
   - can relax this assumption by modelling $\phi$ as a function of covariates

$$
\begin{aligned}
\mu & = \frac{1}{(a + \mathbf{B X})} & OR \\
\mu & = \exp(a + \mathbf{B X}) \\
\\
\alpha & = \frac{\mu^2}{\phi} \\
\beta & = \frac{\mu}{\phi} \\
y & \sim \mathrm{Gamma}(\alpha, \beta)
\end{aligned}
$$

