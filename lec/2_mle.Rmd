---
title: "Maximum Likelihood & Optimisation"
author: "M. Talluto"
date: "24.11.2023"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
    self_contained: false
    lib_dir: lib
  beamer_presentation: default
---


```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png")
library(ggplot2)
library(xtable)
```

## What is a likelihood?

::: {.columns}
:::: {.column}
* In the Zombie example, we knew the probability of being a zombie: $p_z = 0.3$
* We wanted to know the probability of observing some number of zombies $k$ in a sample of $n=10$
* More typically, we would observe $k$ and $n$ via *sampling*, and want to estimate $p_z$.
::::
:::: {.column}

```{r echo = FALSE}
dat = data.frame(k = seq(0,10))
dat$Probability = dbinom(dat$k, 10, 0.3)
ym = 0.4
pt = function(p) p + geom_bar(aes(y = Probability, x = k), stat='identity') + ylim(0,ym) + 
  scale_x_continuous(breaks=0:10, labels=0:10)
pl = ggplot(data = dat)
pt(pl)
```

::::
:::


## Sampling a population

::: {.columns}
:::: {.column}
* We sampled a large population to determine the rate of zombism.
* Assume samples were random, *iid* 
* Given a sample of $n=25$, we observed $k=7$ zombies.
* Estimate $p_z$, the proportion of the population that is a zombie.
::::
:::: {.column}
::::
:::



## Sampling a population

::: {.columns}
:::: {.column}
* We sampled a large population to determine the rate of zombism.
* Assume samples were random, *iid* 
* Given a sample of $n=25$, we observed $k=7$ zombies.
* Estimate $p_z$, the proportion of the population that is a zombie.
* **Use intuition:** We know the best estimate is $p_z = 7/25 = 0.28$. Why?
::::
:::: {.column}
```{r echo = FALSE, warning=FALSE}
n = 25
dat = data.frame(k = rep(0:25, 2))
dat$p = c(rep(7/25, 26), rep(10/25, 26))
dat$Probability = dbinom(dat$k, 25, dat$p)
dat$col = paste0("p_z=",dat$p)
al = 0.85
pl = ggplot(data = dat[dat$p == 7/25,], aes(y = Probability, x = k, fill=col)) + geom_col(alpha = al) + 
  scale_x_continuous(breaks=0:25, labels=0:25) + theme_minimal() + 
	theme(axis.text = element_text(size = 7)) + ylim(0,0.2) + labs(fill = "") + 
	scale_fill_manual(values = "#fb8072", labels = as.expression(bquote(p[z] == .(dat$p[1]))))
pl
```
::::
:::


## Sampling a population

::: {.columns}
:::: {.column}
* We sampled a large population to determine the rate of zombism.
* Assume samples were random, *iid* 
* Given a sample of $n=25$, we observed $k=7$ zombies.
* Estimate $p_z$, the proportion of the population that is a zombie.
* **Use intuition:** We know the best estimate is $p_z = 7/25 = 0.28$. Why?
* Many other values of $p_z$ are also possible.
::::
:::: {.column}
```{r echo = FALSE}
pl = ggplot(data = dat, aes(y = Probability, x = k, fill=col)) + geom_col(alpha = al, position = 'dodge') +
  scale_x_continuous(breaks=0:25, labels=0:25)+ theme_minimal() + ylim(0,0.2) + labs(fill = "") + 
	theme(axis.text = element_text(size = 7)) + 
	scale_fill_manual(values = c("#fb8072", "#8dd3c7"), 
		labels = c(as.expression(bquote(p[z] == .(dat$p[1]))), as.expression(bquote(p[z] == .(dat$p[30])))))

pl
```
::::
:::


## Parameter estimation

::: {.columns}
:::: {.column}
* We need a general method for **parameter estimation**
* In this case, want to estimate $p_z$
* Understanding the **uncertainty** in our estimate would also be nice

::::
:::: {.column}
```{r echo = FALSE}
pl 
```
::::
:::

## The likelihood model
> - If I *assume* a model $\theta$...
> - Can I compute the probability that the data came from this model?

## The likelihood model
* If I *assume* a model $\theta$...
* Can I compute the probability that the data came from this model?

$$
pr(\mathrm{Data} | \theta)
$$

## The likelihood model

::: {.columns}
:::: {.column}
> - Example: $\theta = p_z$
> - Assume $\hat{p}_z = 0.4$
> - What is the probability of observing the data ($k = 7, n = 25$) if this model is true?

::::
:::: {.column}
::::
:::

## The likelihood model

::: {.columns}
:::: {.column}
* Example: $\theta = p_z$
* Assume $\hat{p}_z = 0.4$
* What is the probability of observing the data ($k = 7, n = 25$) if this model is true?

$$
	pr(k=7,n=25 | p_z = 0.4) = ?
$$

Any guesses how?
::::
:::: {.column}
```{r echo = FALSE}
pl 
```
::::
:::


## The likelihood model

::: {.columns}
:::: {.column}
> - We already described this system with a **binomial process**
> - This is a **generative model**: we describe the statistical process (a binomial process with p = 0.4) that produces the observed data.
> - We can evaluate it with built-in functions in R
::::
:::: {.column}
```{r echo = FALSE}
pl 
```
::::
:::


## The likelihood model

::: {.columns}
:::: {.column}
* We already described this system with a **binomial process**
* This is a **generative model**: we describe the statistical process (a binomial process with p = 0.4) that produces the observed data.
* We can evaluate it with built-in functions in R

```{r}
# x is observation, in this case, k = 7
# size is n, the number of trials
# prob is the model parameter
dbinom(x = 7, size = 25, prob = 0.4)
```

::::
:::: {.column}
```{r echo = FALSE}
pl 
```
::::
:::




## The likelihood model

::: {.columns}
:::: {.column}
```{r}
# x is observation, in this case, k = 7
# size is n, the number of trials
# prob is the model parameter
dbinom(x = 7, size = 25, prob = 0.4)
```

We can try another model to see if it's better:

```{r}
dbinom(x = 7, size = 25, prob = 0.2)
```


::::
:::: {.column}
```{r echo = FALSE}
pl 
```
::::
:::


## The likelihood model

::: {.columns}
:::: {.column}
```{r}
# x is observation, in this case, k = 7
# size is n, the number of trials
# prob is the model parameter
dbinom(x = 7, size = 25, prob = 0.4)
```

We can try another model to see if it's better:

```{r}
dbinom(x = 7, size = 25, prob = 0.2)
```

And we can plot it against many different potential models

```{r plot = FALSE}
# different potential values for p, which must be between 0 and 1
models = seq(0, 1, 0.005)
probs = dbinom(x = 7, size = 25, prob = models)
binom_models = data.frame(models, probs)
binom_plot = ggplot(binom_models, aes(x = models, y = probs)) + geom_line()
```


::::
:::: {.column}
```{r echo = FALSE}
binom_plot = binom_plot + theme_minimal() + ylab(expression(pr(k,n~"|"~p[z]))) + xlab(expression(Models~(theta))) +
	annotate(geom = "text", x = 0.6, y = 0.15, label = "k = 7, n = 25")
binom_plot
```
::::
:::



## Likelihood functions

::: {.columns}
:::: {.column}
* The **likelihood function** is a function *f* of the data $X$ and model/parameters $\theta$
* Here $X = \{k,n\}$ and $\theta = \{p_z\}$
* $f(X,\theta)$ returns the *probability of observing* $X$, given a particular model $\theta$: $pr(X|\theta)$
* Here, the *binomial PMF* is a useful likelihood function
::::
:::: {.column}

```{r echo = FALSE}
binom_plot
```

::::
:::



## Likelihood functions

::: {.columns}
:::: {.column}
* Intuition is fine, but how do we estimate or (in some cases) solve for the maximum likelihood? Guessing might take a long time!
::::
:::: {.column}

```{r, echo=FALSE}
binom_plot = binom_plot + geom_point(aes(x = 7/25, y = dbinom(7, 25, 7/25)), size = 2, colour='blue')
binom_plot
```

::::
:::


## Solving for the MLE


$$
\begin{aligned}
\mathcal{L}(k,n|p) & = {n \choose k} p^k(1-p)^{(n-k)} \\
\frac{d \mathcal{L}(k,n|p)}{dp} & = {n \choose k}kp^{k-1}(1-p)^{(n-k)} - {n \choose k} p^k (n-k)(1-p)^{(n-k-1)} \\
& = 0
\end{aligned}
$$

## Solving for the MLE
$$
\begin{aligned}
{n \choose k} p^k (n-k)(1-p)^{(n-k-1)} & = {n \choose k}kp^{k-1}(1-p)^{(n-k)} \\
p(p^{k-1}) (n-k)(1-p)^{(n-k-1)} & = kp^{k-1}(1-p)(1-p)^{(n-k-1)} \\
p (n-k) & = k(1-p) \\
pn -pk & = k-pk \\
pn & = k \\
p & = \frac{k}{n} \\
\end{aligned}
$$

 
## Optimisation

> - In many (most) cases, analytical solutions are unavailable or impractical.
> - We turn to various algorithms for numerical optimisation


## Optimisation

* In many (most) cases, analytical solutions are unavailable or impractical.
* We turn to various algorithms for numerical optimisation

::: {.columns}
:::: {.column}
```{r}
# define our data set
n = 25
k = 7

# write a custom function that returns the log likelihood
llfun = function(p, n, k) {
  dbinom(k, n, p, log=TRUE) ## why log??
}

# we need an initial guess for the parameter we want to optimise
p_init = 0.5
```

::::
:::: {.column}
```{r}

# optim will start at p_init, evaluate llfun using the data we pass
# and return the optimum (if found)
# the terms n = ... and k = ... must be named the way they are in llfun
optim(p_init, llfun, method = "Brent", n = n, k = k, control = list(fnscale = -1), lower=0, upper=1)
```
::::
:::



## Parameter estimation in Stan

* Stan is a modelling language for scientific computing
* We use *probabilistic programming*
   - deterministic variables: `a = b * c`
   - Stochastic variables: `y ~ normal(mu, sigma)`

### Workflow

1. Write a Stan model in a `.stan` file
2. Prepare all data in R
3. Use the `rstan` package to invoke the Stan interpreter
   - Translates your model into a C++ program then compiles for your computer
4. Run the program from R using functions in the `rstan` package.


## Parameter estimation in Stan

::: {.columns}
:::: {.column}
* All Stan programs need a `data` block where you define any input data for your model
	- Variables must have a **type**
	- Here, `int` indicates a variable that is an integer
	- **Constraints** give rules about variables; here k must be positive, and n >= k
::::
:::: {.column}

File: `vu_advstats_students/stan/zomb_p.stan`

```{stan output.var = "zomb_p", eval = FALSE}
data {
	int <lower = 0> k;
	int <lower = k> n;
}
```
::::
:::


## Parameter estimation in Stan

::: {.columns}
:::: {.column}
* In the `parameters` block, we define **unknowns** that we want to estimate
	- `p` is a real number between 0 and 1
::::
:::: {.column}
File: `vu_advstats_students/stan/zomb_p.stan`

```{stan output.var = "zomb_p", eval = FALSE}
data {
	int <lower = 0> k;
	int <lower = k> n;
}
parameters {
	real <lower = 0, upper = 1> p;
}
```
::::
:::

## Parameter estimation in Stan

::: {.columns}
:::: {.column}
* In the `model` block we specify our likelihood function
	- `k` comes from a binomial distribution with probability `p` and `n` observations

::::
:::: {.column}
File: `vu_advstats_students/stan/zomb_p.stan`

```{stan output.var = "zomb_p", cache = TRUE}
data {
	int <lower = 0> k;
	int <lower = k> n;
}
parameters {
	real <lower = 0, upper = 1> p;
}
model {
	k ~ binomial(n, p);
}
```
::::
:::

## Parameter estimation in Stan

::: {.columns}
:::: {.column}
* In R, we load the `rstan` package
* Next we **compile** the model using `stan_model`
	- Compiled models don't need to be re-compiled unless the code in the .stan file changes!

::::
:::: {.column}

```{r eval = FALSE}
# assuming working directory is vu_advstats_students
library(rstan)
zomb_p = stan_model("stan/zomb_p.stan")
```
::::
:::


## Parameter estimation in Stan

::: {.columns}
:::: {.column}
* We next create a data list that we will pass on to Stan
* Finally, we can use `optimizing`, which works similar to `optim` but for Stan models

::::
:::: {.column}

```{r message = FALSE}
# assuming working directory is vu_advstats_students
library(rstan)
# already did this, don't do it twice
# zomb_p = stan_model("stan/zomb_p.stan") 

# prepare data
# names must match the data block in Stan
zomb_data = list(n = 25, k = 7)

# estimate the parameter
optimizing(zomb_p, data = zomb_data)

```
::::
:::




## Generalising to multiple observations
::: {.columns}
:::: {.column}

* Remember the product rule: for two independent events, 

$$pr(A,B) = pr(A)pr(B)$$
	
* Likelihoods are probabilities, and we like to assume each data point is independent. Thus:

$$
\begin{array}
\mathcal{L}(X_{1..n}|\theta) & = \prod_{i=1}^{n} \mathcal{L}(X_i|\theta) \\
 & \mathrm{or} \\
\log \mathcal{L}(X_{1..n}|\theta) &= \sum_{i=1}^{n} \log \mathcal{L}(X_i|\theta)
\end{array}
$$


::::
:::: {.column}


::::
:::

## Generalising to multiple observations

::: {.columns}
:::: {.column}

* Remember the product rule: for two independent events, 

$$pr(A,B) = pr(A)pr(B)$$
	
* Likelihoods are probabilities, and we like to assume each data point is independent. Thus:

$$
\begin{array}
\mathcal{L}(X_{1..n}|\theta) & = \prod_{i=1}^{n} \mathcal{L}(X_i|\theta) \\
 & \mathrm{or} \\
\log \mathcal{L}(X_{1..n}|\theta) &= \sum_{i=1}^{n} \log \mathcal{L}(X_i|\theta)
\end{array}
$$


::::
:::: {.column}

```{r}
# define our data set and initial guess
n_vec = c(25, 12, 134)
k_vec = c(7, 4, 27)
p_init = 0.5

# we take the sum of all the individual log liklihoods
llfun = function(p, n, k) {
  sum(dbinom(k, n, p, log=TRUE))
}

# take only the part we want out of this, $par, the parameter estimate
optim(p_init, llfun, method = "Brent", n = n_vec, k = k_vec, 
	  control = list(fnscale = -1), lower=0, upper=1)$par
```

::::
:::



## Multiple observations in Stan


::: {.columns}
:::: {.column}
* `k` and `n` are **arrays** of `int`s, each with a length of `n_obs`
* Comments indicated with `//`
* The `binomial` function is vectorised, can operate on multiple observations with no changes.

::::
:::: {.column}

```{stan output.var = "zomb_p2", cache = TRUE}
data {
	int <lower = 1> n_obs; // number of data points
	int <lower = 0> k [n_obs];
	int <lower = 0> n [n_obs]; // number of trials for each data point
}
parameters {
	real <lower = 0, upper = 1> p;
}
model {
	k ~ binomial(n, p);
}
```

```{r eval = FALSE}
zomb_p2 = stan_model("stan/zomb_p2.stan") 
```

::::
:::



## Multiple observations in Stan

```{r}
# prepare data
# names must match the data block in Stan
zomb_data = list(n = c(25, 12, 134), k = c(7, 4, 27), n_obs = 3)

# estimate the parameter
optimizing(zomb_p2, data = zomb_data)$par

```





## Why Bayes?
* I want to describe some phenomenon ("model"; $\theta$)
* I have some general ("prior") knowledge about the question: $pr(\theta)$
* I gather additional information ("data"; $X$)

What is the probability that my model is correct given what I already know about it and what Iâ€™ve learned?

$$ pr(\theta | X) $$

## Applying Bayes' Theorem

* We already know an expression for this:

$$
pr(\theta | X) = \frac{pr(X|\theta)pr(\theta)}{pr(X)}
$$

## Applying Bayes' Theorem

* We already know an expression for this:

$$
pr(\theta | X) = \frac{pr(X|\theta)pr(\theta)}{pr(X)}
$$

* The goal, $pr(\theta | X)$, is called the **posterior probability of $\theta$**
* We have already seen $pr(X|\theta)$; this is the **likelihood** of the data
* $pr(\theta)$ is often called the **prior probability of $\theta$**. Could also be called "other information about $\theta$"
* What about $pr(X)$? This is the **normalizing constant**

When we do Bayesian inference, each of these terms is a full **probability distribution**


## The normalizing constant

> - For the first zombie problem, we had a single observation ("I tested positive"), we were able to add up all of the ways one could test positive.
> - $pr(T) = pr(T,Z) + pr(T,Z') = pr(T|Z)pr(Z) + pr(T|Z')pr(Z')$
> - $pr(X) = \sum_i^n pr(X|\theta_i)pr(\theta_i)$ where all possible models are in the set $n$
> - What about for continuous problems?



## The normalizing constant

* For the first zombie problem, we had a single observation ("I tested positive"), we were able to add up all of the ways one could test positive.
* $pr(T) = pr(T,Z) + pr(T,Z') = pr(T|Z)pr(Z) + pr(T|Z')pr(Z')$
* $pr(X) = \sum_i^n pr(X|\theta_i)pr(\theta_i)$ where all possible models are in the set $n$
* What about for continuous problems? 
    - There are an infinite number of possible models if $pr(\theta)$ is a continuous PDF.
    - There are infinitely many possible datasets if X is real-valued



## The normalizing constant

* For the first zombie problem, we had a single observation ("I tested positive"), we were able to add up all of the ways one could test positive.
* $pr(T) = pr(T,Z) + pr(T,Z') = pr(T|Z)pr(Z) + pr(T|Z')pr(Z')$
* $pr(X) = \sum_i^n pr(X|\theta_i)pr(\theta_i)$ where all possible models are in the set $n$
* What about for continuous problems? 
    - There are an infinite number of possible models if $pr(\theta)$ is a continuous PDF.
    - There are infinitely many possible datasets if X is real-valued

$$
pr(X) = \int_a^b pr(X|\theta)pr(\theta)d \theta
$$

* This integral can be challenging to compute


## Proportional Bayes' Theorem

> - $pr(X)$ is a *constant*; it adjusts the height of the distribution so that the posterior integrates to 1
> - If all we want to do is estimate the maximum value, we can safely ignore it

## Proportional Bayes' Theorem

* $pr(X)$ is a *constant*; it adjusts the height of the distribution so that the posterior integrates to 1
* If all we want to do is estimate the maximum value, we can safely ignore it

$$
pr(\theta|X) \propto pr(X|\theta)pr(\theta)
$$

* For our example, we used the binomial PMF for the likelihood:

$$
pr(X|\theta) = pr(k,n|p) = {n \choose k} p^k(1-p)^{(n-k)}
$$

How to choose the prior, $pr(\theta)$?

## What do we know about p?

::: {.columns}
:::: {.column}
* Must be between 0 and 1. 
* We could assign equal probabilities using a uniform distribution:

```{r eval = FALSE}
theta = seq(0, 1, 0.01)
pr_theta = dunif(theta, min = 0, max = 1)
plot(theta, pr_theta, type = 'l')

```

::::
:::: {.column}

```{r echo = FALSE}
theta = seq(0, 1, 0.01)
pr_theta = dunif(theta, min = 0, max = 1)
plot(theta, pr_theta, type = 'l', ylim = c(0, 2), ylab = expression(pr(theta)), xlab = expression(theta))
```
::::
:::

## What do we know about p?

::: {.columns}
:::: {.column}
* Must be between 0 and 1. 
* We could assign equal probabilities using a uniform distribution.
* Not very flexible. Maybe we think central values are slightly more likely?
* A **beta** distribution makes many shapes possible

```{r eval = FALSE}
theta = seq(0, 1, 0.01)
pr_theta_1 = dbeta(theta, shape1 = 1, shape2 = 1)
```

::::
:::: {.column}

```{r echo = FALSE}
theta = seq(0, 1, 0.01)
pr_theta_1 = dbeta(theta, shape1 = 1, shape2 = 1)
pr_theta_2 = dbeta(theta, shape1 = 4, shape2 = 4)
pr_theta_3 = dbeta(theta, shape1 = 0.4, shape2 = 0.4)
pr_theta_4 = dbeta(theta, shape1 = 4, shape2 = 0.5)
pr_theta_5 = dbeta(theta, shape1 = 2, shape2 = 4)

cols = c("#000000", RColorBrewer::brewer.pal(4, 'Paired'))
plot(theta, pr_theta_1, type = 'l', col = cols[1], lwd=2, ylim = c(0,4), 
	 ylab = expression(pr(theta)), xlab = expression(theta))
text(0.3, 1.2, "a = 1, b = 1", col = cols[1])
lines(theta, pr_theta_2, type = 'l', col = cols[2], lwd=2)
text(0.5, 2.3, "a = 4, b = 4", col = cols[2])
lines(theta, pr_theta_3, type = 'l', col = cols[3], lwd=2)
text(0.2, 3.8, "a = 0.4, b = 0.4", col = cols[3])
lines(theta, pr_theta_4, type = 'l', col = cols[4], lwd=2)
text(0.75, 2.8, "a = 4, b = 0.5", col = cols[4])
lines(theta, pr_theta_5, type = 'l', col = cols[5], lwd=2)
text(0.15, 2.2, "a = 2, b = 4", col = cols[5])
```
::::
:::






## Maximum A Posteriori Estimation

* If we just want a Bayesian point estimate for $\theta$, we can use the same algorithms for MLE
* This is know as the **maximum a posteriori** (MAP) estimate, Bayesian equivalent to the MLE
* We ignore the normalising constant and incorporate a prior into the methods we used before



::: {.columns}
:::: {.column width=47%}

```{r}
log_liklihood = function(p, n, k)
  sum(dbinom(k, n, p, log=TRUE))

log_prior = function(p, a, b)
  dbeta(p, a, b, log = TRUE)

log_posterior = function(p, n, k, a, b)
  log_liklihood(p, n, k) + log_prior(p, a, b)
```
::::
:::: {.column width=6%}

::::
:::: {.column width=47%}

```{stan output.var = "zomb_p_bayes", cache = TRUE}
// Saved in vu_advstats_students/stan/zomb_p_bayes.stan
data {
	int <lower = 1> n_obs;
	int <lower = 0> k [n_obs];
	int <lower = 0> n [n_obs];
	real <lower = 0> a;
	real <lower = 0> b;
}
parameters {
	real <lower = 0, upper = 1> p;
}
model {
	k ~ binomial(n, p);
	p ~ beta(a, b);
}
```

::::
:::


## Maximum A Posteriori Estimation

Now we can fit the model, either in R or in Stan

::: {.columns}
:::: {.column}


```{r eval = FALSE}
zomb_p_bayes = stan_model("stan/zomb_p_bayes.stan")
```

```{r}
zomb_data = list(
	n_obs = 1,
	n = as.array(25), # force a single-value array, avoids an error
	k = as.array(7),
	a = 1, # totally flat prior to start
	b = 1
)

## starting value for the optimisation
p_init = 0.5

optim(p_init, log_posterior, method = "Brent", n = zomb_data$n, 
	k = zomb_data$k, a = zomb_data$a, b = zomb_data$b, 
	control = list(fnscale = -1), lower=0, upper=1)$par

optimizing(zomb_p_bayes, data = zomb_data)$par
```



::::
:::: {.column}

```{r echo=FALSE, figure.width = 3, figure.height = 3}
p = seq(0,1,length.out=100)
y_ll = sapply(p, \(x) exp(log_liklihood(x, zomb_data$n, zomb_data$k)))
y_lpr = sapply(p, \(x) exp(log_prior(x, zomb_data$a, zomb_data$b)))
y_lpo = sapply(p, \(x) exp(log_posterior(x, zomb_data$n, zomb_data$k, zomb_data$a, zomb_data$b)))
par(mar = c(5,5,0,1), bty='n', mfrow=c(2,2))
plot(p, y_ll, xlab = "", ylab = "Liklihood", col = 'red', type='l', lwd=3)
plot(p, y_lpr, xlab = "", ylab = "Prior", col = 'blue', type='l', lwd=3)
plot(p, y_lpo, xlab = "", ylab = "Unnormalized Posterior", col = 'purple', type='l', lwd=3)
mtext("p", side = 1, outer=TRUE, line=-2)
```

This prior has no influence on the posterior

::::
:::


## Changing the prior

::: {.columns}
:::: {.column}

```{r}
zomb_data$a = 2; zomb_data$b = 2
optimizing(zomb_p_bayes, data = zomb_data)$par
```

```{r}
zomb_data$a = 3; zomb_data$b = 1.5
optimizing(zomb_p_bayes, data = zomb_data)$par
```
These priors are informative, but relatively weak (our data has weight equivalent to `alpha=7, beta=18`)


::::
:::: {.column}


```{r echo=FALSE, figure.width = 3, figure.height = 3}
y_lpr1 = sapply(p, \(x) exp(log_prior(x, 2, 2)))
y_lpo1 = sapply(p, \(x) exp(log_posterior(x, zomb_data$n, zomb_data$k, 2, 2)))
y_lpr2 = sapply(p, \(x) exp(log_prior(x, 3, 1.5)))
y_lpo2 = sapply(p, \(x) exp(log_posterior(x, zomb_data$n, zomb_data$k, 3, 1.5)))


par(mar = c(5,5,0,1), bty='n', mfrow=c(2,2))
plot(p, y_ll, xlab = "", ylab = "Likelihood", col = 'red', type='l', lwd=3)
plot(p, y_lpr1, xlab = "", ylab = "Prior", col = 'blue', type='l', lwd=3, ylim=c(0, 1.8))
lines(p, y_lpr2, col = 'blue', lwd=2, lty=2)
plot(p, y_lpo1, xlab = "", ylab = "Unnormalized Posterior", col = 'purple', type='l', lwd=3)
lines(p, y_lpo2, col = 'purple', lwd=2, lty=2)
mtext("p", side = 1, outer=TRUE, line=-2)
```

::::
:::



## Changing the prior

::: {.columns}
:::: {.column}

What if we had already conducted an identical prior sample, with 20 zombies and 5 normals?

```{r}
zomb_data$a = 20; zomb_data$b = 5
optimizing(zomb_p_bayes, data = zomb_data)$par
```
::::
:::: {.column}

```{r echo=FALSE, figure.width = 3, figure.height = 3}
y_lpr3 = sapply(p, \(x) exp(log_prior(x, 20, 5)))
y_lpo3 = sapply(p, \(x) exp(log_posterior(x, zomb_data$n, zomb_data$k, 20, 5)))

par(mar = c(5,5,0,1), bty='n', mfrow=c(2,2))
plot(p, y_ll, xlab = "", ylab = "Likelihood", col = 'red', type='l', lwd=3)
plot(p, y_lpr3, xlab = "", ylab = "Prior", col = 'blue', type='l', lwd=3)
plot(p, y_lpo3, xlab = "", ylab = "Unnormalized Posterior", col = 'purple', type='l', lwd=3)
mtext("p", side = 1, outer=TRUE, line=-2)
```

::::
:::


## Geting a normalized posterior

> - We often want to know the full posterior distribution
> - For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$

## Geting a normalized posterior

* We often want to know the full posterior distribution
* For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$
* For the beta-binomial, if:

$$
\begin{aligned}
\overset{\small \color{blue}{prior}}{pr(p)} & = \mathrm{Beta}(a, b) \\ \\
\overset{\small \color{blue}{likelihood}}{pr(k,n | p)} & = \mathrm{Binomial}(k, n, p)
\end{aligned}
$$

## Geting a normalized posterior

* We often want to know the full posterior distribution
* For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$
* For the beta-binomial, if:

$$
\begin{aligned}
\overset{\small \color{blue}{prior}}{pr(p)} & = \mathrm{Beta}(a, b) \\ \\
\overset{\small \color{blue}{likelihood}}{pr(k,n | p)} & = \mathrm{Binomial}(k, n, p) \\ \\
\overset{\small \color{blue}{posterior}}{pr(p | k,n,a,b)} &= \mathrm{Beta}(a + k, b + n - k)
\end{aligned}
$$

## Geting a normalized posterior

* We often want to know the full posterior distribution
* For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$
* In many cases, the shape of the log posterior is appoximately quadratic (the posterior is approximately normal)
* **Laplace approximation** (or Quadratic approximation) is a method for approximating the shape of this curve


## Geting a normalized posterior

* We often want to know the full posterior distribution
* For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$
* In many cases, the shape of the log posterior is appoximately quadratic (the posterior is approximately normal)
* **Laplace approximation** (or Quadratic approximation) is a method for approximating the shape of this curve

```{r, echo = FALSE}
apost = zomb_data$a + zomb_data$k
bpost = zomb_data$b + zomb_data$n - zomb_data$k
plot(p, dbeta(p, apost, bpost, log=TRUE), type='l', xlab = "p", ylab = "Log Posterior", 
     col = 'purple', lwd=2, xlim=c(0.4, 0.7), ylim=c(-1.5, 2), bty='n')

## quick estimate of the mean/variance of the posterior
stdev = sqrt((apost*bpost)/((apost+bpost)^2 * (apost+bpost+1)))
mu = apost / (apost + bpost)
## quadratic estimation
lines(p, dnorm(p, mu, stdev, log=TRUE), col='red', lwd=2, lty=2)
legend("bottomleft", legend = c("Normalized posterior", "Quadratic Estimate"), col=c("purple", "red"), lty=c(1,2), lwd=2, bty='n')
```



## Geting a normalized posterior

* We often want to know the full posterior distribution
* For some problems, we have analytical solutions to $\int pr(X|\theta)pr(\theta)d \theta$
* In many cases, the shape of the log posterior is appoximately quadratic (the posterior is approximately normal)
* **Laplace approximation** (or Quadratic approximation) is a method for approximating the shape of this curve
* When the above are unavailable, we can use simulations (e.g., MCMC)
