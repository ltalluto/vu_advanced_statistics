---
title: "Posterior Inference I"
author: "Lauren. Talluto"
date: "28.11.2024"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
    self_contained: false
    lib_dir: lib
    mathjax: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
  beamer_presentation: default
---


```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png", error = TRUE)
library(ggplot2)
# library(data.table)
# library(cowplot)
library(rstan)
library(gridExtra)
library(bayesplot)
library(kableExtra)
```



## Posterior inference I: Sampling

::: {.columns}
:::: {.column}

::::
:::: {.column}
::::
:::

> - Taking samples turns out to be a very useful way to learn about a distribution!
> - For example: Bayesian linear regression in Stan

## Posterior inference I: Sampling


::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan

Dataset: Palmer penguins

![](../assets/img/palmerpenguins.png){width=150px}
![](../assets/img/culmen_depth.png){width=250px}

::::: {.small}
> Dataset: Dr. Kristen Gorman, University of Alaska ([Gorman et al 2014)](https://doi.org/10.1371/journal.pone.0090081
)
>
> R package `palmerpenguins`
>
> Artwork by @allison_horst
:::::

::::
:::: {.column}

```{r echo = FALSE, warning = FALSE, fig.width = 6.5}
ggplot(palmerpenguins::penguins) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, colour = species)) + 
	theme_minimal() + xlab("Bill Length (mm)") + ylab("Bill Depth (mm)")
```

::::
:::






## Posterior inference I: Sampling


::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan

Dataset: Palmer penguins

![](../assets/img/palmerpenguins.png){width=150px}
![](../assets/img/culmen_depth.png){width=250px}

::::: {.small}
> Dataset: Dr. Kristen Gorman, University of Alaska ([Gorman et al 2014)](https://doi.org/10.1371/journal.pone.0090081
)
>
> R package `palmerpenguins`
>
> Artwork by @allison_horst
:::::

::::
:::: {.column}


```{stan output.var = "penguin_lm", cache = TRUE}
data {
	int <lower=0> n;
	vector [n] x;
	vector [n] y;
}
parameters {
	real intercept;
	real slope;
	real <lower = 0> s;
}
model {
	y ~ normal(intercept + slope * x, s);
}
```

```{r eval = FALSE}
penguin_lm = stan_model("stan/basic_lm.stan") 

```

::::
:::





## Posterior inference I: Sampling


::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan

Dataset: Palmer penguins

![](../assets/img/palmerpenguins.png){width=150px}
![](../assets/img/culmen_depth.png){width=250px}

::::: {.small}
> Dataset: Dr. Kristen Gorman, University of Alaska ([Gorman et al 2014)](https://doi.org/10.1371/journal.pone.0090081
)
>
> R package `palmerpenguins`
>
> Artwork by @allison_horst
:::::

::::
:::: {.column}

```{r, cache = TRUE}
data(penguins, package = "palmerpenguins")
penguins = as.data.frame(penguins[complete.cases(penguins),])
penguins = subset(penguins, species == "Gentoo")
peng_dat = with(penguins, list(
	x = bill_length_mm,
	y = bill_depth_mm,
	n = length(bill_length_mm)
))

# refresh = 0 just turns off the status display to keep the slides neat
# you can omit it in your own code!
peng_fit = sampling(penguin_lm, data = peng_dat, refresh = 0)
```


::::
:::








## Posterior inference I: Sampling


::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan
* What can we learn from our samples?


::::
:::: {.column}


``` {r echo = FALSE, fig.height = 4, fig.width = 4, message = FALSE}
samps = as.matrix(peng_fit)
xx = data.frame(slope = as.matrix(peng_fit, par = "slope"))
bp = bayesplot::mcmc_hist(as.matrix(peng_fit, par = "slope"))
```


```{r, cache = TRUE}
# samples are a bit easier to deal with in a matrix
peng_samps = as.matrix(peng_fit)
head(peng_samps)


```


::::
:::





## Posterior inference I: Sampling


::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan
* What can we learn from our samples?
	- What is the probability that the slope > 0.15?

::::
:::: {.column}

``` {r echo = FALSE, fig.height = 4, fig.width = 4, message = FALSE}
bp + geom_vline(xintercept = 0.15, col = 'red')
```

```{r, cache = TRUE}
slope_015 = peng_samps[, 'slope'] > 0.15
table(slope_015)
sum(slope_015 / nrow(peng_samps))


```


::::
:::





## Posterior inference I: Sampling
::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan
* What can we learn from our samples?
	- What is the probability that the slope > 0.15?
	- What is the probability of a range of values, e.g., $0.15 < slope < 0.2$

::::
:::: {.column}



``` {r echo = FALSE, fig.height = 4, fig.width = 4, message = FALSE}
bp + geom_vline(xintercept = c(0.15, 0.25), col = 'red')
```



```{r, cache = TRUE}
sum(peng_samps[, 'slope'] > 0.15 & peng_samps[, 'slope'] < 0.25) / 
	nrow(peng_samps)


```


::::
:::






## Posterior inference I: Sampling
::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan
* What can we learn from our samples?
	- What is the probability that the slope > 0.15?
	- What is the probability of a range of values, say $ 0.15 < slope < 0.2$
	- What interval encompasses 90% of the probability mass (**90% Credible Interval**)?

::::
:::: {.column}

``` {r echo = FALSE, fig.height = 4, fig.width = 8, message = FALSE}
qnt = t(apply(samps, 2, quantile, c(0.05, 0.95)))
h1 = mcmc_hist(peng_samps, pars = "intercept", bins = 30) + 
	geom_vline(xintercept = qnt[1,], col = 'red')
h2 = mcmc_hist(peng_samps, pars = "slope", bins = 30) +
	geom_vline(xintercept = qnt[2,], col = 'red')
h3 = mcmc_hist(peng_samps, pars = "s", bins = 30) +
	geom_vline(xintercept = qnt[3,], col = 'red')
grid.arrange(h1, h2, h3, nrow = 1)
```

```{r, cache = TRUE}
t(apply(samps, 2, quantile, c(0.05, 0.95)))
```


::::
:::




## Posterior inference I: Sampling
::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan
* What can we learn from our samples?
	- What is the probability that the slope > 0.15?
	- What is the probability of a range of values, say $0.15 < slope < 0.2$
	- What interval encompasses 90% of the probability mass (**90% Credible Interval**)?
* We could emulate the output of `summary(lm(...))`

::::
:::: {.column}



```{r, cache = TRUE}
tab = data.frame(
	estimate = apply(peng_samps[, 1:2], 2, median),
	ste = apply(peng_samps[, 1:2], 2, sd)
)

```

::: {style="width:50%;"}
```{r echo = FALSE}
kable_styling(knitr::kable(tab, digits = 2,
	col.names = c("Estimate", "Std. Error")))

```
:::

::::
:::







## Posterior inference I: Sampling
::: {.columns}
:::: {.column}

* What about central tendency? What's the most likely or "best guess" for each parameter?
* For normally distributed posterior, all three are equal
* Otherwise, median performs best, but **always** prefer an interval to a point estimate

::::
:::: {.column}



```{r, cache = TRUE}
tab = data.frame(
	mean = colMeans(peng_samps[, 1:3]),
	median = apply(peng_samps[, 1:3], 2, median),
	## need optimizing to get the posterior mode
	mode = optimizing(penguin_lm, data = peng_dat)$par)

```

::: {style="width:75%;"}
```{r echo = FALSE}
kable_styling(knitr::kable(tab, digits = 5))
```
:::

::::
:::







## Posterior prediction

::: {.columns}
:::: {.column}

* We can use the medians to easily predict the regression line.

::::
:::: {.column}

::: {style="width:50%;"}
```{r echo = FALSE}
tab = matrix(tab[, 2], nrow = 1, dimnames = list("", c("intercept", "slope", "s")))
kable_styling(knitr::kable(tab, digits = 2))
```
:::

```{r echo = FALSE}

pbase = ggplot(data = penguins) + theme_minimal() + xlab("Bill Length (mm)") + ylab("Bill Depth(mm)")
pl = pbase + 
	geom_abline(intercept = median(peng_samps[, 'intercept']), 
				slope = median(peng_samps[,'slope']), col = "#841D15", linewidth = 1.5) + 
	geom_point(aes(x = bill_length_mm, y = bill_depth_mm), col = scales::hue_pal()(2)[1])
	
pl
```

::::
:::






## Posterior prediction

::: {.columns}
:::: {.column}

* We can use the medians to easily predict the regression line.
* Posterior distributions are *transitive*

> If $\hat{\theta}$ is a set of samples approximating the posterior distribution of $\theta$, and if some desired variable $Z = f(\theta)$, then $f(\hat{\theta})$ approximates the posterior distribution of $Z$

::::
:::: {.column}
::: {style="width:50%;"}
```{r echo = FALSE}
kable_styling(knitr::kable(tab, digits = 2))
```
:::

```{r echo = FALSE}
pl
```

::::
:::





## Posterior prediction

::: {.columns}
:::: {.column}

* We can use the medians to easily predict the regression line.
* Posterior distributions are *transitive*

> If $\hat{\theta}$ is a set of samples approximating the posterior distribution of $\theta$, and if some desired variable $Z = f(\theta)$, then $f(\hat{\theta})$ approximates the posterior distribution of $Z$

* We can use this to get a posterior distribution of **regression lines**
	- Each posterior sample is one potential regression line

::::
:::: {.column}

::: {style="width:50%;"}
```{r echo = FALSE}
kable_styling(knitr::kable(tab, digits = 2))
```
:::

```{r echo = FALSE}
i = sample(nrow(peng_samps), 1)

pl = pbase + 
	geom_abline(intercept = peng_samps[i, 'intercept'], slope = peng_samps[i, 'slope'], 
					  col = "#777777", linewidth = 0.2) + 
	geom_abline(intercept = median(peng_samps[, 'intercept']), 
				slope = median(peng_samps[,'slope']), col = "#841D15", linewidth = 1.5) + 
	geom_point(aes(x = bill_length_mm, y = bill_depth_mm), col = scales::hue_pal()(2)[1])

pl
```

::::
:::







## Posterior prediction

::: {.columns}
:::: {.column}

* We can use the medians to easily predict the regression line.
* Posterior distributions are *transitive*

> If $\hat{\theta}$ is a set of samples approximating the posterior distribution of $\theta$, and if some desired variable $Z = f(\theta)$, then $f(\hat{\theta})$ approximates the posterior distribution of $Z$

* We can use this to get a posterior distribution of **regression lines**
	- Each posterior sample is one potential regression line

::::
:::: {.column}

::: {style="width:50%;"}
```{r echo = FALSE}
kable_styling(knitr::kable(tab, digits = 2))
```
:::

```{r echo = FALSE}
pl = pbase

for(i in sample(nrow(peng_samps), 40)) {
	pl = pl + 
		geom_abline(intercept = peng_samps[i, 'intercept'], slope = peng_samps[i, 'slope'], 
					  col = "#777777", linewidth = 0.2)
}
pl = pl + 
	geom_abline(intercept = median(peng_samps[, 'intercept']), 
				slope = median(peng_samps[,'slope']), col = "#841D15", linewidth = 1.5) + 
	geom_point(aes(x = bill_length_mm, y = bill_depth_mm), col = scales::hue_pal()(2)[1])

pl
```

::::
:::



## Posterior prediction

::: {.columns}
:::: {.column}

* This means we can easily use the samples to predict a credible interval for $\mathbb{E}(y)$ for any arbitrary value of $x$
* What information is this telling us?
	- There is a 90% chance the conditional expectation is in the range(?!)


```{r}
# predict from one x, one sample
pry = function(samp, x) samp[1] + samp[2] * x

test_x = 55.6
# just apply prediction to every row of samples
test_y = apply(peng_samps, 1, pry, x = test_x)
# then get the quantiles
quantile(test_y, c(0.05, 0.95))
```

::::
:::: {.column}


```{r echo = FALSE}

yy = data.frame(x = test_x, y = quantile(test_y, 0.5), low = quantile(test_y, 0.05), hi = quantile(test_y, 0.95))


pl + 
	geom_errorbar(data = yy, aes(x = test_x, ymin = low, ymax = hi), width = 0, col = "#005659", linewidth = 1.5) + 
	geom_point(data = yy, aes(x = test_x, y = y), size = 2.5, pch=21, bg = scales::hue_pal()(2)[2], col = "#005659") 

```

::::
:::


## Posterior prediction

::: {.columns}
:::: {.column}

* This means we can easily use the samples to predict a credible interval for $\mathbb{E}(y)$ for any arbitrary value of $x$
* What information is this telling us?
* We can do the same across many x-values to produce a **confidence ribbon**.
	- This is very similar to regression confidence intervals produced by `lm`


::::
:::: {.column}

```{r}
# predict from many x, one sample
pry = function(samp, x) samp[1] + samp[2] * x

test_x = seq(40, 60, length.out = 200)

# test_x is a vector, so result is a matrix
# each row in this matrix is the prediction for a single x
# each column a single sample
test_y = apply(peng_samps, 1, pry, x = test_x)
colnames(test_y) = paste0("samp", 1:nrow(peng_samps))
rownames(test_y) = paste0("bill_len=", round(test_x, 2))
test_y[1:3, 1:6]
```

::::
:::

## Posterior prediction

::: {.columns}
:::: {.column}

* This means we can easily use the samples to predict a credible interval for $\mathbb{E}(y)$ for any arbitrary value of $x$
* What information is this telling us?
* We can do the same across many x-values to produce a **confidence ribbon**.
	- This is very similar to regression confidence intervals produced by `lm`


::::
:::: {.column}

```{r}
# then we get the quantiles by row
interval_y = apply(test_y, 1, quantile, c(0.05, 0.95))
interval_y[, 1:5]
```

::::
:::

## Posterior prediction

::: {.columns}
:::: {.column}

* This means we can easily use the samples to predict a credible interval for $\mathbb{E}(y)$ for any arbitrary value of $x$
* What information is this telling us?
* We can do the same across many x-values to produce a **confidence ribbon**.
	- This is very similar to regression confidence intervals produced by `lm`


::::
:::: {.column}

```{r echo = FALSE}

yy = data.frame(x = test_x, y = apply(test_y, 1, median), low = interval_y[1,], hi = interval_y[2,])

pbase + 
	geom_ribbon(data = yy, aes(x = x, ymin = low, ymax = hi), 
				fill = "#F8766D55", col = "#7f3c38") +  
	geom_line(data = yy, aes(x = x, y = y), col = "#841D15", linewidth = 1.4) + 
	geom_point(aes(x = bill_length_mm, y = bill_depth_mm), 
			   pch = 21, col = "#666666", bg = "#F8766D", size = 1)

```

::::
:::



## Posterior prediction

::: {.columns}
:::: {.column}

* Our model is **generative**
* It postulates a *statistical* process (not mechanistic) by which the outcomes $y$ are created
* We can use posterior predictive simulations to learn the distribution of **outcomes**
* For a given value of $x$, the interval tells you where 90% of the values of $y$ will fall (not $\mathbb{E}[y]$)
* To do this:
   - for each sample of $a$, $b$, and $s$
   - for each value of a **prediction dataset** $\hat{x}$
   - compute $\eta = \mathbb{E}(y)$ using the regression equation
   - simulate a new dataset $\hat{y}$ from $\eta$ and $s$
   - compute quantiles for $\hat{y} | \hat{x}$
* Similar to typical regression **prediction intervals**



::::
:::: {.column}


```{r}
# from a single sample, generate a new prediction dataset from xhat
sim = function(samp, xhat) {
	eta = samp[1] + samp[2] * xhat
	rnorm(length(xhat), eta, samp[3])
}

test_x = seq(40, 60, length.out = 200)
pr_test_y = matrix(0, ncol = nrow(peng_samps), nrow = length(test_x))

# for clarity, using a for loop. could (should) do this instead with mapply
for(i in 1:nrow(peng_samps))
	pr_test_y[,i] = sim(peng_samps[i,], test_x)

# now get quantiles for each value in x
pr_int_y = apply(pr_test_y, 1, quantile, c(0.05, 0.95))
```

::::
:::


## Posterior prediction

::: {.columns}
:::: {.column}

* Our model is **generative**
* It postulates a *statistical* process (not mechanistic) by which the outcomes $y$ are created
* We can use posterior predictive simulations to learn the distribution of **outcomes**
* For a given value of $x$, the interval tells you where 90% of the values of $y$ will fall (not $\mathbb{E}[y]$)
* To do this:
   - for each sample of $a$, $b$, and $s$
   - for each value of a **prediction dataset** $\hat{x}$
   - compute $\eta = \mathbb{E}(y)$ using the regression equation
   - simulate a new dataset $\hat{y}$ from $\eta$ and $s$
   - compute quantiles for $\hat{y} | \hat{x}$
* Similar to typical regression **prediction intervals**



::::
:::: {.column}

```{r echo = FALSE}

yy = data.frame(x = test_x, y = apply(test_y, 1, median), low = interval_y[1,], 
				hi = interval_y[2,],
				pr_low = pr_int_y[1,], pr_hi = pr_int_y[2,])

pbase + geom_ribbon(data = yy, aes(x = x, ymin = pr_low, ymax = pr_hi), 
					fill = "#8bc4c644", col = "#52c0c4") +  
	geom_ribbon(data = yy, aes(x = x, ymin = low, ymax = hi), 
				fill = "#F8766D77", col = "#7f3c38") +  
	geom_line(data = yy, aes(x = x, y = y), col = "#841D15", linewidth = 1.4) + 
	geom_point(aes(x = bill_length_mm, y = bill_depth_mm), 
			   pch = 21, col = "#666666", bg = "#F8766D", size = 1)

```



::::
:::

