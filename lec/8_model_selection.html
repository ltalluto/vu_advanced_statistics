<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Lauren Talluto" />
  <title>Model Selection &amp; Comparison</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
            pre > code.sourceCode { white-space: pre; position: relative; }
            pre > code.sourceCode > span { line-height: 1.25; }
            pre > code.sourceCode > span:empty { height: 1.2em; }
            .sourceCode { overflow: visible; }
            code.sourceCode > span { color: inherit; text-decoration: inherit; }
            div.sourceCode { margin: 1em 0; }
            pre.sourceCode { margin: 0; }
            @media screen {
            div.sourceCode { overflow: auto; }
            }
            @media print {
            pre > code.sourceCode { white-space: pre-wrap; }
            pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
            }
            pre.numberSource code
              { counter-reset: source-line 0; }
            pre.numberSource code > span
              { position: relative; left: -4em; counter-increment: source-line; }
            pre.numberSource code > span > a:first-child::before
              { content: counter(source-line);
                position: relative; left: -1em; text-align: right; vertical-align: baseline;
                border: none; display: inline-block;
                -webkit-touch-callout: none; -webkit-user-select: none;
                -khtml-user-select: none; -moz-user-select: none;
                -ms-user-select: none; user-select: none;
                padding: 0 4px; width: 4em;
                color: #aaaaaa;
              }
            pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
            div.sourceCode
              {   }
            @media screen {
            pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
            }
            code span.al { color: #ff0000; font-weight: bold; } /* Alert */
            code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
            code span.at { color: #7d9029; } /* Attribute */
            code span.bn { color: #40a070; } /* BaseN */
            code span.bu { color: #008000; } /* BuiltIn */
            code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
            code span.ch { color: #4070a0; } /* Char */
            code span.cn { color: #880000; } /* Constant */
            code span.co { color: #60a0b0; font-style: italic; } /* Comment */
            code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
            code span.do { color: #ba2121; font-style: italic; } /* Documentation */
            code span.dt { color: #902000; } /* DataType */
            code span.dv { color: #40a070; } /* DecVal */
            code span.er { color: #ff0000; font-weight: bold; } /* Error */
            code span.ex { } /* Extension */
            code span.fl { color: #40a070; } /* Float */
            code span.fu { color: #06287e; } /* Function */
            code span.im { color: #008000; font-weight: bold; } /* Import */
            code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
            code span.kw { color: #007020; font-weight: bold; } /* Keyword */
            code span.op { color: #666666; } /* Operator */
            code span.ot { color: #007020; } /* Other */
            code span.pp { color: #bc7a00; } /* Preprocessor */
            code span.sc { color: #4070a0; } /* SpecialChar */
            code span.ss { color: #bb6688; } /* SpecialString */
            code span.st { color: #4070a0; } /* String */
            code span.va { color: #19177c; } /* Variable */
            code span.vs { color: #4070a0; } /* VerbatimString */
            code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
          </style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="lib/header-attrs-2.27/header-attrs.js"></script>
  <script src="lib/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link href="lib/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
  <script src="lib/bootstrap-3.3.5/js/bootstrap.min.js"></script>
  <script src="lib/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
  <script src="lib/bootstrap-3.3.5/shim/respond.min.js"></script>
  <style>h1 {font-size: 34px;}
         h1.title {font-size: 38px;}
         h2 {font-size: 30px;}
         h3 {font-size: 24px;}
         h4 {font-size: 18px;}
         h5 {font-size: 16px;}
         h6 {font-size: 12px;}
         code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
         pre:not([class]) { background-color: white }</style>
  <link href="lib/slidy-2/styles/slidy.css" rel="stylesheet" />
  <script src="lib/slidy-2/scripts/slidy.js"></script>
  <script src="lib/slidy_shiny-1/slidy_shiny.js"></script>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
   href="rmd_style.css" />
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Model Selection &amp; Comparison</h1>
  <p class="author">
Lauren Talluto
  </p>
  <p class="date">05.12.2024</p>
</div>
<div id="there-is-no-right-solution-to-evaluation"
class="slide section level2">
<h1>There is no right solution to evaluation</h1>
<p>Important to assess modelling goals <em>before</em> comparing
models.</p>
<p>So why do we fit models?</p>
<ul class="incremental">
<li>Fit to calibration data</li>
<li>Predictive performance (new data)</li>
<li>Hypothesis testing</li>
<li>Avoiding overfitting</li>
</ul>
</div>
<div id="there-is-no-right-solution-to-evaluation-1"
class="slide section level2">
<h1>There is no right solution to evaluation</h1>
<p>Important to assess modelling goals <em>before</em> comparing
models.</p>
<p>So why do we fit models?</p>
<ul>
<li>Fit to calibration data</li>
<li>Predictive performance (new data)</li>
<li>Hypothesis testing</li>
<li>Avoiding overfitting</li>
</ul>
<p>
<strong>Rule of thumb</strong>: out-of-sample performance is the gold
standard!
</p>
<ul>
<li>We can always improve fit to training/in-sample data by adding
variables to the model</li>
</ul>
</div>
<div id="information-content-deviance" class="slide section level2">
<h1>Information content &amp; deviance</h1>
<ul>
<li>Entropy: how “predictable” is a distribution</li>
<li>High entropy means knowing the distribution tells us little about
future observations</li>
<li>A useful metric is the average log probability</li>
<li>For <span class="math inline">\(n\)</span> possible events, each
with probability <span class="math inline">\(p_i\)</span>:</li>
</ul>
<p><span class="math display">\[
H = -\mathbb{E}\left [\log \left (p \right) \right ] = -\sum_{i=1}^n
p_i\log(p_i)
\]</span></p>
</div>
<div id="entropy" class="slide section level2">
<h1>Entropy</h1>
<p>Imagine a series of flips of a weighted coin:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>pr_unfair <span class="ot">=</span> <span class="fu">c</span>(<span class="fl">0.8</span>, <span class="fl">0.2</span>)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>flips <span class="ot">=</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="st">&quot;Heads&quot;</span>, <span class="st">&quot;Tails&quot;</span>), <span class="at">size =</span> <span class="dv">20</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>, <span class="at">prob =</span> pr_unfair)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">table</span>(flips)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="do">## flips</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="do">## Heads Tails </span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="do">##    17     3</span></span></code></pre></div>
<p>A fair coin has high entropy; each face has a probability of <span
class="math inline">\(\frac{1}{2}\)</span>, this (or any) sequence
cannot help us predict the next roll</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>pr_fair <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="sc">-</span><span class="fu">sum</span>(pr_fair <span class="sc">*</span> <span class="fu">log</span>(pr_fair))</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="do">## [1] 0.6931472</span></span></code></pre></div>
<p>An unfair coin has lower entropy; if we know which face is weighted,
we can predict easily what the next outcome will be.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="sc">-</span><span class="fu">sum</span>(pr_unfair <span class="sc">*</span> <span class="fu">log</span>(pr_unfair))</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="do">## [1] 0.5004024</span></span></code></pre></div>
</div>
<div id="principle-of-maximum-entropy" class="slide section level2">
<h1>Principle of maximum entropy</h1>
<p>The best model in a set is the one that maximizes the entropy of the
residuals</p>
<p>After applying the model, the value of the residuals should tell us
as little as possible about the next residual</p>
</div>
<div id="divergence" class="slide section level2">
<h1>Divergence</h1>
<div class="columns">
<div class="column">
<ul>
<li>What’s the cost (how wrong will we be) of using the wrong model
<span class="math inline">\(q\)</span>, when the true model is <span
class="math inline">\(p\)</span></li>
<li>Or:
<ul>
<li>How much extra entropy do we introduce by using model <span
class="math inline">\(q\)</span>?</li>
</ul></li>
<li>This is known as the <em>Kullback-Leibler</em> (or K-L)
divergence.</li>
</ul>
<p><span class="math display">\[
D_{KL} = \sum_{i=1}^n p_i\log \left (\frac{p_i}{q_i} \right)
\]</span></p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="fu">sum</span>(pr_unfair <span class="sc">*</span> <span class="fu">log</span>(pr_unfair<span class="sc">/</span>pr_fair))</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="do">## [1] 0.1927448</span></span></code></pre></div>
<p>This tells us how much entropy we get from assuming our unfair coin
is fair. This is a bad model: the prob of heads is really 0.8, but we
assumed it was <span class="math inline">\(\frac{1}{2}\)</span>.</p>
</div><div class="column">
<p>If the model is even worse, divergence increases. What if we assume
<span class="math inline">\(pr(Heads) = \frac{1}{10}\)</span> instead of
<span class="math inline">\(\frac{1}{2}\)</span>?</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>pr_10 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">/</span><span class="dv">10</span>, <span class="dv">1</span> <span class="sc">-</span> <span class="dv">1</span><span class="sc">/</span><span class="dv">10</span>)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="fu">sum</span>(pr_unfair <span class="sc">*</span> <span class="fu">log</span>(pr_unfair<span class="sc">/</span>pr_10))</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="do">## [1] 1.362738</span></span></code></pre></div>
<p><img src="8_model_selection_files/figure-slidy/divergence_plot-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="the-true-model-is-unknown" class="slide section level2">
<h1>The “true” model is unknown</h1>
<div class="columns">
<div class="column">
<ul>
<li>We never have access to the true model <span
class="math inline">\(p\)</span> from this equation</li>
</ul>
<p><span class="math display">\[
D_{KL} = \sum_{i=1}^n p_i\log \left (\frac{p_i}{q_i} \right)
\]</span></p>
<ul>
<li><p>We suspect our coin is weighted from the flips, but we don’t know
the probs</p></li>
<li><p>Instead we estimate them by performing an experiment (flipping
the coin) and building a model</p></li>
<li><p>We have two competing hypotheses: call them <span
class="math inline">\(\theta_{map}\)</span> and <span
class="math inline">\(\theta_{fair}\)</span></p></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
D_{KL, \theta_{map}} &amp; = \sum_{i=1}^n \theta_{true}\log
\frac{\theta_{true}}{\theta_{map}} \\
D_{KL, \theta_{fair}} &amp; = \sum_{i=1}^n
\theta_{true}\log  \frac{\theta_{true}}{\theta_{fair}} \\
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(\theta_{true}\)</span> cancels!</li>
<li>We can compare the models’ relative performance without knowing
anything about <span class="math inline">\(\theta_{true}\)</span></li>
</ul>
</div><div class="column">
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>log_likelihood <span class="ot">=</span> <span class="cf">function</span>(probH, flips)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>    <span class="fu">sum</span>(<span class="fu">dbinom</span>(<span class="fu">sum</span>(flips <span class="sc">==</span> <span class="st">&quot;Heads&quot;</span>), <span class="fu">length</span>(flips), <span class="at">prob =</span> probH, <span class="at">log=</span><span class="cn">TRUE</span>))</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>log_posterior <span class="ot">=</span> <span class="cf">function</span>(probH, flips) {</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    <span class="co"># a priori we expect 50/50 heads and tails, and I give the prior a bit of weight because most coins are fair</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    <span class="fu">log_likelihood</span>(probH, flips) <span class="sc">+</span> <span class="fu">dbeta</span>(probH, <span class="dv">15</span>, <span class="dv">15</span>, <span class="at">log=</span><span class="cn">TRUE</span>) </span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>}</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>map <span class="ot">=</span> <span class="fu">optim</span>(pr_fair[<span class="dv">1</span>], <span class="at">method=</span><span class="st">&quot;Brent&quot;</span>, log_posterior, <span class="at">flips=</span>flips, </span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a>            <span class="at">control=</span><span class="fu">list</span>(<span class="at">fnscale=</span><span class="sc">-</span><span class="dv">1</span>), <span class="at">lower=</span><span class="dv">0</span>, <span class="at">upper=</span><span class="dv">1</span>)</span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a><span class="co"># mle with no prior, for comparison</span></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a>mle <span class="ot">=</span> <span class="fu">optim</span>(pr_fair[<span class="dv">1</span>], <span class="at">method=</span><span class="st">&quot;Brent&quot;</span>, log_likelihood, <span class="at">flips=</span>flips, </span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a>            <span class="at">control=</span><span class="fu">list</span>(<span class="at">fnscale=</span><span class="sc">-</span><span class="dv">1</span>), <span class="at">lower=</span><span class="dv">0</span>, <span class="at">upper=</span><span class="dv">1</span>)</span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a>prs_map <span class="ot">=</span> <span class="fu">c</span>(map<span class="sc">$</span>par, <span class="dv">1</span><span class="sc">-</span>map<span class="sc">$</span>par)</span>
<span id="cb6-17"><a href="#cb6-17" tabindex="-1"></a>prs_mle <span class="ot">=</span> <span class="fu">c</span>(mle<span class="sc">$</span>par, <span class="dv">1</span><span class="sc">-</span>mle<span class="sc">$</span>par)</span>
<span id="cb6-18"><a href="#cb6-18" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">rbind</span>(</span>
<span id="cb6-19"><a href="#cb6-19" tabindex="-1"></a>    <span class="at">observed =</span> <span class="fu">table</span>(flips)<span class="sc">/</span><span class="fu">sum</span>(<span class="fu">table</span>(flips)),</span>
<span id="cb6-20"><a href="#cb6-20" tabindex="-1"></a>    <span class="at">map =</span> prs_map,</span>
<span id="cb6-21"><a href="#cb6-21" tabindex="-1"></a>    <span class="at">mle =</span> prs_mle</span>
<span id="cb6-22"><a href="#cb6-22" tabindex="-1"></a>), <span class="dv">2</span>)</span>
<span id="cb6-23"><a href="#cb6-23" tabindex="-1"></a><span class="do">##          Heads Tails</span></span>
<span id="cb6-24"><a href="#cb6-24" tabindex="-1"></a><span class="do">## observed  0.85  0.15</span></span>
<span id="cb6-25"><a href="#cb6-25" tabindex="-1"></a><span class="do">## map       0.65  0.35</span></span>
<span id="cb6-26"><a href="#cb6-26" tabindex="-1"></a><span class="do">## mle       0.85  0.15</span></span></code></pre></div>
</div>
</div>
</div>
<div id="deviance" class="slide section level2">
<h1>Deviance</h1>
<ul>
<li>We can compare the models’ relative performance without knowing
anything about <span class="math inline">\(\theta_{true}\)</span></li>
<li>We just need to know the entropy of each model: <span
class="math inline">\(H_{\theta} = -\mathbb{E}\left [\log  pr \left(x |
\theta \right) \right ]\)</span></li>
<li><strong>Deviance</strong> is an estimate of the entropy of a
model:</li>
</ul>
<p><span class="math display">\[
    D = -2 \sum \log pr \left(x | \theta \right)
\]</span></p>
<ul>
<li>This is simply twice the log liklihood</li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>Dmap <span class="ot">=</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">log_likelihood</span>(prs_map[<span class="dv">1</span>], flips)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>Dfair <span class="ot">=</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">log_likelihood</span>(pr_fair[<span class="dv">1</span>], flips)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>Dmle <span class="ot">=</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">log_likelihood</span>(prs_mle[<span class="dv">1</span>], flips)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">c</span>(<span class="at">map =</span> Dmap,<span class="at">mle =</span> Dmle, <span class="at">fair =</span> Dfair), <span class="dv">3</span>)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="do">##    map    mle   fair </span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="do">##  7.016  2.831 13.648</span></span></code></pre></div>
<ul>
<li>The MAP estimate has lower deviance, meaning it will predict new
observations better than the hypothesis that the coin is fair</li>
<li>The prior has a significant effect as well!</li>
</ul>
</div>
<div id="information-criteria" class="slide section level2">
<h1>Information criteria</h1>
<ul>
<li>Take two models, <span class="math inline">\(\theta_1\)</span> has
one parameter, <span class="math inline">\(\theta_2\)</span> has
two</li>
<li>If the models are nested (<span
class="math inline">\(\theta_1\)</span> is a subset of <span
class="math inline">\(\theta_2\)</span>)</li>
<li><span class="math inline">\(D_1\)</span> will always be greater than
<span class="math inline">\(D_2\)</span>
<ul>
<li>This is due to the increased flexibility gained by adding
parameters</li>
</ul></li>
<li>Thus we need a way of estimating <em>out-of-sample</em> deviance
<ul>
<li>How well do the models fit data that were not used for
calibration</li>
</ul></li>
<li>Information criteria approximate this by penalizing models for
complexity</li>
</ul>
<p><span class="math display">\[
    \mathrm{AIC} = 2k - D\left [ \mathbb{E} \left (\theta \right )
\right ]
\]</span></p>
</div>
<div id="aic-for-the-tsuga-model" class="slide section level2">
<h1>AIC for the Tsuga model</h1>
<ul>
<li>Last time, we had three varieties of models for the effect of
precipitation on <em>Tsuga</em> mortality.</li>
<li>We add a fourth here, with partial pooling but only for intercepts
<ol style="list-style-type: decimal">
<li>Completely pooled</li>
<li>Unpooled intercepts (one intercept per year)</li>
<li>Partially pooled intercepts &amp; slopes (hierarchical model)</li>
<li>Partially pooled intercepts (hierarchical model)</li>
</ol></li>
</ul>
</div>
<div id="aic-for-the-tsuga-model-1" class="slide section level2">
<h1>AIC for the Tsuga model</h1>
<ul>
<li>Last time, we had three varieties of models for the effect of
temperature on <em>Tsuga</em> mortality
<ol style="list-style-type: decimal">
<li>Completely pooled</li>
<li>Unpooled intercepts (one intercept per year)</li>
<li>Partially pooled intercepts (hierarchical model)</li>
</ol></li>
<li>We can estimate the MAP values for the parameters using
<code>rstan::optimizing</code></li>
<li>We just need to modify the Stan program to compute the log
likelihood and the deviance</li>
</ul>
<div class="sourceCode" id="cb8"><pre
class="sourceCode stan"><code class="sourceCode stan"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>    <span class="dt">real</span> deviance = <span class="dv">0</span>;</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    <span class="dt">vector</span> [n] loglik;</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span>:n) {</span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>        loglik[i] = binomial_lpmf(died[i] | n_trees[i], p[i]);</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>        deviance += loglik[i];</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>    }</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>    deviance = -<span class="dv">2</span> * deviance;</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="aic-for-the-tsuga-model-2" class="slide section level2">
<h1>AIC for the Tsuga model</h1>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="fu">library</span>(rstan)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>map_up <span class="ot">=</span> <span class="fu">optimizing</span>(tsuga_unpooled, standat)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>map_p <span class="ot">=</span> <span class="fu">optimizing</span>(tsuga_pooled, standat)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>k1 <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">grepl</span>(<span class="st">&quot;^[a|b]&quot;</span>, <span class="fu">names</span>(map_p<span class="sc">$</span>par)))  <span class="co"># 2 parameters</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>k2 <span class="ot">=</span> <span class="fu">sum</span>(<span class="fu">grepl</span>(<span class="st">&quot;^[a|b]&quot;</span>, <span class="fu">names</span>(map_up<span class="sc">$</span>par)))  <span class="co"># 17 parameters</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>aic <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">2</span><span class="sc">*</span>k1 <span class="sc">-</span> map_up<span class="sc">$</span>par[<span class="st">&quot;deviance&quot;</span>], <span class="dv">2</span><span class="sc">*</span>k2 <span class="sc">-</span> map_p<span class="sc">$</span>par[<span class="st">&quot;deviance&quot;</span>])</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>res <span class="ot">=</span> <span class="fu">rbind</span>(aic, aic <span class="sc">-</span> <span class="fu">min</span>(aic)); <span class="fu">rownames</span>(res) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;AIC&quot;</span>, <span class="st">&quot;delta&quot;</span>); <span class="fu">colnames</span>(res) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;Pooled&quot;</span>, <span class="st">&quot;Unpooled&quot;</span>)</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a>res</span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a><span class="do">##            Pooled  Unpooled</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a><span class="do">## AIC   -5840.80002 -5865.183</span></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a><span class="do">## delta    24.38348     0.000</span></span></code></pre></div>
<ul>
<li>AIC prefers the more complex model:
<ul>
<li>Priors must be uninformative or totally overwhelmed by the data</li>
<li>Posterior must be multivariate normal</li>
<li>Sample size must be very large relative to the number of parameters
(n &gt; ~25*k)</li>
</ul></li>
<li>Moreover, AIC not well-defined for the hierarchial model
<ul>
<li>Parameters are not independent, so the model is less “complex” than
the unpooled</li>
</ul></li>
</ul>
</div>
<div id="deviance-information-critereon" class="slide section level2">
<h1>Deviance information critereon</h1>
<ul>
<li>Bayesian models often have difficult-to-estimate numbers of
parameters</li>
<li>We can instead use posterior samples to approximate model complexity
<ul>
<li>How much to models differ from the “best” model, on average</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
p_D &amp; = \mathbb{E} \left[ D \left (\theta \right ) \right] - D\left
[ \mathbb{E} \left (\theta \right ) \right ] \\
\mathrm{DIC} &amp;= D\left [ \mathbb{E} \left (\theta \right ) \right ]
+ 2p_D
\end{aligned}
\]</span></p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>fit_up <span class="ot">=</span> <span class="fu">sampling</span>(tsuga_unpooled, standat, <span class="at">iter=</span><span class="dv">5000</span>, <span class="at">refresh=</span><span class="dv">0</span>, <span class="at">chains=</span><span class="dv">1</span>)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>fit_p <span class="ot">=</span> <span class="fu">sampling</span>(tsuga_pooled, standat, <span class="at">iter=</span><span class="dv">5000</span>, <span class="at">refresh=</span><span class="dv">0</span>, <span class="at">chains=</span><span class="dv">1</span>)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>fit_pp <span class="ot">=</span> <span class="fu">sampling</span>(tsuga_ppool, standat, <span class="at">iter=</span><span class="dv">5000</span>, <span class="at">refresh=</span><span class="dv">0</span>, <span class="at">chains=</span><span class="dv">1</span>)</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>fit_ipp <span class="ot">=</span> <span class="fu">sampling</span>(tsuga_int_ppool, standat, <span class="at">iter=</span><span class="dv">5000</span>, <span class="at">refresh=</span><span class="dv">0</span>, <span class="at">chains=</span><span class="dv">1</span>)</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="do">## note for the hierarchical model, a MAP is hard to find,</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a><span class="do">## here we use the min deviance as a quick and dirty estimate</span></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a><span class="do">## a better estimate is to compute the mean of all parameters, then </span></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a><span class="do">## compute the deviance with those values</span></span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>pd <span class="ot">=</span> <span class="fu">c</span>(<span class="at">Pooled =</span> <span class="fu">mean</span>(<span class="fu">as.matrix</span>(fit_p, <span class="at">pars=</span><span class="st">&quot;deviance&quot;</span>)) <span class="sc">-</span> map_up<span class="sc">$</span>par[<span class="st">&quot;deviance&quot;</span>],</span>
<span id="cb10-11"><a href="#cb10-11" tabindex="-1"></a>    <span class="at">Unpooled =</span> <span class="fu">mean</span>(<span class="fu">as.matrix</span>(fit_up, <span class="at">pars=</span><span class="st">&quot;deviance&quot;</span>)) <span class="sc">-</span> map_p<span class="sc">$</span>par[<span class="st">&quot;deviance&quot;</span>],</span>
<span id="cb10-12"><a href="#cb10-12" tabindex="-1"></a>    <span class="at">Hierarchical =</span> <span class="fu">mean</span>(<span class="fu">as.matrix</span>(fit_pp, <span class="at">pars=</span><span class="st">&quot;deviance&quot;</span>)) <span class="sc">-</span> <span class="fu">min</span>(<span class="fu">as.matrix</span>(fit_pp, <span class="at">pars=</span><span class="st">&quot;deviance&quot;</span>)),</span>
<span id="cb10-13"><a href="#cb10-13" tabindex="-1"></a>    <span class="at">Hierarchical_intercept =</span> <span class="fu">mean</span>(<span class="fu">as.matrix</span>(fit_ipp, <span class="at">pars=</span><span class="st">&quot;deviance&quot;</span>)) <span class="sc">-</span> <span class="fu">min</span>(<span class="fu">as.matrix</span>(fit_ipp, <span class="at">pars=</span><span class="st">&quot;deviance&quot;</span>)))</span>
<span id="cb10-14"><a href="#cb10-14" tabindex="-1"></a>dic <span class="ot">=</span> <span class="dv">1</span><span class="sc">*</span>pd <span class="sc">+</span> <span class="fu">c</span>(map_up<span class="sc">$</span>par[<span class="st">&quot;deviance&quot;</span>], map_p<span class="sc">$</span>par[<span class="st">&quot;deviance&quot;</span>], <span class="fu">min</span>(<span class="fu">as.matrix</span>(fit_pp, <span class="at">pars=</span><span class="st">&quot;deviance&quot;</span>)),</span>
<span id="cb10-15"><a href="#cb10-15" tabindex="-1"></a>             <span class="fu">min</span>(<span class="fu">as.matrix</span>(fit_ipp, <span class="at">pars=</span><span class="st">&quot;deviance&quot;</span>)))</span>
<span id="cb10-16"><a href="#cb10-16" tabindex="-1"></a>dic <span class="ot">=</span> <span class="fu">rbind</span>(dic, dic <span class="sc">-</span> <span class="fu">min</span>(dic)); <span class="fu">rownames</span>(dic) <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;DIC&quot;</span>, <span class="st">&quot;delta&quot;</span>)</span>
<span id="cb10-17"><a href="#cb10-17" tabindex="-1"></a>dic</span>
<span id="cb10-18"><a href="#cb10-18" tabindex="-1"></a><span class="do">##       Pooled.deviance Unpooled.deviance Hierarchical Hierarchical_intercept</span></span>
<span id="cb10-19"><a href="#cb10-19" tabindex="-1"></a><span class="do">## DIC         5901.1422         5860.5243     5754.922              5872.5218</span></span>
<span id="cb10-20"><a href="#cb10-20" tabindex="-1"></a><span class="do">## delta        146.2202          105.6022        0.000               117.5998</span></span></code></pre></div>
</div>
<div id="log-predictive-pointwise-density" class="slide section level2">
<h1>Log predictive pointwise density</h1>
<ul>
<li>For real models, DIC has some problems
<ul>
<li>assumes posterior is multivariate normal</li>
<li>only assesses the “best” fit, doesn’t fully use the posterior</li>
<li>these can strongly influence inference, as well will see</li>
</ul></li>
<li>Instead of considering deviance of each model, we can consider the
probability of each <em>data point</em>
<ul>
<li>This is closely related (but more fine-grained) to the penalty
applied for DIC</li>
<li>Instead of averaging within models, we average for each point across
<span class="math inline">\(S\)</span> posterior samples</li>
</ul></li>
</ul>
<p><span class="math display">\[
\mathrm{lppd} = \sum_{i=1}^n \log \frac{1}{S} \sum_{j=1}^S pr(y_i |
\theta_j)
\]</span></p>
</div>
<div id="cross-validation" class="slide section level2">
<h1>Cross validation</h1>
<ul>
<li>To best understand model performance, we should compare
<em>out-of-sample</em></li>
<li>Holdout/validation dataset
<ul>
<li>separate, independent dataset not used for calibration</li>
</ul></li>
<li>k-fold
<ul>
<li>separate model into k “folds”</li>
<li>For each fold, fit the model to all data not in fold</li>
<li>validate against model in fold</li>
<li>best model has best average performace across all folds</li>
</ul></li>
<li>LOO
<ul>
<li>For <span class="math inline">\(n\)</span> data points, you fit
<span class="math inline">\(n\)</span> models</li>
<li>Each time, you leave out one data point for validation</li>
</ul></li>
</ul>
</div>
<div id="loo" class="slide section level2">
<h1>LOO</h1>
<ul class="incremental">
<li>We can use the lppd to approximate LOO without the cost of fitting
<span class="math inline">\(n\)</span> models</li>
<li>Conceptually, AIC and DIC are attempts at <em>approximating</em>
LOO</li>
<li>WAIC (Widely applicable information criterion) extends this, using
lppd instead of deviance</li>
</ul>
</div>
<div id="loo-1" class="slide section level2">
<h1>LOO</h1>
<ul>
<li>We can use the lppd to approximate LOO without the cost of fitting
<span class="math inline">\(n\)</span> models</li>
<li>Conceptually, AIC and DIC are attempts at <em>approximating</em>
LOO</li>
<li>WAIC (Widely applicable information criterion) extends this, using
lppd instead of deviance
<ul>
<li>It is pointwise, so it approximates the cost to model fitting of
leaving out each point</li>
<li>Averaged across the entire posterior</li>
</ul></li>
</ul>
</div>
<div id="loo-2" class="slide section level2">
<h1>LOO</h1>
<ul>
<li>We can use the lppd to approximate LOO without the cost of fitting
<span class="math inline">\(n\)</span> models</li>
<li>Conceptually, AIC and DIC are attempts at <em>approximating</em>
LOO</li>
<li>WAIC (Widely applicable information criterion) extends this, using
lppd instead of deviance
<ul>
<li>It is pointwise, so it approximates the cost to model fitting of
leaving out each point</li>
<li>Averaged across the entire posterior</li>
</ul></li>
<li>Another technique, <em>importance sampling</em>, can directly
approximate LOO
<ul>
<li>These methods don’t always work, but when they doo, LOO/WAIC provide
more robust estimates than DIC/AIC</li>
</ul></li>
</ul>
</div>
<div id="loo-for-trees" class="slide section level2">
<h1>LOO for trees</h1>
<p>Package <code>loo</code> can estimate LOO-IC and WAIC for us. All
that is needed is to compute the lpd (log pointwise density) in your
Stan model.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode stan"><code class="sourceCode stan"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>    <span class="dt">vector</span> [n] loglik; <span class="co">// vector, one per data point, because this is the pointwise density</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span>:n) {</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>        <span class="co">// here change binomial to whatever likelihood function applies to the model</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>        <span class="co">// lpmf: &quot;log probability mass function&quot;</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>        <span class="co">// for continuous distributions, use lpdf, e.g., normal_lpdf</span></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>        loglik[i] = binomial_lpmf(died[i] | n_trees[i], p[i]);</span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a>    }</span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="loo-for-trees-1" class="slide section level2">
<h1>LOO for trees</h1>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="do">## put all models into a list for convenience when doing repeated operations</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>fits <span class="ot">=</span> <span class="fu">list</span>(<span class="at">unpooled=</span>fit_up, <span class="at">pooled=</span>fit_p, <span class="at">hierarchical=</span>fit_pp, <span class="at">hierarchical_int=</span>fit_ipp)</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="do">## get a loo object for each model</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>loos <span class="ot">=</span> <span class="fu">lapply</span>(fits, loo, <span class="at">pars=</span><span class="st">&quot;loglik&quot;</span>)</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>loos<span class="sc">$</span>unpooled</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a><span class="do">## Computed from 2500 by 1406 log-likelihood matrix.</span></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a><span class="do">##          Estimate    SE</span></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a><span class="do">## elpd_loo  -2965.9 134.3</span></span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a><span class="do">## p_loo        64.1   7.4</span></span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a><span class="do">## looic      5931.8 268.5</span></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a><span class="do">## ------</span></span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a><span class="do">## MCSE of elpd_loo is NA.</span></span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a><span class="do">## MCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.4]).</span></span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a><span class="do">## Pareto k diagnostic values:</span></span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a><span class="do">##                          Count Pct.    Min. ESS</span></span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a><span class="do">## (-Inf, 0.7]   (good)     1400  99.6%   74      </span></span>
<span id="cb12-22"><a href="#cb12-22" tabindex="-1"></a><span class="do">##    (0.7, 1]   (bad)         5   0.4%   &lt;NA&gt;    </span></span>
<span id="cb12-23"><a href="#cb12-23" tabindex="-1"></a><span class="do">##    (1, Inf)   (very bad)    1   0.1%   &lt;NA&gt;    </span></span>
<span id="cb12-24"><a href="#cb12-24" tabindex="-1"></a><span class="do">## See help(&#39;pareto-k-diagnostic&#39;) for details.</span></span></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">loo_compare</span>(loos), <span class="at">simplify =</span> <span class="cn">FALSE</span>)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="do">##                  elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="do">## hierarchical         0.0       0.0 -2917.0    132.9        76.1     8.8 </span></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="do">## pooled             -38.5      24.0 -2955.5    130.4         9.7     1.1 </span></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a><span class="do">## hierarchical_int   -43.6      21.8 -2960.7    133.3        48.1     5.2 </span></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a><span class="do">## unpooled           -48.9      23.0 -2965.9    134.3        64.1     7.4 </span></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a><span class="do">##                  looic   se_looic</span></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a><span class="do">## hierarchical      5834.0   265.9 </span></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a><span class="do">## pooled            5911.0   260.7 </span></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a><span class="do">## hierarchical_int  5921.3   266.5 </span></span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a><span class="do">## unpooled          5931.8   268.5</span></span></code></pre></div>
</div>
<div id="model-weights" class="slide section level2">
<h1>Model weights</h1>
<ul>
<li>LOO/W/A/D IC can choose a “best” model for us</li>
<li>But all models are wrong, and even models that aren’t the best
contain information</li>
<li>Akaike weights are a computationally simple way of performing
mulimodel inference
<ul>
<li>Roughly speaking, <span class="math inline">\(w_i\)</span> is the
probability that model <span class="math inline">\(i\)</span> is the
best model in the set of <span class="math inline">\(m\)</span>
models</li>
<li>if using <code>loo</code>’s <code>elpd_diff</code>, can omit the
<span class="math inline">\(-\frac{1}{2}\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[
w_i = \frac{e^{-\frac{1}{2}\Delta \mathrm{*IC}_i}}
{\sum_{j=1}^m e^{-\frac{1}{2}\Delta \mathrm{*IC}_j}}
\]</span></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>looics <span class="ot">=</span> <span class="fu">sapply</span>(loos, <span class="cf">function</span>(x) x<span class="sc">$</span>estimates[<span class="st">&#39;looic&#39;</span>, <span class="st">&#39;Estimate&#39;</span>])</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>delta_looic <span class="ot">=</span> looics <span class="sc">-</span> <span class="fu">min</span>(looics)</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>wi <span class="ot">=</span> <span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span><span class="sc">*</span>delta_looic) <span class="sc">/</span> <span class="fu">sum</span>(<span class="fu">exp</span>(<span class="sc">-</span><span class="fl">0.5</span><span class="sc">*</span>delta_looic))</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">rbind</span>(<span class="at">looic =</span> looics, <span class="at">dlooic =</span> delta_looic, <span class="at">weight =</span> wi), <span class="dv">2</span>)</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a><span class="do">##        unpooled  pooled hierarchical hierarchical_int</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a><span class="do">## looic   5931.85 5910.96      5834.02          5921.32</span></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a><span class="do">## dlooic    97.83   76.94         0.00            87.29</span></span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="do">## weight     0.00    0.00         1.00             0.00</span></span></code></pre></div>
</div>
<div id="weights-for-multimodel-inference" class="slide section level2">
<h1>Weights for multimodel inference</h1>
<ul>
<li>Akaike weights can help us decide how much to trust a single ‘best’
model</li>
<li>They underestimate information contained in the posteriors of the
worse models</li>
<li>A more Bayesian approach is to make predictions, averaging the
uncertainty from all models</li>
<li>For this we use <strong>stacking weights</strong>, which choose
weights based on maximising the <code>elpd</code> from LOO</li>
</ul>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>wts <span class="ot">=</span> <span class="fu">loo_model_weights</span>(loos)</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>wts</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="do">## Method: stacking</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a><span class="do">## ------</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a><span class="do">##                  weight</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="do">## unpooled         0.157 </span></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="do">## pooled           0.243 </span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a><span class="do">## hierarchical     0.600 </span></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a><span class="do">## hierarchical_int 0.000</span></span></code></pre></div>
</div>
<div id="bayesian-model-averaging" class="slide section level2">
<h1>Bayesian model averaging</h1>
<ul>
<li>We can produce a posterior predictive distribution in Stan</li>
</ul>
<div class="sourceCode" id="cb16"><pre
class="sourceCode stan"><code class="sourceCode stan"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="kw">generated quantities</span> {</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>    <span class="dt">vector</span> [n] loglik; <span class="co">// vector, one per data point, because this is the pointwise density</span></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>    <span class="dt">int</span> ppd_died [n]; <span class="co">// predicted number of trees dying at each point in the original dataset</span></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span>:n) {</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>        <span class="co">// here change binomial to whatever likelihood function applies to the model</span></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>        <span class="co">// lpmf: &quot;log probability mass function&quot;</span></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>        <span class="co">// for continuous distributions, use lpdf, e.g., normal_lpdf</span></span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>        loglik[i] = binomial_lpmf(died[i] | n_trees[i], p[i]);</span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a>        ppd_died[i] = binomial_rng(<span class="dv">10</span>, p[i]); <span class="co">// here I give a ppd assuming a sample size of 10 trees</span></span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a>    }</span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a>}</span></code></pre></div>
<ul>
<li>Then apply the weights to our predictions</li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>ppd <span class="ot">=</span> <span class="fu">lapply</span>(fits, as.matrix, <span class="at">pars=</span><span class="st">&quot;ppd_died&quot;</span>)</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>weighted_ppd <span class="ot">=</span> <span class="fu">Map</span>(<span class="st">`</span><span class="at">*</span><span class="st">`</span>, ppd, wts)</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>avg_predictions <span class="ot">=</span> <span class="fu">Reduce</span>(<span class="st">`</span><span class="at">+</span><span class="st">`</span>, weighted_ppd)</span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a><span class="do">## now get some quantile intervals, put them alongside the original values and the temperature</span></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a>quants <span class="ot">=</span> <span class="fu">lapply</span>(ppd, <span class="cf">function</span>(x) <span class="fu">cbind</span>(<span class="fu">data.table</span>(<span class="fu">t</span>(<span class="fu">apply</span>(x, <span class="dv">2</span>, quantile, <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.05</span>, <span class="fl">0.95</span>)))), tsuga))</span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a>quants<span class="sc">$</span>ensemble <span class="ot">=</span> <span class="fu">cbind</span>(<span class="fu">data.table</span>(<span class="fu">t</span>(<span class="fu">apply</span>(avg_predictions, <span class="dv">2</span>, quantile, <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.05</span>, <span class="fl">0.95</span>)))), tsuga)</span>
<span id="cb17-8"><a href="#cb17-8" tabindex="-1"></a>quants <span class="ot">=</span> <span class="fu">rbindlist</span>(quants, <span class="at">idcol=</span><span class="st">&quot;model&quot;</span>)</span>
<span id="cb17-9"><a href="#cb17-9" tabindex="-1"></a><span class="fu">colnames</span>(quants)[<span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>] <span class="ot">=</span> <span class="fu">c</span>(<span class="st">&quot;median&quot;</span>, <span class="st">&quot;lower&quot;</span>, <span class="st">&quot;upper&quot;</span>)</span>
<span id="cb17-10"><a href="#cb17-10" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" tabindex="-1"></a>pl <span class="ot">=</span> <span class="fu">ggplot</span>(quants[year <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">2005</span><span class="sc">:</span><span class="dv">2008</span>, <span class="dv">2011</span><span class="sc">:</span><span class="dv">2012</span>)], <span class="fu">aes</span>(<span class="at">x=</span>tot_annual_pp, <span class="at">y =</span> median<span class="sc">/</span><span class="dv">10</span>, <span class="at">colour =</span> model)) <span class="sc">+</span> </span>
<span id="cb17-12"><a href="#cb17-12" tabindex="-1"></a>    <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb17-13"><a href="#cb17-13" tabindex="-1"></a>    <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">x=</span>tot_annual_pp, <span class="at">ymin=</span> lower<span class="sc">/</span><span class="dv">10</span>, <span class="at">ymax=</span>upper<span class="sc">/</span><span class="dv">10</span>, <span class="at">fill =</span> model), <span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> </span>
<span id="cb17-14"><a href="#cb17-14" tabindex="-1"></a>    <span class="fu">facet_grid</span>(model <span class="sc">~</span> year) <span class="sc">+</span> <span class="fu">theme_minimal</span>() <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>tot_annual_pp, <span class="at">y =</span> died<span class="sc">/</span>n), <span class="at">size=</span><span class="fl">0.6</span>) <span class="sc">+</span> </span>
<span id="cb17-15"><a href="#cb17-15" tabindex="-1"></a>    <span class="fu">xlab</span>(<span class="st">&quot;Total Annual Precipitation (mm)&quot;</span>) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;Proportion dying&quot;</span>)</span>
<span id="cb17-16"><a href="#cb17-16" tabindex="-1"></a>    </span></code></pre></div>
</div>
<div id="bayesian-model-averaging-results" class="slide section level2">
<h1>Bayesian model averaging results</h1>
<p><img src="8_model_selection_files/figure-slidy/unnamed-chunk-11-1.png" width="1728" /></p>
</div>

  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

</body>
</html>
