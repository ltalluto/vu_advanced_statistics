---
title: "Markov Chain Monte Carlo"
author: "Matthew Talluto"
date: "23.11.2023"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
    self_contained: false
    lib_dir: lib
  beamer_presentation: default
---


```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png", error = TRUE)
library(ggplot2)
library(data.table)
library(cowplot)
library(rstan)
```

## The German Tank Problem

::: {.columns}
:::: {.column}

* During WW2, the Allies wanted to know the rate of tank production in German factories
* Known: factories stamped serial numbers on the tanks in order, starting with 1
* I have captured a single tank, with (for example), the serial number $s = 200$. 
* How many tanks have been produced? $(N)$

:::: 
:::: {.column}

:::: 
:::


## The German Tank Problem

::: {.columns}
:::: {.column}

### Knowns:

* Captured serial number $s = 200$
* Number of tanks $N >= s$
* Assume tanks are captured completely at random, so all tanks have the same probability of capture
* This sounds like a uniform distribution!
* Minimum of the distribution is 1, the maximum will be N.

:::: 
:::: {.column}

::::
:::



## The German Tank Problem

::: {.columns}
:::: {.column}

### Knowns:

* Captured serial number $s = 200$
* Number of tanks $N >= s$
* Assume tanks are captured completely at random, so all tanks have the same probability of capture
* This sounds like a uniform distribution!
* Minimum of the distribution is 1, the maximum will be N.
* **Hypothesis**: *We got the biggest tank; $s = N$*

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE, fig.height = 7, fig.width = 6}
cols = scales::hue_pal()(4)
vals = c(200, 400, 800, 2000)
s_giv_n = data.table(
	N = rep(vals, times = 2400),
	s = rep(1:2400, length(vals))
)
s_giv_n[, pr := dunif(s, 1, N)]
labs = paste("N =", vals)

pl_top = ggplot() + theme_minimal() + xlab("s") + ylab("pr(N | s)") + 
	xlim(0, 2400) + ylim(0, max(s_giv_n$pr, na.rm = TRUE)*1.2)
p1 = pl_top + geom_line(data = s_giv_n[N==200], aes(x = s, y = pr, colour = factor(N)), size = 0.9) +
	scale_color_hue(labels = labs[1], name = element_blank())
leg = get_legend(p1)
p1 = p1 + guides(colour = "none")

s = 200
tankdat = data.table(
	N = 1:2400
)
tankdat$pr = dunif(s, 1, tankdat$N)
tankpts = tankdat[N %in% c(s, 2*s, 4*s, 10*s)]
tankpts$labs = paste("N =", tankpts$N)

pl = ggplot() + theme_minimal() + xlab("N") + ylab("pr(s | N)") + 
	xlim(0, max(tankdat$N)) + ylim(0, max(tankdat$pr, na.rm = TRUE)*1.2)
p2 = pl + geom_line(data = tankdat[N < 200], aes(x = N, y = pr), col = cols[1], size = 1.5)

plot_grid(p1, ggdraw(leg), pl, ncol = 2, nrow = 2, rel_widths = c(3, 1))
```

:::: 
:::




## The German Tank Problem

::: {.columns}
:::: {.column}

### Knowns:

* Captured serial number $s = 200$
* Number of tanks $N >= s$
* Assume tanks are captured completely at random, so all tanks have the same probability of capture
* This sounds like a uniform distribution!
* Minimum of the distribution is 1, the maximum will be N.
* **Hypothesis**: *We got the biggest tank; $s = N$*

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE, fig.height = 7, fig.width = 6}
yadj = 0.0002
p200 = pl + geom_line(data = tankdat[N <= 200], aes(x = N, y = pr), col = cols[1], size = 1.5) +
	geom_point(data = tankpts[N <= 200], aes(x = N, y = pr), pch = 21, bg = cols[2], size = 1.5) + 
	geom_text(data = tankpts[N <= 200], aes(x = N + 150, y = pr + yadj, label = labs), size = 3)

# grid.arrange(p1, pl, nrow = 2, widths = c(1, 0.8))
plot_grid(p1, ggdraw(leg), p200, ncol = 2, nrow = 2, rel_widths = c(3, 1))
```

:::: 
:::






## The German Tank Problem

::: {.columns}
:::: {.column}

### Hypothesis 2: $N = 400$
* The data $s$ have not changed, just the model
* All numbers are still equally likely to be captured
	- But now there are twice as many possibilities
	- Each individual tank is half as likely to be captured

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE, fig.height = 7, fig.width = 6}
p400 = pl + geom_line(data = tankdat[N <= 200], aes(x = N, y = pr), col = cols[1], size = 1.5) +
	geom_point(data = tankpts[N <= 400], aes(x = N, y = pr), pch = 21, bg = cols[2], size = 1.5) + 
	geom_text(data = tankpts[N <= 400], aes(x = N + 150, y = pr + yadj, label = labs), size = 3)

p1_400 = pl_top + geom_line(data = s_giv_n[N <= 400], aes(x = s, y = pr, colour = factor(N)), size = 0.9) +
	scale_color_hue(labels = labs[1:2], name = element_blank())
leg = get_legend(p1_400)
p1_400 = p1_400 + guides(colour = "none")

plot_grid(p1_400, ggdraw(leg), p400, ncol = 2, nrow = 2, rel_widths = c(3, 1))
```

:::: 
:::





## The German Tank Problem

::: {.columns}
:::: {.column}

### Hypothesis 2: $N = 400$
* The data $s$ have not changed, just the model
* All numbers are still equally likely to be captured
	- But now there are twice as many possibilities
	- Each individual tank is half as likely to be captured
	
### Hypothesis 3: $N = 800$

* The same logic applies!

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE, fig.height = 7, fig.width = 6}
p800 = pl + geom_line(data = tankdat[N <= 200], aes(x = N, y = pr), col = cols[1], size = 1.5) +
	geom_point(data = tankpts[N <= 800], aes(x = N, y = pr), pch = 21, bg = cols[2], size = 1.5) + 
	geom_text(data = tankpts[N <= 800], aes(x = N + 150, y = pr + yadj, label = labs), size = 3)

p1_800 = pl_top + geom_line(data = s_giv_n[N <= 800], aes(x = s, y = pr, colour = factor(N)), size = 0.9) +
	scale_color_hue(labels = labs[1:3], name = element_blank())
leg = get_legend(p1_400)
p1_800 = p1_800 + guides(colour = "none")

plot_grid(p1_800, ggdraw(leg), p800, ncol = 2, nrow = 2, rel_widths = c(3, 1))
```

:::: 
:::





## The German Tank Problem

::: {.columns}
:::: {.column}

### Hypothesis 2: $N = 400$
* The data $s$ have not changed, just the model
* All numbers are still equally likely to be captured
	- But now there are twice as many possibilities
	- Each individual tank is half as likely to be captured
	
### Hypothesis 3: $N = 800$

* The same logic applies!

... and so on.

**What is the MLE?** Is there a logical problem with that?

$$ s \sim  Uniform(min = 1, max = N)$$
:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE, fig.height = 7, fig.width = 6}
pall = pl + geom_line(data = tankdat, aes(x = N, y = pr), col = cols[1], size = 1.5) +
	geom_point(data = tankpts, aes(x = N, y = pr), pch = 21, bg = cols[2], size = 1.5) + 
	geom_text(data = tankpts, aes(x = N + 150, y = pr + yadj, label = labs), size = 3)

p1_all = pl_top + geom_line(data = s_giv_n, aes(x = s, y = pr, colour = factor(N)), size = 0.9) +
	scale_color_hue(labels = labs, name = element_blank())
leg = get_legend(p1_400)
p1_all = p1_all + guides(colour = "none")

plot_grid(p1_all, ggdraw(leg), pall, ncol = 2, nrow = 2, rel_widths = c(3, 1))
```

:::: 
:::







## Rejection sampling

::: {.columns}
:::: {.column}

* There are general algorithms for sampling from unknown distributions
	- Here, the **target distribution**, $t(x)$, is difficult to sample from

$$
t(x) = pr(s|N) \sim Uniform(s,N)
$$

* However, a uniform distribution with a fixed minimum and maximum is easy to sample from
	- We will call this the **proposal distribution** $p(x)$
	- We will rescale it so that it is always taller than $t(x)$

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE}
px_max = 4000 # the maximum of our uniform sample distribution

# write two small functions to get the density of and to sample from p(x)
dpx = function(x) dunif(x, s, px_max)
rpx = function(n) runif(n, s, px_max)

# also write a density function for the target distribution
# and convert it into a log likelihood function while we are at it
dtx = function(x, N, log = FALSE) dunif(x, 1, N, log = log)
llfun = function(N, s) sum(dtx(s, N, log = TRUE))

# this is the highest point of t(x)
ymax = dtx(s, s)
# use it to scale p(x) at the highest point
scale = ymax/dpx(s)
dpx = function(x) scale * dunif(x, s, px_max)

tankdat$px = dpx(s)
tankdat$tx = dtx(s, tankdat$N)
p_rej = pl + geom_line(data = tankdat, aes(x = N, y = tx), col = "#00BFC4", size = 1.5) +
	geom_line(data = tankdat, aes(x = N, y = px), col = "#F8766D", size = 1.5) + 
	annotate(geom = "text", x = 800, y = dtx(s, 550), label = "Target t(x)", col = "#00BFC4") + 
	annotate(geom = "text", x = 1000, y = dpx(1000)*1.03, label = "Proposal p(x)", col = "#F8766D") 
```

:::: 
:::





## Rejection sampling

::: {.columns}
:::: {.column}
::::: {.algo}

**Algorithm**

1. Define target distribution t(x)
2. Define proposal distribution p(x)
3. Draw a random sample $y$ from p(x)
4. Compute **acceptance probability** $r = \frac{t(y)}{p(y)}$
5. Accept the sample with probability $r$, reject with probability $1-r$
6. Repeat until desired number of samples obtained

:::::
:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE, cache = TRUE, message = FALSE}
p_rej
```

:::: 
:::


## Rejection sampling

::: {.columns}
:::: {.column}
::::: {.algo}

**Algorithm**

1. Define target distribution t(x)
2. Define proposal distribution p(x)
3. Draw a random sample $y$ from p(x)
4. Compute **acceptance probability** $r = \frac{t(y)}{p(y)}$
5. Accept the sample with probability $r$, reject with probability $1-r$
6. Repeat until desired number of samples obtained

:::::

This is **incredibly inefficient**


```{r}
s = 200
target = function(x) dunif(s, 1, x)
proposal = function(x) dunif(x, 1, 1e6)

## this scale will make sure p(x) >= t(x)
scale = target(s) / proposal(s) 
candidates = as.integer(runif(1e6, 1, 1e6))
r = target(candidates)/(proposal(candidates) * scale)

## uniform numbers between 0 and 1
test = runif(length(r))  

## TRUE if r > test; WHY?
accept = ifelse(r > test, TRUE, FALSE) 
samples = candidates[accept]

```

:::: 
:::: {.column}

```{r echo = FALSE, warning=FALSE}
samples2 = samples[samples <= max(tankdat$N)]
par(mgp=c(0.5,0,0), mar=c(2,2,2,0.2))
yy = dunif(s, 1, tankdat$N)
plot(tankdat$N, dunif(s, 1, tankdat$N), type='l', ylim = c(0, 0.0055), bty='l', lwd=2, col='#00BFC4', xaxt='n', xlab="N", yaxt='n', ylab="pr(s | N)")
lines(tankdat$N, tankdat$px, lwd=2, col='#F8766D')
par(new=TRUE)
hist(samples2, breaks = 20, freq=FALSE, axes=FALSE, xlim=c(0, max(tankdat$N)),
     col="#C77CFF66", border = "#C77CFF", bty='l', xlab="N", ylab="pr(s | N)", main="")
legend("right", legend=c("proposal", "target", 
       paste("acceptance rate =", round(length(samples)/length(candidates), 3))), lwd=c(2, 2, 0), 
       col=c('#F8766D', '#00BFC4'), bty='n')
```
:::: 
:::



## Markov Chains

::: {.columns}
:::: {.column}

* Markov chains are defined by a **state vector** $S$
   - In this case, the value of $S$ represents some parameter
   - For a model with $k$ parameters, $S$ is a matrix with $k$ columns
* The model is *stochastic* and has a memory
   - Moves via random walk: $S_{t+1} = f(S_t)$
   - Chain must be **recurrent**: it must be possible to (eventually) reach any possible value from any other possible value
* Markov models are commonly used to model stochastic processes happening in discrete time steps (e.g., population growth)

$$ S_t = S_{t-1} + fecundity \times S_{t-1} - mortality \times S_{t-1} $$

:::: 
:::: {.column}

```{r echo = FALSE}
set.seed(123)
psim = function(tmax, stmin = 500, stmax = 1000, stdev = 0.04) {
	state = numeric(tmax)
	state[1] = runif(1, stmin, stmax)
	for(t in 2:tmax) {
		state[t] = state[t-1] + state[t-1] * abs(rnorm(1, 1, stdev)) - 
			state[t-1] * abs(rnorm(1, 1, stdev))
	}
	state
}
state = lapply(1:3, \(i) {
	data.table(
	time = 1:1000,
	pop_size = psim(1000),
	pop_id = i)
})
state = rbindlist(state)

ggplot(state) + geom_line(aes(x = time, y = pop_size, col = factor(pop_id))) + 
	ylab("Population Size") + xlab("Time") + scale_color_hue(guide = "none") + 
	theme_minimal()

```

:::: 
:::

## Markov Chain Monte Carlo
::: {.columns}
:::: {.column}

* MCMC applies Markov chains to increase the acceptance rate of rejection sampling
	- With rejection sampling we jump randomly, anywhere in the state space
	- With MCMC, we take very small steps in space, centered around the most recently accepted value
	- Target acceptance rates of 20-50 %
* Individual samples are *not* independent
* Run for long enough, we can approximate the shape of the posterior

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE}
p_rej + annotate(geom = "text", x = 1500, y = 0.003, 
				 label = paste0("Acceptance rate = ", round(length(samples)/length(candidates), 3)))
```

:::: 
:::

## Metropolis-Hastings

* The most general MCMC algorithm

::: {.columns}
:::: {.column}
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$
:::: 
:::: {.column}

```{r, echo = FALSE, fig.height=9}
sd = 1.5
peaks = seq(-30*sd, 30*sd, 3*sd)
scales = 1 - (abs(peaks) / max(peaks+1))
weird = function(x, log=FALSE) sapply(x, function(xx) {
  val = sum(mapply(function(sc, ct) sc * dnorm(xx, ct, sd), scales, peaks))
  if(log)
    val = log(val)
  val
  })
xpr = seq(-40, 40, length.out=1000)
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
```
:::: 
:::



## Metropolis-Hastings: starting value

::: {.columns}
:::: {.column}
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value

:::: 
:::: {.column}

```{r, echo = FALSE, fig.height=7}
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
cex.pt = 1.3
start = -18.7
points(start, weird(start), pch=21, cex=cex.pt, bg="#00BFC4")
```
:::: 
:::




## Metropolis-Hastings: proposal step

::: {.columns}
:::: {.column}
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value $S_0$
2. Propose a candidate $S_{cand}$ by sampling from a proposal distribution **centred around $S_0$**
    - Frequently, $S_{cand} \sim \mathcal{N}(S_t, \sigma_p)$
    - $\sigma_p$ is the **proposal scale** (more on this later)
3. Compute an acceptance probability $r = \frac{t(S_{cand})}{t(S_0)}$
    - accept or reject as in rejection sampling
    - If the candidate is better, $r > 1$, always accept


:::: 
:::: {.column}

```{r, echo = FALSE, fig.height=7}
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
cand = -18
points(cand, weird(cand), pch=21, cex=cex.pt, bg="#00BFC4")
points(start, weird(start), pch=21, cex=cex.pt, bg="#b6e8ea")
arrows(start, weird(start), cand, weird(cand), length=0.07)
plot(c(start, cand), c(0, -1), ylab = "Time", xlab = "x", 
     xaxt='n', yaxt='n', type='b', pch=21, cex=cex.pt/2, 
     bg="#00BFC4", ylim=c(-50, 0), xlim=range(xpr))
```
:::: 
:::





## Metropolis-Hastings: running the chain

::: {.columns}
:::: {.column}
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value $S_0$
2. Propose a candidate $S_{cand}$ by sampling from a proposal distribution **centred around $S_0$**
    - Frequently, $S_{cand} \sim \mathcal{N}(S_t, \sigma_p)$
    - $\sigma_p$ is the **proposal scale** (more on this later)
3. Compute an acceptance probability $r = \frac{t(S_{cand})}{t(S_0)}$
    - accept or reject as in rejection sampling
    - If the candidate is better, $r > 1$, always accept
4. Continue until the state of the chain converges on the target distribution

:::: 
:::: {.column}

```{r, echo = FALSE, fig.height=7}
cex.pt = cex.pt/2
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
points(start, weird(start), pch=21, cex=cex.pt, bg="#b6e8ea")
points(cand, weird(cand), pch=21, cex=cex.pt, bg="#00BFC4")
samples = numeric(51)
samples[1] = start
samples[2] = cand
for(i in 3:51) {
  samples[i] = rnorm(1, samples[i-1], 0.3)
  points(samples[i], weird(samples[i]), pch=21, cex=cex.pt, bg="#00BFC4")
}
plot(samples, 0:(-50), ylab = "Time", xlab = "x", 
     xaxt='n', yaxt='n', type='b', pch=21, cex=cex.pt, 
     bg="#00BFC4", ylim=c(-50, 0), xlim=range(xpr))
```
:::: 
:::




## Metropolis-Hastings: results

::: {.columns}
:::: {.column}
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value $S_0$
2. Propose a candidate $S_{cand}$ by sampling from a proposal distribution **centred around $S_0$**
    - Frequently, $S_{cand} \sim \mathcal{N}(S_t, \sigma_p)$
    - $\sigma_p$ is the **proposal scale** (more on this later)
3. Compute an acceptance probability $r = \frac{t(S_{cand})}{t(S_0)}$
    - accept or reject as in rejection sampling
    - If the candidate is better, $r > 1$, always accept
4. Continue until the state of the chain converges on the target distribution

:::: 
:::: {.column}

```{r, echo = FALSE, fig.height=7}

## Commented out, because this is slow
## uncomment if you want to try it

# samples = numeric(3e5)
# samples[1] = start
# accept = 0
# for(i in 2:length(samples)) {
#   pr = rnorm(1, samples[i-1], 25)
#   r = exp(weird(pr, log = TRUE) - weird(samples[i-1], log=TRUE))
#   U = runif(1)
#   if(r > U) {
#     accept = accept + 1
#     samples[i] = pr
#   } else {
#     samples[i] = samples[i-1]
#   }
# }
# saveRDS(samples, "misc/metrop_ex_samples.rds")

samples = readRDS("../assets/misc/metrop_ex_samples.rds")
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(samples[1:500], type='l', xaxt='n', xlab='time', yaxt='n', ylab='x', xlim=c(1, 500))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', xlim=range(xpr),
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
par(new=TRUE)
hist(samples, breaks = 300, freq=FALSE, axes=FALSE, xlim=range(xpr), main="",
     col="#C77CFF66", border = "#C77CFF", xlab="", ylab="")
```
:::: 
:::


## Metropolis algorithm summary

::: {.algo}
**Algorithm**

```
Define t(x): log unnormalized posterior (i.e, "target") distribution
Define p(x): the proposal distribution
	common: rnorm(n = 1, mean = x, sd = proposal_scale)
Choose state[0] (the starting value)
for i in 1:n_samples
   candidate = p(state[i-1], proposal_scale)
   r = exp( t(candidate) - t(state[i-1])) ## acceptance probability
   if r > runif(1)  ## coin flip to see if we accept or not
      state[i] = candidate
   else
      state[i] = chain[i-1]
```
:::

## Multivariate Metropolis-Hastings
* Logic is the same
* We cannot easily sample from the joint posterior $pr(\theta_1, \theta_2 | X)$
* We can sample from conditional posteriors $pr(\theta_1, | \theta_2, X)$
* Simply sample from parameters one at a time, in random order (see [sample code](https://github.com/mtalluto/vu_advstats_students/blob/main/r/mh.r))


## Tuning Metropolis-Hastings
* Proposal variance is essential for efficiency
* Ideally we want acceptance rates around 0.235 for high dimensional problems, closer to 0.5 for univariate
* Can use adaptive samplers (see sample code) to automate selection of proposal variance
* A simple way to implement adaptation:
   - run a chain, at each step, if accepted, scale = scale * 1.1; if rejected scale = scale / 1.1
   - must discard these samples, start chain over with constant scale


## Traceplots
* Traceplots are an important indicator that chains are working efficiently
* Other helpful plots: `mcmc_hist()` and `mcmc_pairs` (multivariate)

```{r, echo=FALSE}
# samples = matrix(NA, nrow=1e4, ncol=3)
# samples[1,] = start
# accept = c(0,0,0)
# sc = c(0.5, 15, 2000)
# for(i in 2:nrow(samples)) {
#   for(j in 1:3) {
#     pr = rnorm(1, samples[i-1, j], sc[j])
#     r = exp(weird(pr, log = TRUE) - weird(samples[i-1,j], log=TRUE))
#     U = runif(1)
#     if(r > U) {
#       accept[j] = accept[j] + 1
#       samples[i,j] = pr
#     } else {
#       samples[i,j] = samples[i-1,j]
#     }
#   }
# }
# colnames(samples) = c("too small", "just right", "too big")
# saveRDS(samples, "misc/traceplt_ex.rds")
```

```{r, echo=TRUE, message=FALSE, fig.width=10}
library(bayesplot)
samples = readRDS("../assets/misc/traceplt_ex.rds")
mcmc_trace(samples)

```

## MCMC in Stan
> - Stan uses an even more efficient algorithm called Hamiltonian Monte Carlo (HMC) 
> - HMC imagines the posterior density is a frictionless bowl; more probable locations are lower
> - Place a ball on this bowl, give it a shove in a random direction, record everywhere it goes
> - When the ball slows down past a certain threshold, stop, sample, accept/reject, and shove it again
> - Easiest is to view an [animation](https://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html)
> - Implementation is simple! Just use `sampling` instead of `optimization`.


## Posterior inference I: Sampling

::: {.columns}
:::: {.column}

::::
:::: {.column}
::::
:::

> - Taking samples turns out to be a very useful way to learn about a distribution!
> - For example: Bayesian linear regression in Stan

## Posterior inference I: Sampling


::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan

Dataset: Palmer penguins

![](../assets/img/palmerpenguins.png){width=150px}
![](../assets/img/culmen_depth.png){width=250px}

::::: {.small}
> Dataset: Dr. Kristen Gorman, University of Alaska ([Gorman et al 2014)](https://doi.org/10.1371/journal.pone.0090081
)
>
> R package `palmerpenguins`
>
> Artwork by @allison_horst
:::::

::::
:::: {.column}

```{r echo = FALSE, warning = FALSE, fig.width = 6.5}
ggplot(palmerpenguins::penguins) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, colour = species)) + 
	theme_minimal() + xlab("Bill Length (mm)") + ylab("Bill Depth (mm)")
```

::::
:::






## Posterior inference I: Sampling


::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan

Dataset: Palmer penguins

![](../assets/img/palmerpenguins.png){width=150px}
![](../assets/img/culmen_depth.png){width=250px}

::::: {.small}
> Dataset: Dr. Kristen Gorman, University of Alaska ([Gorman et al 2014)](https://doi.org/10.1371/journal.pone.0090081
)
>
> R package `palmerpenguins`
>
> Artwork by @allison_horst
:::::

::::
:::: {.column}


```{stan output.var = "penguin_lm", cache = TRUE}
data {
	int <lower=0> n;
	vector [n] x;
	vector [n] y;
}
parameters {
	real intercept;
	real slope;
	real <lower = 0> s;
}
model {
	y ~ normal(intercept + slope * x, s);
}
```

```{r eval = FALSE}
penguin_lm = stan_model("stan/basic_lm.stan") 

```

```{r, cache = TRUE}
data(penguins, package = "palmerpenguins")
penguins = as.data.frame(penguins[complete.cases(penguins),])
penguins = subset(penguins, species == "Gentoo")
peng_dat = with(penguins, list(
	x = bill_length_mm,
	y = bill_depth_mm,
	n = length(bill_length_mm)
))
peng_fit = sampling(penguin_lm, data = peng_dat, refresh = 0)
```


::::
:::












## Posterior inference I: Sampling


::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan
* What can we learn from our samples?
	- What is the probability that the slope > 0.15?

::::
:::: {.column}

``` {r echo = FALSE, fig.height = 4, fig.width = 4, message = FALSE}
samps = as.matrix(peng_fit)
xx = data.frame(slope = as.matrix(peng_fit, par = "slope"))
bayesplot::mcmc_hist(as.matrix(peng_fit, par = "slope")) + geom_vline(xintercept = 0.15, col = 'red')
```

```{r, cache = TRUE}
# samples are a bit easier to deal with in a matrix
peng_samps = as.matrix(peng_fit)
head(peng_samps)
sum(peng_samps[, 'slope'] > 0.15) / nrow(peng_samps)


```


::::
:::





## Posterior inference I: Sampling
::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan
* What can we learn from our samples?
	- What is the probability that the slope > 0.15?
	- What is the probability of a range of values, say $ 0.15 < slope < 0.2$

::::
:::: {.column}

``` {r echo = FALSE, fig.height = 4, fig.width = 4, message = FALSE}
samps = as.matrix(peng_fit)
xx = data.frame(slope = as.matrix(peng_fit, par = "slope"))
bayesplot::mcmc_hist(as.matrix(peng_fit, par = "slope")) + geom_vline(xintercept = c(0.15, 0.25), col = 'red')
```

```{r, cache = TRUE}
sum(peng_samps[, 'slope'] > 0.15 & 
		peng_samps[, 'slope'] < 0.25) / 
	nrow(peng_samps)


```


::::
:::






## Posterior inference I: Sampling
::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan
* What can we learn from our samples?
	- What is the probability that the slope > 0.15?
	- What is the probability of a range of values, say $ 0.15 < slope < 0.2$
	- What interval encompasses 90% of the probability mass (**90% Credible Interval**)?

::::
:::: {.column}

``` {r echo = FALSE, fig.height = 4, fig.width = 4, message = FALSE}
samps = as.matrix(peng_fit)
xx = data.frame(slope = as.matrix(peng_fit, par = "slope"))
bayesplot::mcmc_hist(as.matrix(peng_fit, par = "slope")) + geom_vline(xintercept = c(0.17, 0.25), col = 'red')
```

```{r, cache = TRUE}
t(apply(samps, 2, quantile, c(0.05, 0.95)))
```


::::
:::




## Posterior inference I: Sampling
::: {.columns}
:::: {.column}

* Taking samples turns out to be a very useful way to learn about a distribution!
* For example: Bayesian linear regression in Stan
* What can we learn from our samples?
	- What is the probability that the slope > 0.15?
	- What is the probability of a range of values, say $ 0.15 < slope < 0.2$
	- What interval encompasses 90% of the probability mass (**90% Credible Interval**)?
* We could emulate the output of `summary(lm(...))`

::::
:::: {.column}



```{r, cache = TRUE}
tab = data.frame(
	estimate = apply(peng_samps[, 1:2], 2, median),
	ste = apply(peng_samps[, 1:2], 2, sd),
	pr = apply(peng_samps[, 1:2], 2, \(x) sum(x <= 0) / length(x))
)
knitr::kable(tab, digits = 2, 
	col.names = c("Estimate", "Std. Error", "pr(E <= 0)"))

```


::::
:::







## Posterior inference I: Sampling
::: {.columns}
:::: {.column}

* What about central tendency? What's the most likely or "best guess" for each parameter?
* For normally distributed posterior, all three are equal
* Otherwise, median performs best, but **always** prefer an interval to a point estimate

::::
:::: {.column}



```{r, cache = TRUE}
tab = data.frame(
	mean = colMeans(peng_samps[, 1:3]),
	median = apply(peng_samps[, 1:3], 2, median),
	## need optimizing to get the posterior mode
	mode = optimizing(penguin_lm, data = peng_dat)$par)

knitr::kable(tab, digits = 5)
```


::::
:::







## Posterior prediction

::: {.columns}
:::: {.column}

* We can use the medians to easily predict the regression line.

::::
:::: {.column}

```{r echo = FALSE}
tab = matrix(tab[, 2], nrow = 1, dimnames = list("", c("intercept", "slope", "s")))
knitr::kable(tab, digits = 2)

pbase = ggplot(data = penguins) + theme_minimal() + xlab("Bill Length (mm)") + ylab("Bill Depth(mm)")
pl = pbase + 
	geom_abline(intercept = median(peng_samps[, 'intercept']), 
				slope = median(peng_samps[,'slope']), col = "#841D15", size = 1.5) + 
	geom_point(aes(x = bill_length_mm, y = bill_depth_mm), col = scales::hue_pal()(2)[1])
	
pl
```

::::
:::






## Posterior prediction

::: {.columns}
:::: {.column}

* We can use the medians to easily predict the regression line.
* Posterior distributions are *transitive*

> If $\hat{\theta}$ is a set of samples approximating the posterior distribution of $\theta$, and if some desired variable $Z = f(\theta)$, then $f(\hat{\theta})$ approximates the posterior distribution of $Z$

* We can use this to get a posterior distribution of **regression lines**

::::
:::: {.column}

```{r echo = FALSE}
knitr::kable(tab, digits = 2)
pl
```

::::
:::





## Posterior prediction

::: {.columns}
:::: {.column}

* We can use the medians to easily predict the regression line.
* Posterior distributions are *transitive*

> If $\hat{\theta}$ is a set of samples approximating the posterior distribution of $\theta$, and if some desired variable $Z = f(\theta)$, then $f(\hat{\theta})$ approximates the posterior distribution of $Z$

* We can use this to get a posterior distribution of **regression lines**
	- Each posterior sample is one potential regression line

::::
:::: {.column}

```{r echo = FALSE}
knitr::kable(tab, digits = 2)
i = sample(nrow(peng_samps), 1)

pl = pbase + 
	geom_abline(intercept = peng_samps[i, 'intercept'], slope = peng_samps[i, 'slope'], 
					  col = "#777777", size = 0.2) + 
	geom_abline(intercept = median(peng_samps[, 'intercept']), 
				slope = median(peng_samps[,'slope']), col = "#841D15", size = 1.5) + 
	geom_point(aes(x = bill_length_mm, y = bill_depth_mm), col = scales::hue_pal()(2)[1])

pl
```

::::
:::







## Posterior prediction

::: {.columns}
:::: {.column}

* We can use the medians to easily predict the regression line.
* Posterior distributions are *transitive*

> If $\hat{\theta}$ is a set of samples approximating the posterior distribution of $\theta$, and if some desired variable $Z = f(\theta)$, then $f(\hat{\theta})$ approximates the posterior distribution of $Z$

* We can use this to get a posterior distribution of **regression lines**
	- Each posterior sample is one potential regression line

::::
:::: {.column}

```{r echo = FALSE}
knitr::kable(tab, digits = 2)
pl = pbase

for(i in sample(nrow(peng_samps), 40)) {
	pl = pl + geom_abline(intercept = peng_samps[i, 'intercept'], slope = peng_samps[i, 'slope'], 
					  col = "#777777", size = 0.2)
}
pl = pl + 
	geom_abline(intercept = median(peng_samps[, 'intercept']), 
				slope = median(peng_samps[,'slope']), col = "#841D15", size = 1.5) + 
	geom_point(aes(x = bill_length_mm, y = bill_depth_mm), col = scales::hue_pal()(2)[1])

pl
```

::::
:::



## Posterior prediction

::: {.columns}
:::: {.column}

* This means we can easily use the samples to predict a credible interval for $\mathbb{E}(y)$ for any arbitrary value of $x$
* What information is this telling us?
	- There is a 90% chance the conditional expectation is in the range(?!)


```{r}
# predict from one x, one sample
pry = function(samp, x) samp[1] + samp[2] * x

test_x = 55.6
# just apply prediction to every row of samples
test_y = apply(peng_samps, 1, pry, x = test_x)
# then get the quantiles
quantile(test_y, c(0.05, 0.95))
```

::::
:::: {.column}


```{r echo = FALSE}

yy = data.frame(x = test_x, y = quantile(test_y, 0.5), low = quantile(test_y, 0.05), hi = quantile(test_y, 0.95))


pl + 
	geom_errorbar(data = yy, aes(x = test_x, ymin = low, ymax = hi), width = 0, col = "#005659", size = 1.5) + 
	geom_point(data = yy, aes(x = test_x, y = y), size = 2.5, pch=21, bg = scales::hue_pal()(2)[2], col = "#005659") 

```

::::
:::


## Posterior prediction

::: {.columns}
:::: {.column}

* This means we can easily use the samples to predict a credible interval for $\mathbb{E}(y)$ for any arbitrary value of $x$
* What information is this telling us?
* We can do the same across many x-values to produce a **confidence ribbon**.
	- This is very similar to regression confidence intervals produced by `lm`


```{r}
# predict from many x, one sample
pry = function(samp, x) samp[1] + samp[2] * x

test_x = seq(40, 60, length.out = 200)

# same as before, but now test_x is a vector, so result is a matrix
# each row in this matrix is the prediction for a single x, each column a single sample
test_y = apply(peng_samps, 1, pry, x = test_x)
test_y[1:3, 1:7]

# then we get the quantiles by row
interval_y = apply(test_y, 1, quantile, c(0.05, 0.95))
interval_y[, 1:5]
```

::::
:::: {.column}


```{r echo = FALSE}

yy = data.frame(x = test_x, y = apply(test_y, 1, median), low = interval_y[1,], hi = interval_y[2,])

pbase + geom_ribbon(data = yy, aes(x = x, ymin = low, ymax = hi), fill = "#F8766D55", col = "#7f3c38") +  
	geom_abline(intercept = median(peng_samps[, 'intercept']), 
				slope = median(peng_samps[,'slope']), col = "#841D15", size = 1.4) + 
	geom_point(aes(x = bill_length_mm, y = bill_depth_mm), pch = 21, col = "#666666", bg = "#F8766D", size = 1)

```

::::
:::



## Posterior prediction

::: {.columns}
:::: {.column}

* Our model is **generative**
* It postulates a *statistical* process (not mechanistic) by which the outcomes $y$ are created
* We can use posterior predictive simulations to learn the distribution of **outcomes**
* For a given value of $x$, the interval tells you where 90% of the values of $y$ will fall (not $\mathbb{E}[y]$)
* To do this:
   - for each sample of $a$, $b$, and $s$
   - for each value of a **prediction dataset** $\hat{x}$
   - compute $\eta = \mathbb{E}(y)$ using the regression equation
   - simulate a new dataset $\hat{y}$ from $\eta$ and $s$
   - compute quantiles for $\hat{y} | \hat{x}$
* Similar to typical regression **prediction intervals**



::::
:::: {.column}


```{r}
# from a single sample, generate a new prediction dataset from xhat
sim = function(samp, xhat) {
	eta = samp[1] + samp[2] * xhat
	rnorm(length(xhat), eta, samp[3])
}

test_x = seq(40, 60, length.out = 200)
pr_test_y = matrix(0, ncol = nrow(peng_samps), nrow = length(test_x))

# for clarity, using a for loop. could (should) do this instead with mapply
for(i in 1:nrow(peng_samps))
	pr_test_y[,i] = sim(peng_samps[i,], test_x)

# now get quantiles for each value in x
pr_int_y = apply(pr_test_y, 1, quantile, c(0.05, 0.95))
```

::::
:::


## Posterior prediction

::: {.columns}
:::: {.column}

* Our model is **generative**
* It postulates a *statistical* process (not mechanistic) by which the outcomes $y$ are created
* We can use posterior predictive simulations to learn the distribution of **outcomes**
* For a given value of $x$, the interval tells you where 90% of the values of $y$ will fall (not $\mathbb{E}[y]$)
* To do this:
   - for each sample of $a$, $b$, and $s$
   - for each value of a **prediction dataset** $\hat{x}$
   - compute $\eta = \mathbb{E}(y)$ using the regression equation
   - simulate a new dataset $\hat{y}$ from $\eta$ and $s$
   - compute quantiles for $\hat{y} | \hat{x}$
* Similar to typical regression **prediction intervals**



::::
:::: {.column}

```{r echo = FALSE}

yy = data.frame(x = test_x, y = apply(test_y, 1, median), low = interval_y[1,], hi = interval_y[2,],
				pr_low = pr_int_y[1,], pr_hi = pr_int_y[2,])

pbase + geom_ribbon(data = yy, aes(x = x, ymin = pr_low, ymax = pr_hi), fill = "#8bc4c644", col = "#52c0c4") +  
	geom_ribbon(data = yy, aes(x = x, ymin = low, ymax = hi), fill = "#F8766D77", col = "#7f3c38") +  
	geom_abline(intercept = median(peng_samps[, 'intercept']), 
				slope = median(peng_samps[,'slope']), col = "#841D15", size = 1.4) + 
	geom_point(aes(x = bill_length_mm, y = bill_depth_mm), pch = 21, col = "#666666", bg = "#F8766D", size = 1)

```



::::
:::

