---
title: "Markov Chain Monte Carlo"
author: "Matthew Talluto"
date: "23.11.2023"
output:
  slidy_presentation:
    theme: cerulean
    toc_depth: 2
    css: rmd_style.css
    self_contained: false
    lib_dir: lib
  beamer_presentation: default
---


```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.width=5.5, fig.height=5.5, collapse = TRUE, comment = "##", dev="png", error = TRUE)
library(ggplot2)
library(data.table)
```

## The German Tank Problem

::: {.columns}
:::: {.column}

* During WW2, the Allies wanted to know the rate of tank production in German factories
* Known: factories stamped serial numbers on the tanks in order, starting with 1
* I have captured a single tank, with (for example), the serial number $s = 200$. 
* How many tanks have been produced? $(N)$
* We know:
	- $N >= 200$
	- $pr(N < 200) = 0$

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE}
s = 200
tankdat = data.table(
	N = 1:2400
)
tankdat$pr = dunif(s, 1, tankdat$N)
tankpts = tankdat[N %in% c(s, 2*s, 4*s, 10*s)]
tankpts$labs = paste("N =", tankpts$N)

pl = ggplot() + theme_minimal() + xlab("N") + ylab("pr(s | N)") + 
	xlim(0, max(tankdat$N)) + ylim(0, max(tankdat$pr, na.rm = TRUE)*1.2)
p2 = pl + geom_line(data = tankdat[N < 200], aes(x = N, y = pr), col = "#00BFC4", size = 1.5)
p2
```

:::: 
:::

## The German Tank Problem

::: {.columns}
:::: {.column}

### Knowns:

* Captured serial number $s = 200$
* Number of tanks $N >= s$

> - Assume for a minute that, just by chance, we caught the tank with the highest serial number.
> - Assume tanks are captured completely at random, so all tanks have the same probability of capture
> - This sounds like a uniform distribution!

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE}
p2
```

:::: 
:::


## The German Tank Problem

::: {.columns}
:::: {.column}

### Knowns:

* Captured serial number $s = 200$
* Number of tanks $N >= s$

* Assume for a minute that, just by chance, we caught the tank with the highest serial number, so $N = 200$.
* Assume tanks are captured completely at random, so all tanks have the same probability of capture
* This sounds like a uniform distribution!
* $pr(s | N) = Uniform(s, 1, 200)$

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE}
pl + geom_line(data = tankdat[N <= 200], aes(x = N, y = pr), col = "#00BFC4", size = 1.5) + 
	geom_point(data = tankpts[N <= 200], aes(x = N, y = pr), pch = 21, bg = "#F8766D", size = 1.5) + 
	geom_text(data = tankpts[N <= 200], aes(x = N + 150, y = pr + .0001, label = labs), size = 3)
```

:::: 
:::




## The German Tank Problem

::: {.columns}
:::: {.column}

* We can imagine another scenario where $N = 400$
* The data $s$ have not changed, just the model
* All numbers are still equally likely to be captured
	- But now there are twice as many possibilities
	- Each individual tank is half as likely to be captured

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE}
tankpts$labs = paste("N =", tankpts$N)
pl + geom_line(data = tankdat[N <= 200], aes(x = N, y = pr), col = "#00BFC4", size = 1.5) +
	geom_point(data = tankpts[N <= 400], aes(x = N, y = pr), pch = 21, bg = "#F8766D", size = 1.5) + 
	geom_text(data = tankpts[N <= 400], aes(x = N + 150, y = pr + .0001, label = labs), size = 3)
```

:::: 
:::



## The German Tank Problem

::: {.columns}
:::: {.column}

* We can imagine another scenario where $N = 400$
* The data $s$ have not changed, just the model
* All numbers are still equally likely to be captured
	- But now there are twice as many possibilities
	- Each individual tank is half as likely to be captured
* The same logic applies if $N = 800$

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE}
pl + geom_line(data = tankdat[N <= 200], aes(x = N, y = pr), col = "#00BFC4", size = 1.5) + 
	geom_point(data = tankpts[N <= 800], aes(x = N, y = pr), pch = 21, bg = "#F8766D", size = 1.5) + 
	geom_text(data = tankpts[N <= 800], aes(x = N + 150, y = pr + .0001, label = labs), size = 3) 
```

:::: 
:::



## The German Tank Problem

::: {.columns}
:::: {.column}

* We can imagine another scenario where $N = 400$
* The data $s$ have not changed, just the model
* All numbers are still equally likely to be captured
	- But now there are twice as many possibilities
	- Each individual tank is half as likely to be captured
* The same logic applies if $N = 800$
* And so on...
	- $s ~ Uniform(min = 1, max = N)$
* What is the MLE? Is there a logical problem with that?

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE}
tankpts$labs = paste("N =", tankpts$N)
pl + geom_line(data = tankdat, aes(x = N, y = pr), col = "#00BFC4", size = 1.5) +
	geom_point(data = tankpts, aes(x = N, y = pr), pch = 21, bg = "#F8766D", size = 1.5) + 
	geom_text(data = tankpts, aes(x = N + 150, y = pr + .0001, label = labs), size = 3) 
```

:::: 
:::






## Rejection sampling

::: {.columns}
:::: {.column}

* There are general algorithms for sampling from unknown distributions
	- Here, the **target distribution**, $t(x)$, is difficult to sample from

$$
t(x) = pr(s|N) \sim Uniform(s,N)
$$

* However, a uniform distribution with a fixed minimum and maximum is easy to sample from
	- We will call this the **proposal distribution** $p(x)$
	- We will rescale it so that it is always taller than $t(x)$

:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE}
px_max = 4000 # the maximum of our uniform sample distribution

# write two small functions to get the density of and to sample from p(x)
dpx = function(x) dunif(x, s, px_max)
rpx = function(n) runif(n, s, px_max)

# also write a density function for the target distribution
# and convert it into a log likelihood function while we are at it
dtx = function(x, N, log = FALSE) dunif(x, 1, N, log = log)
llfun = function(N, s) sum(dtx(s, N, log = TRUE))

# this is the highest point of t(x)
ymax = dtx(s, s)
# use it to scale p(x) at the highest point
scale = ymax/dpx(s)
dpx = function(x) scale * dunif(x, s, px_max)

tankdat$px = dpx(s)
tankdat$tx = dtx(s, tankdat$N)
pl + geom_line(data = tankdat, aes(x = N, y = tx), col = "#00BFC4", size = 1.5) +
	geom_line(data = tankdat, aes(x = N, y = px), col = "#F8766D", size = 1.5) + 
	annotate(geom = "text", x = 800, y = dtx(s, 550), label = "Target t(x)", col = "#00BFC4") + 
	annotate(geom = "text", x = 1000, y = dpx(1000)*1.03, label = "Proposal p(x)", col = "#F8766D") 
```

:::: 
:::





## Rejection sampling

::: {.columns}
:::: {.column}
::::: {.algo}

**Algorithm**

1. Define target distribution t(x)
2. Define proposal distribution p(x)
3. Draw a random sample $y$ from p(x)
4. Compute **acceptance probability** $r = \frac{t(y)}{p(y)}$
5. Accept the sample with probability $r$, reject with probability $1-r$
6. Repeat until desired number of samples obtained

:::::
:::: 
:::: {.column}

```{r echo = FALSE, warning = FALSE, cache = TRUE, message = FALSE}
pl + geom_line(data = tankdat, aes(x = N, y = tx), col = "#00BFC4", size = 1.5) +
	geom_line(data = tankdat, aes(x = N, y = px), col = "#F8766D", size = 1.5) + 
	annotate(geom = "text", x = 800, y = dtx(s, 550), label = "Target t(x)", col = "#00BFC4") + 
	annotate(geom = "text", x = 1000, y = dpx(1000)*1.03, label = "Proposal p(x)", col = "#F8766D") 

```

:::: 
:::


## Rejection sampling

::: {.columns}
:::: {.column}
::::: {.algo}

**Algorithm**

1. Define target distribution t(x)
2. Define proposal distribution p(x)
3. Draw a random sample $y$ from p(x)
4. Compute **acceptance probability** $r = \frac{t(y)}{p(y)}$
5. Accept the sample with probability $r$, reject with probability $1-r$
6. Repeat until desired number of samples obtained

:::::

This is **incredibly inefficient**


```{r}
s = 200
target = function(x) dunif(s, 1, x)
proposal = function(x) dunif(x, 1, 1e6)

## this scale will make sure p(x) >= t(x)
scale = target(s) / proposal(s) 
candidates = as.integer(runif(1e6, 1, 1e6))
r = target(candidates)/(proposal(candidates) * scale)

## uniform numbers between 0 and 1
test = runif(length(r))  

## TRUE if r > test; WHY?
accept = ifelse(r > test, TRUE, FALSE) 
samples = candidates[accept]

```

:::: 
:::: {.column}

```{r echo = FALSE, warning=FALSE}
samples2 = samples[samples <= max(tankdat$N)]
par(mgp=c(0.5,0,0), mar=c(2,2,2,0.2))
yy = dunif(s, 1, tankdat$N)
plot(tankdat$N, dunif(s, 1, tankdat$N), type='l', ylim = c(0, 0.0055), bty='l', lwd=2, col='#00BFC4', xaxt='n', xlab="N", yaxt='n', ylab="pr(s | N)")
lines(tankdat$N, tankdat$px, lwd=2, col='#F8766D')
par(new=TRUE)
hist(samples2, breaks = 20, freq=FALSE, axes=FALSE, xlim=c(0, max(tankdat$N)),
     col="#C77CFF66", border = "#C77CFF", bty='l', xlab="N", ylab="pr(s | N)", main="")
legend("right", legend=c("proposal", "target", 
       paste("acceptance rate =", round(length(samples)/length(candidates), 3))), lwd=c(2, 2, 0), 
       col=c('#F8766D', '#00BFC4'), bty='n')
```
:::: 
:::



## Markov Chains

* Markov chains are defined by a **state vector** $S$
   - In this case, the value of $S$ represents some parameter
   - For a model with $k$ parameters, $S$ is a matrix with $k$ columns
* The model is *stochastic* and has a memory
   - Moves via random walk: $S_{t+1} = f(S_t)$
   - Chain must be **recurrent**: it must be possible to (eventually) reach any possible value from any other possible value
* Markov models are commonly used to model stochastic processes happening in discrete time steps (e.g., population growth)


## Markov Chain Monte Carlo

* MCMC lets us sample from a posterior distribution
* Individual samples are *not* independent (because proposals are centred around the previous value)
* Run for long enough, we can approximate the shape of the posterior


## Metropolis-Hastings

* The most general MCMC algorithm

::: {.columns}
:::: {.column}
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$
:::: 
:::: {.column}

```{r, echo = FALSE, fig.height=9}
sd = 1.5
peaks = seq(-30*sd, 30*sd, 3*sd)
scales = 1 - (abs(peaks) / max(peaks+1))
weird = function(x, log=FALSE) sapply(x, function(xx) {
  val = sum(mapply(function(sc, ct) sc * dnorm(xx, ct, sd), scales, peaks))
  if(log)
    val = log(val)
  val
  })
xpr = seq(-40, 40, length.out=1000)
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
```
:::: 
:::



## Metropolis-Hastings: starting value

::: {.columns}
:::: {.column}
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value

:::: 
:::: {.column}

```{r, echo = FALSE, fig.height=7}
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
cex.pt = 1.3
start = -18.7
points(start, weird(start), pch=21, cex=cex.pt, bg="#00BFC4")
```
:::: 
:::




## Metropolis-Hastings: proposal step

::: {.columns}
:::: {.column}
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value $S_0$
2. Propose a candidate $S_{cand}$ by sampling from a proposal distribution **centred around $S_0$**
    - Frequently, $S_{cand} \sim \mathcal{N}(S_t, \sigma_p)$
    - $\sigma_p$ is the **proposal scale** (more on this later)
3. Compute an acceptance probability $r = \frac{t(S_{cand})}{t(S_0)}$
    - in practice: $r = e^{\log t(S_{cand}) - \log t(S_0)}$
    - accept or reject as in rejection sampling
    - If the candidate is better, $r > 1$, always accept


:::: 
:::: {.column}

```{r, echo = FALSE, fig.height=7}
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
cand = -18
points(cand, weird(cand), pch=21, cex=cex.pt, bg="#00BFC4")
points(start, weird(start), pch=21, cex=cex.pt, bg="#b6e8ea")
arrows(start, weird(start), cand, weird(cand), length=0.07)
plot(c(start, cand), c(0, -1), ylab = "Time", xlab = "x", 
     xaxt='n', yaxt='n', type='b', pch=21, cex=cex.pt/2, 
     bg="#00BFC4", ylim=c(-50, 0), xlim=range(xpr))
```
:::: 
:::





## Metropolis-Hastings: running the chain

::: {.columns}
:::: {.column}
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value $S_0$
2. Propose a candidate $S_{cand}$ by sampling from a proposal distribution **centred around $S_0$**
    - Frequently, $S_{cand} \sim \mathcal{N}(S_t, \sigma_p)$
    - $\sigma_p$ is the **proposal scale** (more on this later)
3. Compute an acceptance probability $r = \frac{t(S_{cand})}{t(S_0)}$
    - in practice: $r = e^{\log t(S_{cand}) - \log t(S_0)}$
    - accept or reject as in rejection sampling
    - If the candidate is better, $r > 1$, always accept
4. Continue; asymptotically, the state of the chain converges on the target distribution

:::: 
:::: {.column}

```{r, echo = FALSE, fig.height=7}
cex.pt = cex.pt/2
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', 
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
points(start, weird(start), pch=21, cex=cex.pt, bg="#b6e8ea")
points(cand, weird(cand), pch=21, cex=cex.pt, bg="#00BFC4")
samples = numeric(51)
samples[1] = start
samples[2] = cand
for(i in 3:51) {
  samples[i] = rnorm(1, samples[i-1], 0.3)
  points(samples[i], weird(samples[i]), pch=21, cex=cex.pt, bg="#00BFC4")
}
plot(samples, 0:(-50), ylab = "Time", xlab = "x", 
     xaxt='n', yaxt='n', type='b', pch=21, cex=cex.pt, 
     bg="#00BFC4", ylim=c(-50, 0), xlim=range(xpr))
```
:::: 
:::




## Metropolis-Hastings: results

::: {.columns}
:::: {.column}
* For an unknown (unnormalized) **target distribution** $t(x)$ where we can compute the (proporitonal) height
  - For example, a posterior distribution

$$
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
$$

1. Choose a starting value $S_0$
2. Propose a candidate $S_{cand}$ by sampling from a proposal distribution **centred around $S_0$**
    - Frequently, $S_{cand} \sim \mathcal{N}(S_t, \sigma_p)$
    - $\sigma_p$ is the **proposal scale** (more on this later)
3. Compute an acceptance probability $r = \frac{t(S_{cand})}{t(S_0)}$
    - in practice: $r = e^{\log t(S_{cand}) - \log t(S_0)}$
    - accept or reject as in rejection sampling
    - If the candidate is better, $r > 1$, always accept
4. Continue; asymptotically, the state of the chain converges on the target distribution

:::: 
:::: {.column}

```{r, echo = FALSE, fig.height=7}

## Commented out, because this is slow
## uncomment if you want to try it

# samples = numeric(3e5)
# samples[1] = start
# accept = 0
# for(i in 2:length(samples)) {
#   pr = rnorm(1, samples[i-1], 25)
#   r = exp(weird(pr, log = TRUE) - weird(samples[i-1], log=TRUE))
#   U = runif(1)
#   if(r > U) {
#     accept = accept + 1
#     samples[i] = pr
#   } else {
#     samples[i] = samples[i-1]
#   }
# }
# saveRDS(samples, "misc/metrop_ex_samples.rds")

samples = readRDS("../assets/misc/metrop_ex_samples.rds")
par(bty='l', mgp=c(0.5,0,0), mar=c(2,2,2,0.2), mfrow=c(2, 1))
plot(samples[1:500], type='l', xaxt='n', xlab='time', yaxt='n', ylab='x', xlim=c(1, 500))
plot(xpr, weird(xpr), type='l', xaxt='n', yaxt='n', bty='l', xlim=range(xpr),
     lwd = 2, col="#F8766D", main="The Weird Distribution", xlab = "x", ylab = "weird(x)")
par(new=TRUE)
hist(samples, breaks = 300, freq=FALSE, axes=FALSE, xlim=range(xpr), main="",
     col="#C77CFF66", border = "#C77CFF", xlab="", ylab="")
```
:::: 
:::


## Metropolis algorithm summary

```
Define t(x): log unnormalized posterior (i.e, "target") distribution
Define p(x): the proposal distribution
	common: rnorm(n = 1, mean = x, sd = proposal_scale)
Choose state[0] (the starting value)
for i in 1:n_samples
   candidate = p(state[i-1], proposal_scale)
   r = exp( t(candidate) - t(state[i-1])) ## acceptance probability
   if r > runif(1)  ## coin flip to see if we accept or not
      state[i] = candidate
   else
      state[i] = chain[i-1]
```

## Multivariate Metropolis-Hastings
* Logic is the same
* We cannot easily sample from the joint posterior $pr(\theta_1, \theta_2 | X)$
* We can sample from conditional posteriors $pr(\theta_1, | \theta_2, X)$
* Simply sample from parameters one at a time, in random order (see [sample code](https://github.com/mtalluto/vu_advanced_statistics/blob/main/r/mh.r))


## Tuning Metropolis-Hastings
* Proposal variance is essential for efficiency
* Ideally we want acceptance rates around 0.235 for high dimensional problems, closer to 0.5 for univariate
* Can use adaptive samplers (see sample code) to automate selection of proposal variance
* A simple way to implement adaptation:
   - run a chain, at each step, if accepted, scale = scale * 1.1; if rejected scale = scale / 1.1
   - must discard these samples, start chain over with constant scale


## Traceplots
* Traceplots are an important indicator that chains are working efficiently
* Other helpful plots: `mcmc_hist()` and `mcmc_pairs` (multivariate)

```{r, echo=FALSE}
# samples = matrix(NA, nrow=1e4, ncol=3)
# samples[1,] = start
# accept = c(0,0,0)
# sc = c(0.5, 15, 2000)
# for(i in 2:nrow(samples)) {
#   for(j in 1:3) {
#     pr = rnorm(1, samples[i-1, j], sc[j])
#     r = exp(weird(pr, log = TRUE) - weird(samples[i-1,j], log=TRUE))
#     U = runif(1)
#     if(r > U) {
#       accept[j] = accept[j] + 1
#       samples[i,j] = pr
#     } else {
#       samples[i,j] = samples[i-1,j]
#     }
#   }
# }
# colnames(samples) = c("too small", "just right", "too big")
# saveRDS(samples, "misc/traceplt_ex.rds")
```

```{r, echo=TRUE, message=FALSE, fig.width=10}
library(bayesplot)
samples = readRDS("../assets/misc/traceplt_ex.rds")
mcmc_trace(samples)

```

## Hamiltonian monte carlo
- Only gets a mention here, we won't implement this
- We will use this when we start using Stan
- HMC imagines the posterior density is a frictionless bowl; more probable locations are lower
- Place a ball on this bowl, give it a shove in a random direction, record everywhere it goes
- The ball curves around the surface, sample from the path
- once the ball slows down past a certain threshold, drop in a new place and shove it again

