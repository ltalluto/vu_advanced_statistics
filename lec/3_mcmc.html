<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Matthew Talluto" />
  <title>Markov Chain Monte Carlo</title>
  <style type="text/css">
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
            pre > code.sourceCode { white-space: pre; position: relative; }
            pre > code.sourceCode > span { line-height: 1.25; }
            pre > code.sourceCode > span:empty { height: 1.2em; }
            .sourceCode { overflow: visible; }
            code.sourceCode > span { color: inherit; text-decoration: inherit; }
            div.sourceCode { margin: 1em 0; }
            pre.sourceCode { margin: 0; }
            @media screen {
            div.sourceCode { overflow: auto; }
            }
            @media print {
            pre > code.sourceCode { white-space: pre-wrap; }
            pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
            }
            pre.numberSource code
              { counter-reset: source-line 0; }
            pre.numberSource code > span
              { position: relative; left: -4em; counter-increment: source-line; }
            pre.numberSource code > span > a:first-child::before
              { content: counter(source-line);
                position: relative; left: -1em; text-align: right; vertical-align: baseline;
                border: none; display: inline-block;
                -webkit-touch-callout: none; -webkit-user-select: none;
                -khtml-user-select: none; -moz-user-select: none;
                -ms-user-select: none; user-select: none;
                padding: 0 4px; width: 4em;
                color: #aaaaaa;
              }
            pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
            div.sourceCode
              {   }
            @media screen {
            pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
            }
            code span.al { color: #ff0000; font-weight: bold; } /* Alert */
            code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
            code span.at { color: #7d9029; } /* Attribute */
            code span.bn { color: #40a070; } /* BaseN */
            code span.bu { color: #008000; } /* BuiltIn */
            code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
            code span.ch { color: #4070a0; } /* Char */
            code span.cn { color: #880000; } /* Constant */
            code span.co { color: #60a0b0; font-style: italic; } /* Comment */
            code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
            code span.do { color: #ba2121; font-style: italic; } /* Documentation */
            code span.dt { color: #902000; } /* DataType */
            code span.dv { color: #40a070; } /* DecVal */
            code span.er { color: #ff0000; font-weight: bold; } /* Error */
            code span.ex { } /* Extension */
            code span.fl { color: #40a070; } /* Float */
            code span.fu { color: #06287e; } /* Function */
            code span.im { color: #008000; font-weight: bold; } /* Import */
            code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
            code span.kw { color: #007020; font-weight: bold; } /* Keyword */
            code span.op { color: #666666; } /* Operator */
            code span.ot { color: #007020; } /* Other */
            code span.pp { color: #bc7a00; } /* Preprocessor */
            code span.sc { color: #4070a0; } /* SpecialChar */
            code span.ss { color: #bb6688; } /* SpecialString */
            code span.st { color: #4070a0; } /* String */
            code span.va { color: #19177c; } /* Variable */
            code span.vs { color: #4070a0; } /* VerbatimString */
            code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
          </style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="lib/header-attrs-2.23/header-attrs.js"></script>
  <script src="lib/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link href="lib/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
  <script src="lib/bootstrap-3.3.5/js/bootstrap.min.js"></script>
  <script src="lib/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
  <script src="lib/bootstrap-3.3.5/shim/respond.min.js"></script>
  <style>h1 {font-size: 34px;}
         h1.title {font-size: 38px;}
         h2 {font-size: 30px;}
         h3 {font-size: 24px;}
         h4 {font-size: 18px;}
         h5 {font-size: 16px;}
         h6 {font-size: 12px;}
         code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
         pre:not([class]) { background-color: white }</style>
  <link href="lib/slidy-2/styles/slidy.css" rel="stylesheet" />
  <script src="lib/slidy-2/scripts/slidy.js"></script>
  <script src="lib/slidy_shiny-1/slidy_shiny.js"></script>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
   href="rmd_style.css" />
</head>
<body>
<div class="slide titlepage">
  <h1 class="title">Markov Chain Monte Carlo</h1>
  <p class="author">
Matthew Talluto
  </p>
  <p class="date">23.11.2023</p>
</div>
<div id="the-german-tank-problem" class="slide section level2">
<h1>The German Tank Problem</h1>
<div class="columns">
<div class="column">
<ul>
<li>During WW2, the Allies wanted to know the rate of tank production in
German factories</li>
<li>Known: factories stamped serial numbers on the tanks in order,
starting with 1</li>
<li>I have captured a single tank, with (for example), the serial number
<span class="math inline">\(s = 200\)</span>.</li>
<li>How many tanks have been produced? <span
class="math inline">\((N)\)</span></li>
</ul>
</div><div class="column">

</div>
</div>
</div>
<div id="the-german-tank-problem-1" class="slide section level2">
<h1>The German Tank Problem</h1>
<div class="columns">
<div class="column">
<h3 id="knowns">Knowns:</h3>
<ul>
<li>Captured serial number <span class="math inline">\(s =
200\)</span></li>
<li>Number of tanks <span class="math inline">\(N &gt;= s\)</span></li>
<li>Assume tanks are captured completely at random, so all tanks have
the same probability of capture</li>
<li>This sounds like a uniform distribution!</li>
<li>Minimum of the distribution is 1, the maximum will be N.</li>
</ul>
</div><div class="column">

</div>
</div>
</div>
<div id="the-german-tank-problem-2" class="slide section level2">
<h1>The German Tank Problem</h1>
<div class="columns">
<div class="column">
<h3 id="knowns-1">Knowns:</h3>
<ul>
<li>Captured serial number <span class="math inline">\(s =
200\)</span></li>
<li>Number of tanks <span class="math inline">\(N &gt;= s\)</span></li>
<li>Assume tanks are captured completely at random, so all tanks have
the same probability of capture</li>
<li>This sounds like a uniform distribution!</li>
<li>Minimum of the distribution is 1, the maximum will be N.</li>
<li><strong>Hypothesis</strong>: <em>We got the biggest tank; <span
class="math inline">\(s = N\)</span></em></li>
</ul>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-1-1.png" width="576" /></p>
</div>
</div>
</div>
<div id="the-german-tank-problem-3" class="slide section level2">
<h1>The German Tank Problem</h1>
<div class="columns">
<div class="column">
<h3 id="knowns-2">Knowns:</h3>
<ul>
<li>Captured serial number <span class="math inline">\(s =
200\)</span></li>
<li>Number of tanks <span class="math inline">\(N &gt;= s\)</span></li>
<li>Assume tanks are captured completely at random, so all tanks have
the same probability of capture</li>
<li>This sounds like a uniform distribution!</li>
<li>Minimum of the distribution is 1, the maximum will be N.</li>
<li><strong>Hypothesis</strong>: <em>We got the biggest tank; <span
class="math inline">\(s = N\)</span></em></li>
</ul>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-2-1.png" width="576" /></p>
</div>
</div>
</div>
<div id="the-german-tank-problem-4" class="slide section level2">
<h1>The German Tank Problem</h1>
<div class="columns">
<div class="column">
<h3 id="hypothesis-2-n-400">Hypothesis 2: <span class="math inline">\(N
= 400\)</span></h3>
<ul>
<li>The data <span class="math inline">\(s\)</span> have not changed,
just the model</li>
<li>All numbers are still equally likely to be captured
<ul>
<li>But now there are twice as many possibilities</li>
<li>Each individual tank is half as likely to be captured</li>
</ul></li>
</ul>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-3-1.png" width="576" /></p>
</div>
</div>
</div>
<div id="the-german-tank-problem-5" class="slide section level2">
<h1>The German Tank Problem</h1>
<div class="columns">
<div class="column">
<h3 id="hypothesis-2-n-400-1">Hypothesis 2: <span
class="math inline">\(N = 400\)</span></h3>
<ul>
<li>The data <span class="math inline">\(s\)</span> have not changed,
just the model</li>
<li>All numbers are still equally likely to be captured
<ul>
<li>But now there are twice as many possibilities</li>
<li>Each individual tank is half as likely to be captured</li>
</ul></li>
</ul>
<h3 id="hypothesis-3-n-800">Hypothesis 3: <span class="math inline">\(N
= 800\)</span></h3>
<ul>
<li>The same logic applies!</li>
</ul>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-4-1.png" width="576" /></p>
</div>
</div>
</div>
<div id="the-german-tank-problem-6" class="slide section level2">
<h1>The German Tank Problem</h1>
<div class="columns">
<div class="column">
<h3 id="hypothesis-2-n-400-2">Hypothesis 2: <span
class="math inline">\(N = 400\)</span></h3>
<ul>
<li>The data <span class="math inline">\(s\)</span> have not changed,
just the model</li>
<li>All numbers are still equally likely to be captured
<ul>
<li>But now there are twice as many possibilities</li>
<li>Each individual tank is half as likely to be captured</li>
</ul></li>
</ul>
<h3 id="hypothesis-3-n-800-1">Hypothesis 3: <span
class="math inline">\(N = 800\)</span></h3>
<ul>
<li>The same logic applies!</li>
</ul>
<p>… and so on.</p>
<p><strong>What is the MLE?</strong> Is there a logical problem with
that?</p>
<p><span class="math display">\[ s \sim  Uniform(min = 1, max =
N)\]</span></p>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-5-1.png" width="576" /></p>
</div>
</div>
</div>
<div id="rejection-sampling" class="slide section level2">
<h1>Rejection sampling</h1>
<div class="columns">
<div class="column">
<ul>
<li>There are general algorithms for sampling from unknown distributions
<ul>
<li>Here, the <strong>target distribution</strong>, <span
class="math inline">\(t(x)\)</span>, is difficult to sample from</li>
</ul></li>
</ul>
<p><span class="math display">\[
t(x) = pr(s|N) \sim Uniform(s,N)
\]</span></p>
<ul>
<li>However, a uniform distribution with a fixed minimum and maximum is
easy to sample from
<ul>
<li>We will call this the <strong>proposal distribution</strong> <span
class="math inline">\(p(x)\)</span></li>
<li>We will rescale it so that it is always taller than <span
class="math inline">\(t(x)\)</span></li>
</ul></li>
</ul>
</div><div class="column">

</div>
</div>
</div>
<div id="rejection-sampling-1" class="slide section level2">
<h1>Rejection sampling</h1>
<div class="columns">
<div class="column">
<div class="algo">
<p><strong>Algorithm</strong></p>
<ol style="list-style-type: decimal">
<li>Define target distribution t(x)</li>
<li>Define proposal distribution p(x)</li>
<li>Draw a random sample <span class="math inline">\(y\)</span> from
p(x)</li>
<li>Compute <strong>acceptance probability</strong> <span
class="math inline">\(r = \frac{t(y)}{p(y)}\)</span></li>
<li>Accept the sample with probability <span
class="math inline">\(r\)</span>, reject with probability <span
class="math inline">\(1-r\)</span></li>
<li>Repeat until desired number of samples obtained</li>
</ol>
</div>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-7-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="rejection-sampling-2" class="slide section level2">
<h1>Rejection sampling</h1>
<div class="columns">
<div class="column">
<div class="algo">
<p><strong>Algorithm</strong></p>
<ol style="list-style-type: decimal">
<li>Define target distribution t(x)</li>
<li>Define proposal distribution p(x)</li>
<li>Draw a random sample <span class="math inline">\(y\)</span> from
p(x)</li>
<li>Compute <strong>acceptance probability</strong> <span
class="math inline">\(r = \frac{t(y)}{p(y)}\)</span></li>
<li>Accept the sample with probability <span
class="math inline">\(r\)</span>, reject with probability <span
class="math inline">\(1-r\)</span></li>
<li>Repeat until desired number of samples obtained</li>
</ol>
</div>
<p>This is <strong>incredibly inefficient</strong></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>s <span class="ot">=</span> <span class="dv">200</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>target <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">dunif</span>(s, <span class="dv">1</span>, x)</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>proposal <span class="ot">=</span> <span class="cf">function</span>(x) <span class="fu">dunif</span>(x, <span class="dv">1</span>, <span class="fl">1e6</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="do">## this scale will make sure p(x) &gt;= t(x)</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>scale <span class="ot">=</span> <span class="fu">target</span>(s) <span class="sc">/</span> <span class="fu">proposal</span>(s) </span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>candidates <span class="ot">=</span> <span class="fu">as.integer</span>(<span class="fu">runif</span>(<span class="fl">1e6</span>, <span class="dv">1</span>, <span class="fl">1e6</span>))</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>r <span class="ot">=</span> <span class="fu">target</span>(candidates)<span class="sc">/</span>(<span class="fu">proposal</span>(candidates) <span class="sc">*</span> scale)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="do">## uniform numbers between 0 and 1</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>test <span class="ot">=</span> <span class="fu">runif</span>(<span class="fu">length</span>(r))  </span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="do">## TRUE if r &gt; test; WHY?</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>accept <span class="ot">=</span> <span class="fu">ifelse</span>(r <span class="sc">&gt;</span> test, <span class="cn">TRUE</span>, <span class="cn">FALSE</span>) </span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>samples <span class="ot">=</span> candidates[accept]</span></code></pre></div>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-9-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="markov-chains" class="slide section level2">
<h1>Markov Chains</h1>
<div class="columns">
<div class="column">
<ul>
<li>Markov chains are defined by a <strong>state vector</strong> <span
class="math inline">\(S\)</span>
<ul>
<li>In this case, the value of <span class="math inline">\(S\)</span>
represents some parameter</li>
<li>For a model with <span class="math inline">\(k\)</span> parameters,
<span class="math inline">\(S\)</span> is a matrix with <span
class="math inline">\(k\)</span> columns</li>
</ul></li>
<li>The model is <em>stochastic</em> and has a memory
<ul>
<li>Moves via random walk: <span class="math inline">\(S_{t+1} =
f(S_t)\)</span></li>
<li>Chain must be <strong>recurrent</strong>: it must be possible to
(eventually) reach any possible value from any other possible value</li>
</ul></li>
<li>Markov models are commonly used to model stochastic processes
happening in discrete time steps (e.g., population growth)</li>
</ul>
<p><span class="math display">\[ S_t = S_{t-1} + fecundity \times
S_{t-1} - mortality \times S_{t-1} \]</span></p>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-10-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="markov-chain-monte-carlo" class="slide section level2">
<h1>Markov Chain Monte Carlo</h1>
<div class="columns">
<div class="column">
<ul>
<li>MCMC applies Markov chains to increase the acceptance rate of
rejection sampling
<ul>
<li>With rejection sampling we jump randomly, anywhere in the state
space</li>
<li>With MCMC, we take very small steps in space, centered around the
most recently accepted value</li>
<li>Target acceptance rates of 20-50 %</li>
</ul></li>
<li>Individual samples are <em>not</em> independent</li>
<li>Run for long enough, we can approximate the shape of the
posterior</li>
</ul>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-11-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="metropolis-hastings" class="slide section level2">
<h1>Metropolis-Hastings</h1>
<ul>
<li>The most general MCMC algorithm</li>
</ul>
<div class="columns">
<div class="column">
<ul>
<li>For an unknown (unnormalized) <strong>target distribution</strong>
<span class="math inline">\(t(x)\)</span> where we can compute the
(proporitonal) height
<ul>
<li>For example, a posterior distribution</li>
</ul></li>
</ul>
<p><span class="math display">\[
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
\]</span></p>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-12-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="metropolis-hastings-starting-value"
class="slide section level2">
<h1>Metropolis-Hastings: starting value</h1>
<div class="columns">
<div class="column">
<ul>
<li>For an unknown (unnormalized) <strong>target distribution</strong>
<span class="math inline">\(t(x)\)</span> where we can compute the
(proporitonal) height
<ul>
<li>For example, a posterior distribution</li>
</ul></li>
</ul>
<p><span class="math display">\[
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
\]</span></p>
<ol style="list-style-type: decimal">
<li>Choose a starting value</li>
</ol>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-13-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="metropolis-hastings-proposal-step"
class="slide section level2">
<h1>Metropolis-Hastings: proposal step</h1>
<div class="columns">
<div class="column">
<ul>
<li>For an unknown (unnormalized) <strong>target distribution</strong>
<span class="math inline">\(t(x)\)</span> where we can compute the
(proporitonal) height
<ul>
<li>For example, a posterior distribution</li>
</ul></li>
</ul>
<p><span class="math display">\[
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
\]</span></p>
<ol style="list-style-type: decimal">
<li>Choose a starting value <span
class="math inline">\(S_0\)</span></li>
<li>Propose a candidate <span class="math inline">\(S_{cand}\)</span> by
sampling from a proposal distribution <strong>centred around <span
class="math inline">\(S_0\)</span></strong>
<ul>
<li>Frequently, <span class="math inline">\(S_{cand} \sim
\mathcal{N}(S_t, \sigma_p)\)</span></li>
<li><span class="math inline">\(\sigma_p\)</span> is the
<strong>proposal scale</strong> (more on this later)</li>
</ul></li>
<li>Compute an acceptance probability <span class="math inline">\(r =
\frac{t(S_{cand})}{t(S_0)}\)</span>
<ul>
<li>accept or reject as in rejection sampling</li>
<li>If the candidate is better, <span class="math inline">\(r &gt;
1\)</span>, always accept</li>
</ul></li>
</ol>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-14-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="metropolis-hastings-running-the-chain"
class="slide section level2">
<h1>Metropolis-Hastings: running the chain</h1>
<div class="columns">
<div class="column">
<ul>
<li>For an unknown (unnormalized) <strong>target distribution</strong>
<span class="math inline">\(t(x)\)</span> where we can compute the
(proporitonal) height
<ul>
<li>For example, a posterior distribution</li>
</ul></li>
</ul>
<p><span class="math display">\[
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
\]</span></p>
<ol style="list-style-type: decimal">
<li>Choose a starting value <span
class="math inline">\(S_0\)</span></li>
<li>Propose a candidate <span class="math inline">\(S_{cand}\)</span> by
sampling from a proposal distribution <strong>centred around <span
class="math inline">\(S_0\)</span></strong>
<ul>
<li>Frequently, <span class="math inline">\(S_{cand} \sim
\mathcal{N}(S_t, \sigma_p)\)</span></li>
<li><span class="math inline">\(\sigma_p\)</span> is the
<strong>proposal scale</strong> (more on this later)</li>
</ul></li>
<li>Compute an acceptance probability <span class="math inline">\(r =
\frac{t(S_{cand})}{t(S_0)}\)</span>
<ul>
<li>accept or reject as in rejection sampling</li>
<li>If the candidate is better, <span class="math inline">\(r &gt;
1\)</span>, always accept</li>
</ul></li>
<li>Continue until the state of the chain converges on the target
distribution</li>
</ol>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-15-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="metropolis-hastings-results" class="slide section level2">
<h1>Metropolis-Hastings: results</h1>
<div class="columns">
<div class="column">
<ul>
<li>For an unknown (unnormalized) <strong>target distribution</strong>
<span class="math inline">\(t(x)\)</span> where we can compute the
(proporitonal) height
<ul>
<li>For example, a posterior distribution</li>
</ul></li>
</ul>
<p><span class="math display">\[
pr(\theta | X) \propto pr(X | \theta)pr(\theta)
\]</span></p>
<ol style="list-style-type: decimal">
<li>Choose a starting value <span
class="math inline">\(S_0\)</span></li>
<li>Propose a candidate <span class="math inline">\(S_{cand}\)</span> by
sampling from a proposal distribution <strong>centred around <span
class="math inline">\(S_0\)</span></strong>
<ul>
<li>Frequently, <span class="math inline">\(S_{cand} \sim
\mathcal{N}(S_t, \sigma_p)\)</span></li>
<li><span class="math inline">\(\sigma_p\)</span> is the
<strong>proposal scale</strong> (more on this later)</li>
</ul></li>
<li>Compute an acceptance probability <span class="math inline">\(r =
\frac{t(S_{cand})}{t(S_0)}\)</span>
<ul>
<li>accept or reject as in rejection sampling</li>
<li>If the candidate is better, <span class="math inline">\(r &gt;
1\)</span>, always accept</li>
</ul></li>
<li>Continue until the state of the chain converges on the target
distribution</li>
</ol>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-16-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="metropolis-algorithm-summary" class="slide section level2">
<h1>Metropolis algorithm summary</h1>
<div class="algo">
<p><strong>Algorithm</strong></p>
<pre><code>Define t(x): log unnormalized posterior (i.e, &quot;target&quot;) distribution
Define p(x): the proposal distribution
    common: rnorm(n = 1, mean = x, sd = proposal_scale)
Choose state[0] (the starting value)
for i in 1:n_samples
   candidate = p(state[i-1], proposal_scale)
   r = exp( t(candidate) - t(state[i-1])) ## acceptance probability
   if r &gt; runif(1)  ## coin flip to see if we accept or not
      state[i] = candidate
   else
      state[i] = chain[i-1]</code></pre>
</div>
</div>
<div id="multivariate-metropolis-hastings" class="slide section level2">
<h1>Multivariate Metropolis-Hastings</h1>
<ul>
<li>Logic is the same</li>
<li>We cannot easily sample from the joint posterior <span
class="math inline">\(pr(\theta_1, \theta_2 | X)\)</span></li>
<li>We can sample from conditional posteriors <span
class="math inline">\(pr(\theta_1, | \theta_2, X)\)</span></li>
<li>Simply sample from parameters one at a time, in random order (see <a
href="https://github.com/mtalluto/vu_advstats_students/blob/main/r/mh.r">sample
code</a>)</li>
</ul>
</div>
<div id="tuning-metropolis-hastings" class="slide section level2">
<h1>Tuning Metropolis-Hastings</h1>
<ul>
<li>Proposal variance is essential for efficiency</li>
<li>Ideally we want acceptance rates around 0.235 for high dimensional
problems, closer to 0.5 for univariate</li>
<li>Can use adaptive samplers (see sample code) to automate selection of
proposal variance</li>
<li>A simple way to implement adaptation:
<ul>
<li>run a chain, at each step, if accepted, scale = scale * 1.1; if
rejected scale = scale / 1.1</li>
<li>must discard these samples, start chain over with constant
scale</li>
</ul></li>
</ul>
</div>
<div id="traceplots" class="slide section level2">
<h1>Traceplots</h1>
<ul>
<li>Traceplots are an important indicator that chains are working
efficiently</li>
<li>Other helpful plots: <code>mcmc_hist()</code> and
<code>mcmc_pairs</code> (multivariate)</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">library</span>(bayesplot)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>samples <span class="ot">=</span> <span class="fu">readRDS</span>(<span class="st">&quot;../assets/misc/traceplt_ex.rds&quot;</span>)</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="fu">mcmc_trace</span>(samples)</span></code></pre></div>
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-18-1.png" width="960" /></p>
</div>
<div id="mcmc-in-stan" class="slide section level2">
<h1>MCMC in Stan</h1>
<ul class="incremental">
<li>Stan uses an even more efficient algorithm called Hamiltonian Monte
Carlo (HMC)</li>
<li>HMC imagines the posterior density is a frictionless bowl; more
probable locations are lower</li>
<li>Place a ball on this bowl, give it a shove in a random direction,
record everywhere it goes</li>
<li>When the ball slows down past a certain threshold, stop, sample,
accept/reject, and shove it again</li>
<li>Easiest is to view an <a
href="https://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html">animation</a></li>
<li>Implementation is simple! Just use <code>sampling</code> instead of
<code>optimization</code>.</li>
</ul>
</div>
<div id="posterior-inference-i-sampling" class="slide section level2">
<h1>Posterior inference I: Sampling</h1>
<div class="columns">
<div class="column">

</div><div class="column">

</div>
</div>
<ul class="incremental">
<li>Taking samples turns out to be a very useful way to learn about a
distribution!</li>
<li>For example: Bayesian linear regression in Stan</li>
</ul>
</div>
<div id="posterior-inference-i-sampling-1" class="slide section level2">
<h1>Posterior inference I: Sampling</h1>
<div class="columns">
<div class="column">
<ul>
<li>Taking samples turns out to be a very useful way to learn about a
distribution!</li>
<li>For example: Bayesian linear regression in Stan</li>
</ul>
<p>Dataset: Palmer penguins</p>
<p><img src="../assets/img/palmerpenguins.png" width="150" /> <img
src="../assets/img/culmen_depth.png" width="250" /></p>
<div class="small">
<blockquote>
<p>Dataset: Dr. Kristen Gorman, University of Alaska (<a
href="https://doi.org/10.1371/journal.pone.0090081">Gorman et al
2014)</a></p>
<p>R package <code>palmerpenguins</code></p>
<p>Artwork by <span class="citation">@allison_horst</span></p>
</blockquote>
</div>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-19-1.png" width="624" /></p>
</div>
</div>
</div>
<div id="posterior-inference-i-sampling-2" class="slide section level2">
<h1>Posterior inference I: Sampling</h1>
<div class="columns">
<div class="column">
<ul>
<li>Taking samples turns out to be a very useful way to learn about a
distribution!</li>
<li>For example: Bayesian linear regression in Stan</li>
</ul>
<p>Dataset: Palmer penguins</p>
<p><img src="../assets/img/palmerpenguins.png" width="150" /> <img
src="../assets/img/culmen_depth.png" width="250" /></p>
<div class="small">
<blockquote>
<p>Dataset: Dr. Kristen Gorman, University of Alaska (<a
href="https://doi.org/10.1371/journal.pone.0090081">Gorman et al
2014)</a></p>
<p>R package <code>palmerpenguins</code></p>
<p>Artwork by <span class="citation">@allison_horst</span></p>
</blockquote>
</div>
</div><div class="column">
<div class="sourceCode" id="cb4"><pre
class="sourceCode stan"><code class="sourceCode stan"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>    <span class="dt">int</span> &lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; n;</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="dt">vector</span> [n] x;</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    <span class="dt">vector</span> [n] y;</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>}</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    <span class="dt">real</span> intercept;</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>    <span class="dt">real</span> slope;</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>    <span class="dt">real</span> &lt;<span class="kw">lower</span> = <span class="dv">0</span>&gt; s;</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>}</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>    y ~ normal(intercept + slope * x, s);</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>penguin_lm <span class="ot">=</span> <span class="fu">stan_model</span>(<span class="st">&quot;stan/basic_lm.stan&quot;</span>) </span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="fu">data</span>(penguins, <span class="at">package =</span> <span class="st">&quot;palmerpenguins&quot;</span>)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>penguins <span class="ot">=</span> <span class="fu">as.data.frame</span>(penguins[<span class="fu">complete.cases</span>(penguins),])</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>penguins <span class="ot">=</span> <span class="fu">subset</span>(penguins, species <span class="sc">==</span> <span class="st">&quot;Gentoo&quot;</span>)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>peng_dat <span class="ot">=</span> <span class="fu">with</span>(penguins, <span class="fu">list</span>(</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>    <span class="at">x =</span> bill_length_mm,</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a>    <span class="at">y =</span> bill_depth_mm,</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    <span class="at">n =</span> <span class="fu">length</span>(bill_length_mm)</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>))</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a>peng_fit <span class="ot">=</span> <span class="fu">sampling</span>(penguin_lm, <span class="at">data =</span> peng_dat, <span class="at">refresh =</span> <span class="dv">0</span>)</span></code></pre></div>
</div>
</div>
</div>
<div id="posterior-inference-i-sampling-3" class="slide section level2">
<h1>Posterior inference I: Sampling</h1>
<div class="columns">
<div class="column">
<ul>
<li>Taking samples turns out to be a very useful way to learn about a
distribution!</li>
<li>For example: Bayesian linear regression in Stan</li>
<li>What can we learn from our samples?
<ul>
<li>What is the probability that the slope &gt; 0.15?</li>
</ul></li>
</ul>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-23-1.png" width="384" /></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># samples are a bit easier to deal with in a matrix</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>peng_samps <span class="ot">=</span> <span class="fu">as.matrix</span>(peng_fit)</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="fu">head</span>(peng_samps)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="do">##           parameters</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="do">## iterations intercept     slope         s      lp__</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="do">##       [1,]  3.548550 0.2398139 0.7500717 -25.63111</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="do">##       [2,]  5.053155 0.2090023 0.7605168 -24.45003</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="do">##       [3,]  5.125631 0.2062183 0.7480150 -24.81033</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="do">##       [4,]  5.405012 0.2003530 0.7545575 -24.86389</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a><span class="do">##       [5,]  5.038352 0.2096524 0.7480333 -24.43136</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a><span class="do">##       [6,]  4.850949 0.2141400 0.7414262 -24.63011</span></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a><span class="fu">sum</span>(peng_samps[, <span class="st">&#39;slope&#39;</span>] <span class="sc">&gt;</span> <span class="fl">0.15</span>) <span class="sc">/</span> <span class="fu">nrow</span>(peng_samps)</span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a><span class="do">## [1] 0.99525</span></span></code></pre></div>
</div>
</div>
</div>
<div id="posterior-inference-i-sampling-4" class="slide section level2">
<h1>Posterior inference I: Sampling</h1>
<div class="columns">
<div class="column">
<ul>
<li>Taking samples turns out to be a very useful way to learn about a
distribution!</li>
<li>For example: Bayesian linear regression in Stan</li>
<li>What can we learn from our samples?
<ul>
<li>What is the probability that the slope &gt; 0.15?</li>
<li>What is the probability of a range of values, say $ 0.15 &lt; slope
&lt; 0.2$</li>
</ul></li>
</ul>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-25-1.png" width="384" /></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="fu">sum</span>(peng_samps[, <span class="st">&#39;slope&#39;</span>] <span class="sc">&gt;</span> <span class="fl">0.15</span> <span class="sc">&amp;</span> </span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>        peng_samps[, <span class="st">&#39;slope&#39;</span>] <span class="sc">&lt;</span> <span class="fl">0.25</span>) <span class="sc">/</span> </span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a>    <span class="fu">nrow</span>(peng_samps)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="do">## [1] 0.9605</span></span></code></pre></div>
</div>
</div>
</div>
<div id="posterior-inference-i-sampling-5" class="slide section level2">
<h1>Posterior inference I: Sampling</h1>
<div class="columns">
<div class="column">
<ul>
<li>Taking samples turns out to be a very useful way to learn about a
distribution!</li>
<li>For example: Bayesian linear regression in Stan</li>
<li>What can we learn from our samples?
<ul>
<li>What is the probability that the slope &gt; 0.15?</li>
<li>What is the probability of a range of values, say $ 0.15 &lt; slope
&lt; 0.2$</li>
<li>What interval encompasses 90% of the probability mass (<strong>90%
Credible Interval</strong>)?</li>
</ul></li>
</ul>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-27-1.png" width="384" /></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="fu">t</span>(<span class="fu">apply</span>(samps, <span class="dv">2</span>, quantile, <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>)))</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="do">##            </span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="do">## parameters           5%         95%</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="do">##   intercept   3.2975308   6.8620439</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a><span class="do">##   slope       0.1711119   0.2455597</span></span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a><span class="do">##   s           0.6817993   0.8465178</span></span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="do">##   lp__      -28.4826597 -24.5796425</span></span></code></pre></div>
</div>
</div>
</div>
<div id="posterior-inference-i-sampling-6" class="slide section level2">
<h1>Posterior inference I: Sampling</h1>
<div class="columns">
<div class="column">
<ul>
<li>Taking samples turns out to be a very useful way to learn about a
distribution!</li>
<li>For example: Bayesian linear regression in Stan</li>
<li>What can we learn from our samples?
<ul>
<li>What is the probability that the slope &gt; 0.15?</li>
<li>What is the probability of a range of values, say $ 0.15 &lt; slope
&lt; 0.2$</li>
<li>What interval encompasses 90% of the probability mass (<strong>90%
Credible Interval</strong>)?</li>
</ul></li>
<li>We could emulate the output of <code>summary(lm(...))</code></li>
</ul>
</div><div class="column">
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>tab <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>    <span class="at">estimate =</span> <span class="fu">apply</span>(peng_samps[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], <span class="dv">2</span>, median),</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>    <span class="at">ste =</span> <span class="fu">apply</span>(peng_samps[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], <span class="dv">2</span>, sd),</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>    <span class="at">pr =</span> <span class="fu">apply</span>(peng_samps[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], <span class="dv">2</span>, \(x) <span class="fu">sum</span>(x <span class="sc">&lt;=</span> <span class="dv">0</span>) <span class="sc">/</span> <span class="fu">length</span>(x))</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(tab, <span class="at">digits =</span> <span class="dv">2</span>, </span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>    <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">&quot;Estimate&quot;</span>, <span class="st">&quot;Std. Error&quot;</span>, <span class="st">&quot;pr(E &lt;= 0)&quot;</span>))</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Estimate</th>
<th align="right">Std. Error</th>
<th align="right">pr(E &lt;= 0)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">intercept</td>
<td align="right">5.09</td>
<td align="right">1.08</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">slope</td>
<td align="right">0.21</td>
<td align="right">0.02</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="posterior-inference-i-sampling-7" class="slide section level2">
<h1>Posterior inference I: Sampling</h1>
<div class="columns">
<div class="column">
<ul>
<li>What about central tendency? What’s the most likely or “best guess”
for each parameter?</li>
<li>For normally distributed posterior, all three are equal</li>
<li>Otherwise, median performs best, but <strong>always</strong> prefer
an interval to a point estimate</li>
</ul>
</div><div class="column">
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>tab <span class="ot">=</span> <span class="fu">data.frame</span>(</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>    <span class="at">mean =</span> <span class="fu">colMeans</span>(peng_samps[, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>]),</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>    <span class="at">median =</span> <span class="fu">apply</span>(peng_samps[, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>], <span class="dv">2</span>, median),</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>    <span class="do">## need optimizing to get the posterior mode</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>    <span class="at">mode =</span> <span class="fu">optimizing</span>(penguin_lm, <span class="at">data =</span> peng_dat)<span class="sc">$</span>par)</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(tab, <span class="at">digits =</span> <span class="dv">5</span>)</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">mean</th>
<th align="right">median</th>
<th align="right">mode</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">intercept</td>
<td align="right">5.09055</td>
<td align="right">5.08527</td>
<td align="right">5.12095</td>
</tr>
<tr class="even">
<td align="left">slope</td>
<td align="right">0.20825</td>
<td align="right">0.20835</td>
<td align="right">0.20761</td>
</tr>
<tr class="odd">
<td align="left">s</td>
<td align="right">0.75813</td>
<td align="right">0.75605</td>
<td align="right">0.74274</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="posterior-prediction" class="slide section level2">
<h1>Posterior prediction</h1>
<div class="columns">
<div class="column">
<ul>
<li>We can use the medians to easily predict the regression line.</li>
</ul>
</div><div class="column">
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">intercept</th>
<th align="right">slope</th>
<th align="right">s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="right">5.09</td>
<td align="right">0.21</td>
<td align="right">0.76</td>
</tr>
</tbody>
</table>
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-31-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="posterior-prediction-1" class="slide section level2">
<h1>Posterior prediction</h1>
<div class="columns">
<div class="column">
<ul>
<li>We can use the medians to easily predict the regression line.</li>
<li>Posterior distributions are <em>transitive</em></li>
</ul>
<blockquote>
<p>If <span class="math inline">\(\hat{\theta}\)</span> is a set of
samples approximating the posterior distribution of <span
class="math inline">\(\theta\)</span>, and if some desired variable
<span class="math inline">\(Z = f(\theta)\)</span>, then <span
class="math inline">\(f(\hat{\theta})\)</span> approximates the
posterior distribution of <span class="math inline">\(Z\)</span></p>
</blockquote>
<ul>
<li>We can use this to get a posterior distribution of
<strong>regression lines</strong></li>
</ul>
</div><div class="column">
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">intercept</th>
<th align="right">slope</th>
<th align="right">s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="right">5.09</td>
<td align="right">0.21</td>
<td align="right">0.76</td>
</tr>
</tbody>
</table>
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-32-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="posterior-prediction-2" class="slide section level2">
<h1>Posterior prediction</h1>
<div class="columns">
<div class="column">
<ul>
<li>We can use the medians to easily predict the regression line.</li>
<li>Posterior distributions are <em>transitive</em></li>
</ul>
<blockquote>
<p>If <span class="math inline">\(\hat{\theta}\)</span> is a set of
samples approximating the posterior distribution of <span
class="math inline">\(\theta\)</span>, and if some desired variable
<span class="math inline">\(Z = f(\theta)\)</span>, then <span
class="math inline">\(f(\hat{\theta})\)</span> approximates the
posterior distribution of <span class="math inline">\(Z\)</span></p>
</blockquote>
<ul>
<li>We can use this to get a posterior distribution of
<strong>regression lines</strong>
<ul>
<li>Each posterior sample is one potential regression line</li>
</ul></li>
</ul>
</div><div class="column">
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">intercept</th>
<th align="right">slope</th>
<th align="right">s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="right">5.09</td>
<td align="right">0.21</td>
<td align="right">0.76</td>
</tr>
</tbody>
</table>
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-33-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="posterior-prediction-3" class="slide section level2">
<h1>Posterior prediction</h1>
<div class="columns">
<div class="column">
<ul>
<li>We can use the medians to easily predict the regression line.</li>
<li>Posterior distributions are <em>transitive</em></li>
</ul>
<blockquote>
<p>If <span class="math inline">\(\hat{\theta}\)</span> is a set of
samples approximating the posterior distribution of <span
class="math inline">\(\theta\)</span>, and if some desired variable
<span class="math inline">\(Z = f(\theta)\)</span>, then <span
class="math inline">\(f(\hat{\theta})\)</span> approximates the
posterior distribution of <span class="math inline">\(Z\)</span></p>
</blockquote>
<ul>
<li>We can use this to get a posterior distribution of
<strong>regression lines</strong>
<ul>
<li>Each posterior sample is one potential regression line</li>
</ul></li>
</ul>
</div><div class="column">
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">intercept</th>
<th align="right">slope</th>
<th align="right">s</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"></td>
<td align="right">5.09</td>
<td align="right">0.21</td>
<td align="right">0.76</td>
</tr>
</tbody>
</table>
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-34-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="posterior-prediction-4" class="slide section level2">
<h1>Posterior prediction</h1>
<div class="columns">
<div class="column">
<ul>
<li>This means we can easily use the samples to predict a credible
interval for <span class="math inline">\(\mathbb{E}(y)\)</span> for any
arbitrary value of <span class="math inline">\(x\)</span></li>
<li>What information is this telling us?
<ul>
<li>There is a 90% chance the conditional expectation is in the
range(?!)</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># predict from one x, one sample</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>pry <span class="ot">=</span> <span class="cf">function</span>(samp, x) samp[<span class="dv">1</span>] <span class="sc">+</span> samp[<span class="dv">2</span>] <span class="sc">*</span> x</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>test_x <span class="ot">=</span> <span class="fl">55.6</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="co"># just apply prediction to every row of samples</span></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>test_y <span class="ot">=</span> <span class="fu">apply</span>(peng_samps, <span class="dv">1</span>, pry, <span class="at">x =</span> test_x)</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a><span class="co"># then get the quantiles</span></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="fu">quantile</span>(test_y, <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a><span class="do">##       5%      95% </span></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a><span class="do">## 16.34892 16.99035</span></span></code></pre></div>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-36-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="posterior-prediction-5" class="slide section level2">
<h1>Posterior prediction</h1>
<div class="columns">
<div class="column">
<ul>
<li>This means we can easily use the samples to predict a credible
interval for <span class="math inline">\(\mathbb{E}(y)\)</span> for any
arbitrary value of <span class="math inline">\(x\)</span></li>
<li>What information is this telling us?</li>
<li>We can do the same across many x-values to produce a
<strong>confidence ribbon</strong>.
<ul>
<li>This is very similar to regression confidence intervals produced by
<code>lm</code></li>
</ul></li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># predict from many x, one sample</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>pry <span class="ot">=</span> <span class="cf">function</span>(samp, x) samp[<span class="dv">1</span>] <span class="sc">+</span> samp[<span class="dv">2</span>] <span class="sc">*</span> x</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>test_x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">40</span>, <span class="dv">60</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a><span class="co"># same as before, but now test_x is a vector, so result is a matrix</span></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a><span class="co"># each row in this matrix is the prediction for a single x, each column a single sample</span></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>test_y <span class="ot">=</span> <span class="fu">apply</span>(peng_samps, <span class="dv">1</span>, pry, <span class="at">x =</span> test_x)</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>test_y[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">7</span>]</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a><span class="do">##       iterations</span></span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a><span class="do">##            [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]</span></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a><span class="do">##   [1,] 13.14111 13.41325 13.37436 13.41913 13.42445 13.41655 13.54323</span></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a><span class="do">##   [2,] 13.16521 13.43425 13.39509 13.43927 13.44552 13.43807 13.56181</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a><span class="do">##   [3,] 13.18931 13.45526 13.41581 13.45940 13.46659 13.45959 13.58039</span></span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a><span class="co"># then we get the quantiles by row</span></span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a>interval_y <span class="ot">=</span> <span class="fu">apply</span>(test_y, <span class="dv">1</span>, quantile, <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))</span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a>interval_y[, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb13-19"><a href="#cb13-19" tabindex="-1"></a><span class="do">##      </span></span>
<span id="cb13-20"><a href="#cb13-20" tabindex="-1"></a><span class="do">##           [,1]     [,2]     [,3]     [,4]     [,5]</span></span>
<span id="cb13-21"><a href="#cb13-21" tabindex="-1"></a><span class="do">##   5%  13.11697 13.14181 13.16670 13.19165 13.21614</span></span>
<span id="cb13-22"><a href="#cb13-22" tabindex="-1"></a><span class="do">##   95% 13.72653 13.74386 13.76265 13.78045 13.79788</span></span></code></pre></div>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-38-1.png" width="528" /></p>
</div>
</div>
</div>
<div id="posterior-prediction-6" class="slide section level2">
<h1>Posterior prediction</h1>
<div class="columns">
<div class="column">
<ul>
<li>Our model is <strong>generative</strong></li>
<li>It postulates a <em>statistical</em> process (not mechanistic) by
which the outcomes <span class="math inline">\(y\)</span> are
created</li>
<li>We can use posterior predictive simulations to learn the
distribution of <strong>outcomes</strong></li>
<li>For a given value of <span class="math inline">\(x\)</span>, the
interval tells you where 90% of the values of <span
class="math inline">\(y\)</span> will fall (not <span
class="math inline">\(\mathbb{E}[y]\)</span>)</li>
<li>To do this:
<ul>
<li>for each sample of <span class="math inline">\(a\)</span>, <span
class="math inline">\(b\)</span>, and <span
class="math inline">\(s\)</span></li>
<li>for each value of a <strong>prediction dataset</strong> <span
class="math inline">\(\hat{x}\)</span></li>
<li>compute <span class="math inline">\(\eta = \mathbb{E}(y)\)</span>
using the regression equation</li>
<li>simulate a new dataset <span class="math inline">\(\hat{y}\)</span>
from <span class="math inline">\(\eta\)</span> and <span
class="math inline">\(s\)</span></li>
<li>compute quantiles for <span class="math inline">\(\hat{y} |
\hat{x}\)</span></li>
</ul></li>
<li>Similar to typical regression <strong>prediction
intervals</strong></li>
</ul>
</div><div class="column">
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="co"># from a single sample, generate a new prediction dataset from xhat</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>sim <span class="ot">=</span> <span class="cf">function</span>(samp, xhat) {</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>    eta <span class="ot">=</span> samp[<span class="dv">1</span>] <span class="sc">+</span> samp[<span class="dv">2</span>] <span class="sc">*</span> xhat</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>    <span class="fu">rnorm</span>(<span class="fu">length</span>(xhat), eta, samp[<span class="dv">3</span>])</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>}</span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>test_x <span class="ot">=</span> <span class="fu">seq</span>(<span class="dv">40</span>, <span class="dv">60</span>, <span class="at">length.out =</span> <span class="dv">200</span>)</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>pr_test_y <span class="ot">=</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">ncol =</span> <span class="fu">nrow</span>(peng_samps), <span class="at">nrow =</span> <span class="fu">length</span>(test_x))</span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a><span class="co"># for clarity, using a for loop. could (should) do this instead with mapply</span></span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(peng_samps))</span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>    pr_test_y[,i] <span class="ot">=</span> <span class="fu">sim</span>(peng_samps[i,], test_x)</span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a><span class="co"># now get quantiles for each value in x</span></span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>pr_int_y <span class="ot">=</span> <span class="fu">apply</span>(pr_test_y, <span class="dv">1</span>, quantile, <span class="fu">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))</span></code></pre></div>
</div>
</div>
</div>
<div id="posterior-prediction-7" class="slide section level2">
<h1>Posterior prediction</h1>
<div class="columns">
<div class="column">
<ul>
<li>Our model is <strong>generative</strong></li>
<li>It postulates a <em>statistical</em> process (not mechanistic) by
which the outcomes <span class="math inline">\(y\)</span> are
created</li>
<li>We can use posterior predictive simulations to learn the
distribution of <strong>outcomes</strong></li>
<li>For a given value of <span class="math inline">\(x\)</span>, the
interval tells you where 90% of the values of <span
class="math inline">\(y\)</span> will fall (not <span
class="math inline">\(\mathbb{E}[y]\)</span>)</li>
<li>To do this:
<ul>
<li>for each sample of <span class="math inline">\(a\)</span>, <span
class="math inline">\(b\)</span>, and <span
class="math inline">\(s\)</span></li>
<li>for each value of a <strong>prediction dataset</strong> <span
class="math inline">\(\hat{x}\)</span></li>
<li>compute <span class="math inline">\(\eta = \mathbb{E}(y)\)</span>
using the regression equation</li>
<li>simulate a new dataset <span class="math inline">\(\hat{y}\)</span>
from <span class="math inline">\(\eta\)</span> and <span
class="math inline">\(s\)</span></li>
<li>compute quantiles for <span class="math inline">\(\hat{y} |
\hat{x}\)</span></li>
</ul></li>
<li>Similar to typical regression <strong>prediction
intervals</strong></li>
</ul>
</div><div class="column">
<p><img src="3_mcmc_files/figure-slidy/unnamed-chunk-40-1.png" width="528" /></p>
</div>
</div>
</div>

  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>

</body>
</html>
